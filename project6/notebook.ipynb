{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XC5a7dX4ndCA"
   },
   "source": [
    "## Multilayer Feedforward Network - Dealing with Missing Data\n",
    "\n",
    "Hillary Cheruiyot\n",
    "\n",
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oUF2VEoNndCT"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import numpy.random as nd\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from timeit import default_timer as timer\n",
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j6h0JQmVndCZ"
   },
   "source": [
    "## Part 1: Construct a feed forward neural network\n",
    "    \n",
    "In this part of the project we are to construct three feed forward neural networks consisting of an input layer, one hidden layer with 1, 2 and 4 nodes respectively, and an output layer.  The hidden layer uses the sigmoid as the activation function and use a linear  output node. \n",
    "We should code the equations from scratch. \n",
    "\n",
    "We are given three datasets containing ($x,y$) points where $y=f(x)$:\n",
    "\n",
    "- In the first dataset, $f(x)$ is a **single step** function (data in`data/step_df.csv`), \n",
    "- In the second dataset, $f(x)$ is a **one hump** function (data in `data/one_hump_df.csv`),\n",
    "- In the third dataset, $f(x)$ is a **two equal humps** function (data in `data/two_hump_df.csv`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gtFzoyn1ndCc"
   },
   "source": [
    "**1.1** Create a plot of each dataset and explore the structure of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "26mT_GzLVOE8"
   },
   "source": [
    "**Read and explore the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ehEpkeWmndCe"
   },
   "source": [
    "First, we read in and have a look at the three datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hK2Uq9yhndCk"
   },
   "outputs": [],
   "source": [
    "# get data\n",
    "step_df = pd.read_csv(\"data/step_df.csv\")\n",
    "one_hump_df = pd.read_csv(\"data/one_hump_df.csv\")\n",
    "two_hump_df = pd.read_csv(\"data/two_hump_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VQ5FGpEcndCo"
   },
   "outputs": [],
   "source": [
    "# HTML display helper functions\n",
    "\n",
    "def html_enclose(tagname, content) -> str:\n",
    "    return HTML(\"<{}>{}</{}>\".format(tagname, content, tagname))\n",
    "\n",
    "def html_strong(s : str) -> str:\n",
    "    return html_enclose(\"b\", s)\n",
    "\n",
    "def html_hr() -> str:\n",
    "    return HTML(\"<hr />\")\n",
    "  \n",
    "def html_br() -> str:\n",
    "    return HTML(\"<br />\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-xCY0xoJUkL6"
   },
   "source": [
    "An overview of the three datasets is given below including some statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1537
    },
    "colab_type": "code",
    "id": "BatIv-myndCt",
    "outputId": "901d999e-adec-4838-8f73-e2d7e8c9d2c9"
   },
   "outputs": [],
   "source": [
    "display(html_strong(\"Step Dataset\"))\n",
    "display(step_df.head())\n",
    "display(step_df.describe())\n",
    "display(html_br())\n",
    "display(html_br())\n",
    "display(html_strong(\"One Hump Dataset\"))\n",
    "display(one_hump_df.head())\n",
    "display(one_hump_df.describe())\n",
    "display(html_br())\n",
    "display(html_br())\n",
    "display(html_strong(\"Two Hump Dataset\"))\n",
    "display(two_hump_df.head())\n",
    "display(two_hump_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oDhqu05BVW8Y"
   },
   "source": [
    "**Create a plot of each dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PPFKh3SFUuyW"
   },
   "source": [
    "The following helper functions create scatter plots of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mXHOhtVwndDd"
   },
   "outputs": [],
   "source": [
    "def setup_figure(nrows = 1, ncols = 1, a_width = 8., a_height = 4.):\n",
    "    figsize = (ncols * a_width, nrows * a_height)\n",
    "    fig, ax = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "    fig.subplots_adjust(left=0.1, right=0.9, top=1.7, bottom=0.1)\n",
    "    return fig, ax\n",
    "\n",
    "def xy_scatter(x, y, label, ax, title = \"y vs. x\", xlabel=\"x\", ylabel=\"y\",\n",
    "               color=\"C0\", alpha=1.0, fontsize = 16, marker = \"o\"):\n",
    "  if len(x) != len(y):\n",
    "    msg = \"x({}) has length {} but y({}) has length {}\".format(\n",
    "        type(x),\n",
    "        len(x),\n",
    "        type(y),\n",
    "        len(y))\n",
    "    raise Exception(msg)\n",
    "  \n",
    "  ax.scatter(x, y, color=color, marker=marker, label=label)\n",
    "  if not (xlabel is None):\n",
    "      ax.set_xlabel(xlabel, fontsize=fontsize)\n",
    "  if not (ylabel is None):\n",
    "      ax.set_ylabel(ylabel, fontsize=fontsize)\n",
    "  if not (title is None):\n",
    "      ax.set_title(title, fontsize=fontsize*1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k0dxhpjandDh"
   },
   "outputs": [],
   "source": [
    "# some constants\n",
    "STEP_DATA=0\n",
    "ONE_HUMP_DATA=1\n",
    "TWO_HUMP_DATA=2\n",
    "datasets = [step_df, one_hump_df, two_hump_df]\n",
    "dataset_names = ['step','one_hump', 'two_hump']\n",
    "dataset_titles = [\"Step Data\", \"One Hump Data\", \"Two Hump Data\"]\n",
    "\n",
    "def plot_dataset(df, data_title, ax, color_cycle_start=0):\n",
    "    color = \"C{}\".format(color_cycle_start)\n",
    "    return xy_scatter(df.x, df.y, data_title, ax, title=data_title,\n",
    "                      xlabel='x', ylabel='y', color=color, marker='*')\n",
    "\n",
    "# dedicated plot functions for each of the three datasets\n",
    "# since we will be replotting pretty often\n",
    "def plot_step(ax, color_cycle_start=3):\n",
    "    plot_dataset(step_df, dataset_titles[STEP_DATA], ax,\n",
    "                 color_cycle_start=color_cycle_start)\n",
    "\n",
    "def plot_one_hump(ax, color_cycle_start=3):\n",
    "    plot_dataset(one_hump_df, dataset_titles[ONE_HUMP_DATA], ax,\n",
    "                 color_cycle_start=color_cycle_start)\n",
    "\n",
    "def plot_two_hump(ax, color_cycle_start=3):\n",
    "    plot_dataset(two_hump_df, dataset_titles[TWO_HUMP_DATA], ax,\n",
    "                 color_cycle_start=color_cycle_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1468
    },
    "colab_type": "code",
    "id": "febQ0vagndDl",
    "outputId": "d4c7fb06-6828-4781-e713-e597b8c0a9c1"
   },
   "outputs": [],
   "source": [
    "fig, axes = setup_figure(nrows = 3, ncols = 1)\n",
    "\n",
    "step_ax = plot_step(axes[0])\n",
    "one_hump_ax = plot_one_hump(axes[1])\n",
    "two_hump_ax = plot_two_hump(axes[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N_BZboYRW0zI"
   },
   "source": [
    "We have 100 observations, 1 predictor x and 1 response variable y in all three cases.\n",
    "\n",
    "The step data appears to be sampled from a sigmoid function with a steep onset from y=0 starting around x=1.95 The value y=1 is reached around x=2.42.\n",
    "\n",
    "The one hump data appears to be sampled from a 1-hump function with a steep onset from y=0 starting around x=1.93 The plateau is reached around x=2.4 and maintained up to x=5.73 before droping back to y=0. The one hump data can be visually approximated by sampling a weighted sum of two step function, one being inverted.\n",
    "\n",
    "The two hump data appears to be sampled from a 2-hump function with a steep onset from y=0 starting around x=0.83 and x=4.96. The plateau is reached around x=1.81 and x=5.71 respectively. The two hump data can be visually approximated by sampling a weighted sum of two 1-hump functions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "id": "zY82BeeRXSWU",
    "outputId": "65b2fdb5-f63a-4dd3-ac27-5df7d0e603da"
   },
   "outputs": [],
   "source": [
    "# code for investigating x and y\n",
    "two_hump_df.sort_values(['x'])[two_hump_df.x>0.5].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2MkuBYIIbTdu",
    "outputId": "c0439bcd-190e-43e5-f3cd-5d2fc0bb615a"
   },
   "outputs": [],
   "source": [
    "step_df.shape, one_hump_df.shape, two_hump_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xvCIynHCndDq"
   },
   "source": [
    "\n",
    "**1.2**  Give values to the weights **manually**, perform a forward pass using the data for the **single step** function and a hidden layer of **one** node, and plot the output from the network, in the same plot as the true $y$ values. Adjust the weigths (again manualy) until the plots match as closely as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pE3Tz8tScFHo"
   },
   "source": [
    "Let us construct a network with a single hidden layer. The nature of the data suggests to choose a sigmoid as activation function. The affine transformation is a linear transformation of the predictor values x using a weight and a bias. For this single perceptron, the output y is just the output from the single node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4hmtiX4UeEb6"
   },
   "source": [
    "**Neural network functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JoDwZPnzndDr"
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0 / (1 + np.exp(-z))\n",
    "\n",
    "def affine(x, w, b):\n",
    "    return w*x + b\n",
    "\n",
    "def perceptron(x, w, b):\n",
    "    return sigmoid(affine(x, w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VNYY3rfleY2w"
   },
   "source": [
    "**Plotting functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "by5od9yjndDw"
   },
   "outputs": [],
   "source": [
    "# scatter plot function\n",
    "def plot_overlay(x, y, label, ax, color=\"C0\", alpha=0.6):\n",
    "    xy_scatter(x, y, label, ax, color=color, alpha=alpha, marker = \"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uT1n-vnMndDz"
   },
   "outputs": [],
   "source": [
    "# overlay a plot of true data and network output\n",
    "# parameters\n",
    "#  n: number of layers\n",
    "#  x: input\n",
    "#  yy: network output\n",
    "#  plot_function: function for plotting true data from single step, one hump and two humps datasets \n",
    "def plot_network_output(n, x, yy, w, b, ax, plot_function, plot_title,\n",
    "                        alpha=0.5, fontsize=16):\n",
    "    plot_function(ax)\n",
    "    plot_overlay(x, yy, \"Perceptron\", ax, color=\"C4\", alpha=alpha)\n",
    "    data_desc = ax.get_title()\n",
    "    ax.set_title(plot_title, fontsize=fontsize)\n",
    "    ax.legend(loc=\"best\")\n",
    "    \n",
    "# plot the output for 1 layer network\n",
    "def plot_1n_output(x, yy, w, b, ax, plot_function, data_desc=\"\"):\n",
    "    plot_title = \"1-node network approximation of {}\\nw={} b={}\".format(\n",
    "        data_desc, w, b)\n",
    "    plot_network_output(1, x, yy, w, b, ax, plot_function, plot_title)\n",
    "\n",
    "# plot for multiple layers network\n",
    "def plot_1n_multiples(x, yys, ws, bs, axes, plot_function, data_desc=\"\",\n",
    "                      offset=0):\n",
    "    for i in range(offset, len(yys)):\n",
    "        plot_1n_output(x, yys[i], ws[i], bs[i], axes[i-offset], plot_function,\n",
    "                       data_desc=data_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9S4Bd-BfgR6u"
   },
   "source": [
    "**Adjusting weights and bias using single step data and one node **\n",
    "\n",
    "We choose possible weight and bias values as a best guess. The plots below shows six attemps made to manually adjust until the network output matches the single step data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1485
    },
    "colab_type": "code",
    "id": "SJ0_qRR1ndEM",
    "outputId": "698ff6d1-c191-4a1c-a756-24726de311f0"
   },
   "outputs": [],
   "source": [
    "## set weights\n",
    "offset = 0\n",
    "yy_step1n = []\n",
    "ws = [ 1, 32, 32]\n",
    "bs =  [-0, 0, -64]\n",
    "\n",
    "\n",
    "## get predictions\n",
    "for i in range(offset, len(ws)):\n",
    "    yy_step1n.append(perceptron(step_df.x, ws[i], bs[i]))\n",
    "\n",
    "## plot \n",
    "fig, axes = setup_figure(nrows = len(yy_step1n) - offset, ncols = 1)\n",
    "plot_1n_multiples(step_df.x, yy_step1n, ws, bs, axes, plot_step,\n",
    "                  data_desc=dataset_titles[STEP_DATA], offset=offset)\n",
    "\n",
    "## save selected weights for later\n",
    "hidden_params = {\n",
    "    STEP_DATA : {\n",
    "      'w' :[ws[-1]],\n",
    "      'h' : [bs[-1]],\n",
    "      'yy' : yy_step1n[-1],\n",
    "      'n' : 1,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1cV5ISS2lERS",
    "outputId": "7f26e6da-0606-497d-b1bd-79eba392b82b"
   },
   "outputs": [],
   "source": [
    "display(html_enclose(\"p\",(\"The weight {} and bias {} offer the closest match between\" +\n",
    "                  \"the 1-layer network output and single step data.\").format(\n",
    "    hidden_params[0]['w'][0],\n",
    "    hidden_params[0]['h'][0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tRLixto4ndEc"
   },
   "source": [
    "**1.3** Do the same for the **one hump** function data, this time using a hidden layer consisting of **two** nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kp8NXdYmH6Mq"
   },
   "source": [
    "We know investigate a neural network wth one hidden layer consisting of two nodes that would fit the one-hump data.\n",
    "\n",
    "**Neural network functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z7_l0hw0ndEe"
   },
   "outputs": [],
   "source": [
    "# function for generating the output of a multi-layer network\n",
    "# parameters\n",
    "#  n: number of layers\n",
    "#  x: input vector\n",
    "#  w: weight vector\n",
    "#  b: bias vector\n",
    "#  w_out: output weight\n",
    "#  b_out: output biais\n",
    "def perceptron_layer(n, x, w, b, w_out=1, b_out=0):\n",
    "    y_out = np.zeros(len(x))\n",
    "    for i in range(0,n):\n",
    "        result = np.array(perceptron(x, w[i], b[i]))\n",
    "        y_out += result\n",
    "    return  affine(y_out, w_out, b_out)\n",
    "\n",
    "# function for visualizing the network output vs true data\n",
    "# for a given weight and bias\n",
    "def plot_nn_output(x, yy, w, b, ax, plot_function, w_out = 1, b_out = 0, n=2,\n",
    "                   data_desc=\"\"):\n",
    "    plot_title = \"{}-node network approximation of {}\\nw={} b={}\".format(\n",
    "        n, data_desc, w, b)\n",
    "    # overlay a plot of true data and network output\n",
    "    plot_network_output(n, x, yy, w, b, ax, plot_function, plot_title)\n",
    "\n",
    "# function for visualizing the network output vs true data for an array of weights and biass \n",
    "def plot_nn_multiples(x, yys, ws, bs, axes, plot_function, w_out = 1,\n",
    "                      b_out = 0, data_desc=\"\", offset=0, n=2):\n",
    "    # todo - remove\n",
    "    #print(\"step_df.x: \", type(step_df.x), step_df.x.shape)\n",
    "    #print(\"yy: \", type(yy_), yy_.shape)\n",
    "    for i in range(offset, len(yys)):\n",
    "        plot_nn_output(x, yys[i], ws[i], bs[i], axes[i+offset], plot_function,\n",
    "                       w_out=w_out,\n",
    "                       b_out=b_out,\n",
    "                       n=n,\n",
    "                       data_desc=data_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rXXg2vJlKrXg"
   },
   "source": [
    "**Adjusting weights and bias using one hump data and two nodes **\n",
    "\n",
    "We start with a neural network with one single node with a sigmoid activation function. We choose possible weight and bias values as a best guess. The plots below shows four attemps made to manually adjust until the network output matches the first slope of the one hump data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7DfALWedZouO"
   },
   "outputs": [],
   "source": [
    "# function for plotting the output of 1-layer network with a single node with a sigmoid activation function\n",
    "#  parameters:\n",
    "#   x: input\n",
    "#   weights: vector of weights\n",
    "#   biases: vector of biases\n",
    "def compare_multiple_weights_biases_1n(x, weights, biases,\n",
    "                                       plot_function=plot_step, data_desc=\"\",\n",
    "                                       a_width=8, a_height=4):\n",
    "  if len(weights) != len(biases):\n",
    "    msg = (\"weights ({}) different length ({})\" +\n",
    "           \" from biases ({}) length ({})\").format(\n",
    "        type(weights),\n",
    "        len(weights),\n",
    "        type(biases),\n",
    "        len(biases),)\n",
    "    raise Exception(msg)\n",
    "  \n",
    "  fig, axes = setup_figure(nrows=len(weights), ncols=1,\n",
    "                           a_width=a_width, a_height=a_height)\n",
    "  yys = []\n",
    "  for i in range(len(weights)):\n",
    "    yys.append(perceptron(x, weights[i], biases[i]))\n",
    "  # handle length one case\n",
    "  if len(yys) == 1:\n",
    "    axes = [axes]\n",
    "  plot_1n_multiples(x, yys, weights, biases, axes, plot_function, offset=0,\n",
    "                   data_desc=data_desc)\n",
    "  return fig, axes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1946
    },
    "colab_type": "code",
    "id": "0T9oir8Bhj_G",
    "outputId": "29127c75-79ac-4879-8d5b-2e1c0166bea3"
   },
   "outputs": [],
   "source": [
    "# One hump\n",
    "\n",
    "## try out for one sigmoid at a time:\n",
    "\n",
    "### set weights\n",
    "offset = 0\n",
    "yy_one_hump_left = []\n",
    "ws_left = [ 1, 10, 10, 16]\n",
    "bs_left =  [ 0, 0, -20, -32]\n",
    "for i in range(offset, len(ws_left)):\n",
    "    yy_one_hump_left.append(perceptron(one_hump_df.x, ws_left[i], bs_left[i]))\n",
    "\n",
    "## plot \n",
    "fig, axes = setup_figure(nrows = len(yy_one_hump_left) - offset, ncols = 1)\n",
    "plot_1n_multiples(one_hump_df.x, yy_one_hump_left, ws_left, bs_left,\n",
    "                  axes, plot_one_hump, offset=offset,\n",
    "                  data_desc=dataset_titles[ONE_HUMP_DATA])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dSMWkT5iL4Gq"
   },
   "source": [
    "We continue with a neural network with one single node and a sigmoid activation function. We choose possible weight and bias values as a best guess. The plots below shows three attemps made to manually adjust until the network output matches the second slope of the one hump data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1485
    },
    "colab_type": "code",
    "id": "l8HzuLyPkp4r",
    "outputId": "42462939-8489-478a-80c2-c86fd9c006ec"
   },
   "outputs": [],
   "source": [
    "# One hump - second slope\n",
    "\n",
    "## try out for one sigmoid at a time:\n",
    "\n",
    "### set weights\n",
    "offset = 0\n",
    "yy_one_hump_right = []\n",
    "ws_right = [ -15, -15, -15]\n",
    "bs_right =  [0, 60, 90]\n",
    "for i in range(offset, len(ws_right)):\n",
    "    yy_one_hump_right.append(perceptron(one_hump_df.x,\n",
    "                                        ws_right[i], bs_right[i]))\n",
    "\n",
    "## plot \n",
    "fig, axes = setup_figure(nrows = len(yy_one_hump_right) - offset, ncols = 1)\n",
    "plot_1n_multiples(one_hump_df.x, yy_one_hump_right, ws_right, bs_right,\n",
    "                  axes, plot_one_hump, offset=offset,\n",
    "                 data_desc=dataset_titles[ONE_HUMP_DATA])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UjykyIZiMtSk"
   },
   "source": [
    "Finally we use the optimal weights obtained above to create a neural network with two nodes and a sigmoid activation function. We now choose possible weight and bias values for the output layer as a best guess. The plots below shows two attemps made to manually adjust until the network output matches the whole one hump data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1024
    },
    "colab_type": "code",
    "id": "wGJVTrnkndEq",
    "outputId": "bc52cdc5-a3ab-442a-f2c7-f7978add1913"
   },
   "outputs": [],
   "source": [
    "# One hump\n",
    "\n",
    "## use combination of weights from individual sigmoids\n",
    "\n",
    "## use combination of weights from individual sigmoids\n",
    "ws_best = (ws_left[-1], ws_right[-1])\n",
    "bs_best = (bs_left[-1], bs_right[-1])\n",
    "yy_one_hump_2n = []\n",
    "\n",
    "w_outs = [\n",
    "  1,\n",
    "  1,]\n",
    "b_outs = [\n",
    "  0,\n",
    "  -1,]\n",
    "\n",
    "## get predictions\n",
    "for i in range(offset, len(w_outs)):\n",
    "    result = perceptron_layer(2, one_hump_df.x, ws_best, bs_best, w_outs[i], b_outs[i])\n",
    "    yy_one_hump_2n.append(result)\n",
    "\n",
    "## plot \n",
    "fig, axes = setup_figure(nrows = len(yy_one_hump_2n) - offset, ncols = 1,\n",
    "                         a_width = 8., a_height = 4.)\n",
    "plot_nn_multiples(one_hump_df.x, yy_one_hump_2n, ws, bs, axes, plot_one_hump,\n",
    "                 data_desc=dataset_titles[ONE_HUMP_DATA])\n",
    "\n",
    "## save selected weights for later\n",
    "hidden_params[ONE_HUMP_DATA] = {\n",
    "    'name' : dataset_names[STEP_DATA],\n",
    "    'data_desc' : dataset_titles[ONE_HUMP_DATA],\n",
    "    'w' : ws_best,\n",
    "    'h' : bs_best,\n",
    "    'yy' : yy_one_hump_2n[-1],\n",
    "    'n' : 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "colab_type": "code",
    "id": "U4B1vB8jXbg-",
    "outputId": "60b1c981-8b1b-4c64-da46-d026df98391a"
   },
   "outputs": [],
   "source": [
    "display(html_enclose(\"p\",(\n",
    "    \"The input weight W_in = {}, input bias b_in = {}, \" +\n",
    "    \"output weight W_out = {} and output bias b_out = {} offer the \" +\n",
    "    \"closest match between the 1-layer network with two nodes and \" +\n",
    "    \"one-hump data.\").format(\n",
    "         hidden_params[ONE_HUMP_DATA]['w'],\n",
    "         hidden_params[ONE_HUMP_DATA]['h'],\n",
    "         w_outs,\n",
    "         b_outs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pu7ZdgQYndEz"
   },
   "source": [
    "**1.4** Do the same for the **two hump** function data but this time increase the number of hidden nodes to **four**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aGYf8HF2ZvnM"
   },
   "source": [
    "We now investigate a neural network with one hidden layer consisting of four nodes that would fit the two-hump data.\n",
    "\n",
    "**Adjusting weights and bias using two-hump data and four nodes **\n",
    "\n",
    "We start with a neural network with one single node with a sigmoid activation function. We choose possible weight and bias values as a best guess. The following plots below shows three iterative attempts, to manually adjusting the weights until the network output matches the left slope of the first hump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1525
    },
    "colab_type": "code",
    "id": "hgORwV2ihbt_",
    "outputId": "ad9e9dae-4788-405b-8701-5bd061a41b22"
   },
   "outputs": [],
   "source": [
    "display(html_enclose(\"h3\", \"First sigmoid\"))\n",
    "ws1 = [1, 20, 20]\n",
    "bs1 = [0, 0, -20]\n",
    "fig, ax = compare_multiple_weights_biases_1n(two_hump_df.x, ws1, bs1,\n",
    "                                plot_function=plot_two_hump,\n",
    "                                data_desc=dataset_titles[TWO_HUMP_DATA])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WttlEM_C-c0m"
   },
   "source": [
    "We followed a similar iterative process with the second, third, and fourth sigmoids. For brevity, only the final results appear in the plots below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 603
    },
    "colab_type": "code",
    "id": "so92X2aQnPFx",
    "outputId": "78f93f91-ba71-4af1-b510-f829e9239a2a"
   },
   "outputs": [],
   "source": [
    "display(html_enclose(\"h3\", \"Second sigmoid\"))\n",
    "ws2 = [-20]\n",
    "bs2 = [60]\n",
    "fig, ax = compare_multiple_weights_biases_1n(two_hump_df.x, ws2, bs2,\n",
    "                                plot_function=plot_two_hump,\n",
    "                                data_desc=dataset_titles[TWO_HUMP_DATA])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 603
    },
    "colab_type": "code",
    "id": "WrCKXrFSoXpm",
    "outputId": "df76895e-e596-488d-c308-2e7255d91b23"
   },
   "outputs": [],
   "source": [
    "display(html_enclose(\"h3\", \"Third sigmoid\"))\n",
    "ws3 = [20]\n",
    "bs3 = [-100]\n",
    "fig, ax = compare_multiple_weights_biases_1n(two_hump_df.x, ws3, bs3,\n",
    "                                plot_function=plot_two_hump,\n",
    "                                data_desc=dataset_titles[TWO_HUMP_DATA])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 603
    },
    "colab_type": "code",
    "id": "hndKSo-apehd",
    "outputId": "11052ea6-36eb-4589-962c-677537aaad74"
   },
   "outputs": [],
   "source": [
    "display(html_enclose(\"h3\", \"Fourth sigmoid\"))\n",
    "ws4 = [- 20]\n",
    "bs4 = [140]\n",
    "fig, ax = compare_multiple_weights_biases_1n(two_hump_df.x, ws4, bs4,\n",
    "                                plot_function=plot_two_hump,\n",
    "                                data_desc=dataset_titles[TWO_HUMP_DATA])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DKNFGjF1F5_E"
   },
   "source": [
    "Using the manually tuned weights and biases for each individual sigmoid, we use them in the combined network output. In the final step, we tune the output transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1064
    },
    "colab_type": "code",
    "id": "qeRXMCpFndE1",
    "outputId": "9875be80-7423-44e7-e680-dacd5bf079ae"
   },
   "outputs": [],
   "source": [
    "display(html_enclose(\"h3\", \"Tuning the linear output layer weight and bias\"))\n",
    "## set weights\n",
    "offset = 0\n",
    "ws = [\n",
    "    ws1[-1],\n",
    "    ws2[-1],\n",
    "    ws3[-1],\n",
    "    ws4[-1]\n",
    "]\n",
    "\n",
    "bs = [\n",
    "    bs1[-1],\n",
    "    bs2[-1],\n",
    "    bs3[-1],\n",
    "    bs4[-1],\n",
    "]\n",
    "yy_two_hump_4n = []\n",
    "\n",
    "w_outs = [1, 1]\n",
    "b_outs = [0, -2]\n",
    "\n",
    "## get predictions\n",
    "nplots = len(w_outs)\n",
    "for i in range(offset, nplots):\n",
    "    result = perceptron_layer(4, two_hump_df.x, ws, bs, w_out=w_outs[i],\n",
    "                              b_out=b_outs[i])\n",
    "    yy_two_hump_4n.append(result)\n",
    "\n",
    "## plot\n",
    "fig, axes = setup_figure(nrows = nplots - offset, ncols = 1)\n",
    "plot_nn_multiples(two_hump_df.x, yy_two_hump_4n, ws, bs, axes, plot_two_hump,\n",
    "             offset=offset, n=4, data_desc=dataset_titles[TWO_HUMP_DATA])\n",
    "\n",
    "## save selected weights for later\n",
    "hidden_params[TWO_HUMP_DATA] = {\n",
    "    'yy' : yy_two_hump_4n[-1],\n",
    "    'w' : ws,\n",
    "    'h' : bs,\n",
    "    'n' : 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "lpebH85RpFEY",
    "outputId": "55df40bf-3b71-4f26-fd65-ff8fb9648d97"
   },
   "outputs": [],
   "source": [
    "display(html_enclose(\"p\",(\"To summarize, the manually selected hidden layer weights\" +\n",
    "                        \" W_in = {},  the manually selected hidden layer biases B_in = {}, \"  +\n",
    "                        \"combined with the linear output layer weight w_out = {} \" +\n",
    "                         \"and output bias b_out = {} offer \" +\n",
    "                        \"a very close match between the 1-layer network with four \" +\n",
    "                        \"nodes and two-hump data.\").format(\n",
    "    hidden_params[TWO_HUMP_DATA]['w'],\n",
    "    hidden_params[TWO_HUMP_DATA]['h'],\n",
    "    w_outs,\n",
    "    b_outs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8XDILaHcBRIN"
   },
   "source": [
    "But while this may appear sufficient graphically, there is no guarantee that this is these are the optimum values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8HgNIYnqndE5"
   },
   "source": [
    "\n",
    "**1.5** Choose the appropriate loss function and calculate and report the loss from all three cases. Derive the gradient of the output layer's weights for all three cases (step, one hump and two humps). Use the weights for the hidden layers we found in the previous question and perform gradient descent on the weights of this layer (output layer). What is the optimised weight value and loss we obtained? How many steps did we take to reach this value? What is the threshold value we used to stop? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "70vpy4lrBfud"
   },
   "source": [
    "Keeping the manually selected weights and biases for the hidden layer, we will attempt gradient descent to optimize the weight and bias of the linear output layer.\n",
    "\n",
    "The first issue is: what loss function should we use to calculate the optimization? Since the output layer is a linear transformation, one obvious candidate is mean square error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U6FVuTR_sR_-"
   },
   "outputs": [],
   "source": [
    "# loss functions: mean square error and gradient\n",
    "\n",
    "def mse(y, y_pred):\n",
    "  # Returns the mean square error given a set of actual and predicted values.\n",
    "  if y.shape[0] != y_pred.shape[0]:\n",
    "    msg = \"[mse] : Mismatch: size({}), size({})\".format(\n",
    "        y.shape[0], y_pred.shape[0])\n",
    "    raise Exception(msg)\n",
    "  return np.mean((y_pred - y)**2) # sum then divide by 1/n\n",
    "\n",
    "def grad_mse(x, y, w_out, b_out, n, ws_h, bs_h, epsilon = None):\n",
    "  # returns the gradient of the Mean Square Error\n",
    "  \n",
    "  # get hidden layer output without any transformation\n",
    "  # (considered a constant in the differentiation)\n",
    "  h = perceptron_layer(n, x, ws_h, bs_h)\n",
    "  \n",
    "  dw_terms = 2*h*(w_out*h + b_out - y)\n",
    "  db_terms = 2*(w_out*h + b_out - y)\n",
    "  return np.mean(dw_terms), np.mean(db_terms) # sum then divide by 1/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D_E3zF0IDJEu"
   },
   "source": [
    "We usually see maximum likelihood in combination with sigmoid functions, which we have used as the activation functions in the hidden layer. However, the way the network combines the values causes it to fall outside $(0,1)$, the domain of likelihood function (and thus the negative log-likelihood function, which we would use in practice.)\n",
    "\n",
    "However, it is possible to first map the range of the network's output onto the domain of the likelihood function. We can do this by passing the output through a sigmoid.\n",
    "\n",
    "Below we define the loss function sigmoid-transformed negative log likelihood and its respective gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-jutgwABhnhW"
   },
   "outputs": [],
   "source": [
    "def epsilon_neighbors(epsilon = 10**-9, n = 1):\n",
    "    # returns an n-tuple of random numbers from\n",
    "    # (-epsilon, epsilon)\n",
    "    if n is None:\n",
    "        print(\"[epsilon_neighbors] Warning: n was set to none\")\n",
    "    elif n < 1:\n",
    "        msg = \"n needs to be greater than one: n={}\".format(n)\n",
    "        raise Exception(msg)\n",
    "    if epsilon >= 1:\n",
    "        msg = \"Epsilon should be << one; provided value: {}\".format(\n",
    "            epsilon)\n",
    "        raise Exception(msg)\n",
    "        \n",
    "    sample = None\n",
    "    if n == 1:\n",
    "        sample = (np.random.random_sample()*2 - 1)*epsilon\n",
    "    else:\n",
    "        sample = (np.random.random_sample((n, ))*2 - 1)*epsilon\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P32gNE1qEys7"
   },
   "outputs": [],
   "source": [
    "def sigmoid_log_loss(y, y_pred):\n",
    "    p = sigmoid(y_pred)\n",
    "    # Negative log likelihood of observations\n",
    "    # first term: y*log(p), p = y_pred\n",
    "    i = 0\n",
    "    while np.sum(p <= 0) > 0:\n",
    "      i += 1\n",
    "      p += np.abs(epsilon_neighbors(n=p.shape[0]))\n",
    "    l_term1 = np.sum(y*np.log(p))\n",
    "    # second term: (1-y)*log(1 - p), p = sigmoid(y_pred)\n",
    "    while np.sum((1-p) <= 0) > 0:\n",
    "      i += 1\n",
    "      p -= np.abs(epsilon_neighbors(n=p.shape[0]))\n",
    "    l_term2 = np.sum((1 - y)*np.log((1- p)))\n",
    "    if i > 50:\n",
    "      print(\"Warning: jittered epsilons {} times!\".format(i))\n",
    "    # sum of terms\n",
    "    return - (l_term1 + l_term2)\n",
    "  \n",
    "def grad_sigmoid_log_loss(x, y, w_out, b_out, n, ws_h, bs_h,\n",
    "                          epsilon = 10**-9):\n",
    "    # get hidden layer output without any transformation\n",
    "    # (considered a constant in the differentiation)\n",
    "    h = sigmoid(perceptron_layer(n, x, ws_h, bs_h))\n",
    "    \n",
    "    # partial derivative with respect to w_out\n",
    "    dLa_dw = h*y/(w_out*h + b_out - 1)/(w_out*h + b_out)\n",
    "    dLb_dw = -h/(w_out*h + b_out -1)\n",
    "    dL_dw = dLa_dw + dLb_dw\n",
    "    \n",
    "    # partial derivative with respect to b_out\n",
    "    dLa_db = y/(w_out*h + b_out - 1)/(w_out*h + b_out)\n",
    "    dLb_db = -1/(w_out*h + b_out -1)\n",
    "    dL_db = dLa_db + dLb_db\n",
    "    return (-np.sum(dL_db), -np.sum(dL_dw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DgRdEjiAFwaO"
   },
   "source": [
    "As a third possibility, we will also define the Mean Average Error, or L1, as a potential loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NomrKVb3g05b"
   },
   "outputs": [],
   "source": [
    "def mae(y, y_pred):\n",
    "  return np.sum(np.absolute(y_pred - y))\n",
    "\n",
    "def grad_mae(x, y, w_out, b_out, n, ws_h, bs_h, epsilon = None):\n",
    "  # get hidden layer output without any transformation\n",
    "  # (considered a constant in the differentiation)\n",
    "  h = perceptron_layer(n, x, ws_h, bs_h)\n",
    "  \n",
    "  # screen for division by zero\n",
    "  dw_denom_input = w_out*h + b_out - y\n",
    "  # if it contains at least one zero, add some random epsilons\n",
    "  if np.sum(dw_denom_input == 0) > 0:\n",
    "    dw_denom_input += epsilon_neighbors(n=dw_denom_input.shape[0])\n",
    "  \n",
    "  dw_terms = h*(w_out*h + b_out - y)/np.abs(dw_denom_input)\n",
    "\n",
    "  dh_denom_input = w_out*h + b_out - y\n",
    "  # if it contains at least one zero, add some random epsilons\n",
    "  if np.sum(dw_denom_input == 0) > 0:\n",
    "    dw_denom_input += epsilon_neighbors(n=dw_denom_input.shape[0])\n",
    "  dh_terms = w_out*(w_out*h + b_out - y)/np.abs(dh_denom_input)\n",
    "  \n",
    "  return np.sum(dw_terms), np.sum(dh_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mMVuzvtSFqdT"
   },
   "source": [
    "Next we will define the general procedures for gradient descent. We will include an option to receive a record of loss at every step so we can analyze the performance of different loss functions and parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mzK3GVL1ndE7"
   },
   "outputs": [],
   "source": [
    "def loss(y, y_pred, loss_function=mse):\n",
    "  return loss_function(y, y_pred)\n",
    "\n",
    "def grad(x, y, w_out, b_out, n, ws_h, bs_h,\n",
    "         grad_function=grad_mse, epsilon = 10**-9):\n",
    "  # wrapper function, returning the tuple (gradient)\n",
    "  return grad_function(x, y, w_out, b_out, n, ws_h, bs_h, epsilon=epsilon)\n",
    "  \n",
    "\n",
    "def grad_descent(x : np.array, y : np.array,\n",
    "                 w_out0 : float,\n",
    "                 b_out0 : float,\n",
    "                 n : int,\n",
    "                 ws_h : list,\n",
    "                 bs_h : list,\n",
    "                 loss_function=mse,\n",
    "                 grad_function=grad_mse,\n",
    "                 l_rate = 0.01,\n",
    "                 threshold = 10**-5,\n",
    "                 max_iter = 10000,\n",
    "                 epsilon=10**-9,\n",
    "                 trace=False):\n",
    "  # start at initial point\n",
    "  # get value (d_w, d_h) for gradient\n",
    "  # step in opposite direction from gradient, i.e. (w_i - d_w*l_rate, h_i - d_h*l_rate)\n",
    "  # check if within threshold\n",
    "  steps = 0\n",
    "  w_i = w_out0\n",
    "  b_i = b_out0\n",
    "  \n",
    "  y_pred = perceptron_layer(n, x, ws_h, bs_h, w_i, b_i)\n",
    "  current_loss = loss(y, y_pred, loss_function=loss_function)\n",
    "  if trace:\n",
    "      losses = [current_loss]\n",
    "  \n",
    "  # let l=loss, t=threshold to test against\n",
    "  within_threshold = lambda l, t : (l < t)\n",
    "\n",
    "  while not within_threshold(current_loss, threshold) and steps < max_iter:\n",
    "    (d_w, d_b) = grad(x, y, w_i, b_i, n, ws_h, bs_h,\n",
    "                      grad_function=grad_function, epsilon=epsilon)\n",
    "    # we want to move in the opposite direction\n",
    "    (w_i, b_i) = (w_i - d_w*l_rate, b_i - d_b*l_rate)\n",
    "\n",
    "    steps += 1\n",
    "    y_pred = perceptron_layer(n, x, ws_h, bs_h, w_i, b_i)\n",
    "    current_loss = loss(y, y_pred, loss_function)\n",
    "    \n",
    "    # if trace enabled, logs current loss\n",
    "    if trace:\n",
    "      losses += [current_loss]\n",
    "    \n",
    "    # check for errors\n",
    "    if current_loss < 0:\n",
    "      msg = (\"grad_descent error: loss {} is < 0!\" +\n",
    "             \" check the loss function.\").format(current_loss)\n",
    "      raise Exception(msg)\n",
    "    elif np.isnan(current_loss):\n",
    "      msg = (\"grad_descent error: loss is {} \" +\n",
    "             \"for epsilon={}! check the loss function.\").format(\n",
    "          current_loss, 10**-9)\n",
    "      raise Exception(msg)\n",
    "  \n",
    "  if trace:\n",
    "    result = (w_i, b_i, current_loss, steps, tuple(losses))\n",
    "  else:\n",
    "    result = (w_i, b_i, current_loss, steps)\n",
    "  return result\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xioHqQxrGcuO"
   },
   "source": [
    "And here are a few utilitizes to report the results of the gradient descent, and optionally to plot loss for each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pa1Kj_gcpbdO"
   },
   "outputs": [],
   "source": [
    "def plot_loss(ax, losses : np.array, threshold : float,\n",
    "              opt_variant=\"\", unit_name=\"\", loss_function_name=\"\",\n",
    "              learning_rate=0, other_desc=\"\", fontsize=16):\n",
    "  # creates a plot of loss for each step of gradient descent\n",
    "  steps = list(range(0, len(losses))) # x-axis\n",
    "\n",
    "  # build plot title  \n",
    "  loss_desc = \"Loss\"\n",
    "  title = \"\"\n",
    "  if len(opt_variant):\n",
    "    title = optimization_variant\n",
    "  else:\n",
    "    title = \"Gradient descent optimization\"\n",
    "  if len(unit_name):\n",
    "    title += \" on {} layer weights\".format(unit_name)\n",
    "  if len(loss_function_name):\n",
    "    loss_desc = loss_function_name\n",
    "    title += \"\\n{} loss\".format(loss_desc)\n",
    "  if learning_rate:\n",
    "    title += \", learning rate={}\".format(learning_rate)\n",
    "  if len(other_desc):\n",
    "    title += \", {}\".format(initial_values)\n",
    "  title += \", threshold: {}\".format(threshold)\n",
    "  \n",
    "  # labels and formatting for plot\n",
    "  xaxis_label = \"Steps\"\n",
    "  yaxis_label = \"Loss\"\n",
    "  if len(losses) <= 150:\n",
    "    fmt_str = \"o-\"\n",
    "  else:\n",
    "    fmt_str = \"-\"\n",
    "  \n",
    "  # plot\n",
    "  ax.plot(np.array(steps), np.array(losses), fmt_str, label=loss_desc)\n",
    "  ax.set_xlim(-steps[-1]*0.02, steps[-1]*1.02)\n",
    "  ax.set_xlabel(xaxis_label, fontsize=fontsize)\n",
    "  ax.set_ylabel(yaxis_label, fontsize=fontsize)\n",
    "  ax.set_title(title, fontsize=fontsize)\n",
    "  ax.legend(loc=\"best\", fontsize=fontsize*.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SBkFe4t0QESY"
   },
   "outputs": [],
   "source": [
    "def show_opt_info(x, y, dataset_plot, ws, bs, w_out0, b_out_0,\n",
    "                  axes, title, offset=0,\n",
    "                  loss_function=mse,\n",
    "                  grad_function=grad_mse,\n",
    "                  learning_rate=10**-2, threshold=10**-4,\n",
    "                  max_iter=10**3, hidden_layer_size=1, plot_steps=False):\n",
    "  display(HTML(\"<h3>{}</h3>\".format(title)))\n",
    "  # do gradient descent\n",
    "  w_out, b_out, network_loss, steps, losses = grad_descent(\n",
    "      x,\n",
    "      y,\n",
    "      w_out0,\n",
    "      b_out0,\n",
    "      hidden_layer_size,\n",
    "      ws,\n",
    "      bs,\n",
    "      loss_function=loss_function,\n",
    "      grad_function=grad_function,\n",
    "      l_rate = learning_rate, # learning rate\n",
    "      threshold = threshold, # threshold\n",
    "      max_iter=max_iter, # maximum iterations before stopping\n",
    "      trace = plot_steps # save sequence of losses if we want to plot loss vs step #\n",
    "  )\n",
    "  display(HTML(\"<br /><em>Gradient Descent Results</em>\"))\n",
    "  table_data = {\n",
    "       \"Steps\" : [steps],\n",
    "      \"Network Loss\" : [network_loss],\n",
    "      \"Optimized b_out\" : [b_out],\n",
    "      \"Optimized w_out\": [w_out],\n",
    "      \"Threshold\" : [threshold],\n",
    "      \"Learning Rate\" : [learning_rate],\n",
    "  }\n",
    "  table_df = pd.DataFrame(data=table_data)\n",
    "  display(table_df.T)\n",
    "  yy = perceptron_layer(hidden_layer_size, x, ws, bs, w_out=w_out, b_out=b_out)\n",
    "  plot_nn_output(step_df.x,\n",
    "                    yy,\n",
    "                    ws,\n",
    "                    bs,\n",
    "                    axes[0+offset],\n",
    "                    dataset_plot,\n",
    "                    w_out = w_out,\n",
    "                    b_out = b_out,\n",
    "                    n=hidden_layer_size)\n",
    "  # only plot loss vs step # if trace is turned on\n",
    "  if plot_steps:\n",
    "    plot_loss(axes[1+offset],\n",
    "              losses, 10**-5,\n",
    "              unit_name=\"linear output\",\n",
    "              learning_rate=10**-2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w6WSLmzLGuE1"
   },
   "source": [
    "First, we will look at optimizing the output layer's weight and bias for an estimation of the Step dataset.\n",
    "\n",
    "As a first step, we will try optimizing with MSE as the loss function, keeping the maximum number of steps relatively low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1356
    },
    "colab_type": "code",
    "id": "p4JB_UbCENEl",
    "outputId": "0a2adf2c-c5f1-428e-98a5-3ee4f021774f"
   },
   "outputs": [],
   "source": [
    "# step dataset / 1-node network : mse\n",
    "\n",
    "fig, axes = setup_figure(2, 1)\n",
    "\n",
    "## initial linear output weights\n",
    "w_out0 = 1\n",
    "b_out0 = 0\n",
    "hidden_nodes = 1\n",
    "\n",
    "## gradient descent parameters\n",
    "learning_rate_ = 10**-2\n",
    "threshold_ = 10**-5\n",
    "max_iter_ = 10**3\n",
    "\n",
    "show_opt_info(\n",
    "    datasets[STEP_DATA].x, # x,\n",
    "    datasets[STEP_DATA].y, # y,\n",
    "    plot_step, # dataset_plot\n",
    "    hidden_params[STEP_DATA]['w'], # ws,\n",
    "    hidden_params[STEP_DATA]['h'], # bs,\n",
    "    w_out0,\n",
    "    b_out0,\n",
    "    axes,\n",
    "    \"<em>{} vs Network Output</em><br/>\".format(dataset_titles[STEP_DATA]) +\n",
    "    \"{}-node hidden layer (manually tuned weights)<br />\".format(hidden_nodes) +\n",
    "    \"Linear output layer (optimized model MSE)\", # title,\n",
    "    learning_rate=learning_rate_,\n",
    "    threshold=threshold_,\n",
    "    max_iter=max_iter_,\n",
    "    hidden_layer_size=hidden_nodes,\n",
    "    plot_steps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J3Yg6VfNHTzF"
   },
   "source": [
    "Even with the gradient descent to 1000 steps, it does fairly well estimating the true data, except for the two points in the middle of the graph.\n",
    "\n",
    "Looking at the plot of loss versus step number, we can see that the loss decreases rapidly in the first 300 or so steps. After this, the decrease levels out.\n",
    "\n",
    "It may take a much longer time for gradient descent to find a minimum under our threshold, or to improve the output significantly.\n",
    "\n",
    "Now, we will look at how MAE loss does for the first 1000 steps with the same parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1356
    },
    "colab_type": "code",
    "id": "UdYPupkjvytx",
    "outputId": "b8fc4a3f-7011-4ef8-f9d2-04ed6de7f340"
   },
   "outputs": [],
   "source": [
    "# step dataset / 1-node network : mae\n",
    "\n",
    "fig, axes = setup_figure(2, 1)\n",
    "\n",
    "## initial linear output weights\n",
    "w_out0 = 1\n",
    "b_out0 = 0\n",
    "hidden_nodes = 1\n",
    "\n",
    "## gradient descent parameters\n",
    "learning_rate_ = 10**-2\n",
    "threshold_ = 10**-5\n",
    "max_iter_ = 10**3\n",
    "\n",
    "show_opt_info(\n",
    "    datasets[STEP_DATA].x, # x,\n",
    "    datasets[STEP_DATA].y, # y,\n",
    "    plot_step, # dataset_plot\n",
    "    hidden_params[STEP_DATA]['w'], # ws,\n",
    "    hidden_params[STEP_DATA]['h'], # bs,\n",
    "    w_out0,\n",
    "    b_out0,\n",
    "    axes,\n",
    "    \"<em>{} vs Network Output</em><br/>\".format(dataset_titles[STEP_DATA]) +\n",
    "    \"{}-node hidden layer (manually tuned weights)<br />\".format(hidden_nodes) +\n",
    "    \"Linear output layer (optimized model MAE)\", # title,\n",
    "    loss_function=mae,\n",
    "    grad_function=grad_mae,\n",
    "    learning_rate=learning_rate_,\n",
    "    threshold=threshold_,\n",
    "    max_iter=max_iter_,\n",
    "    hidden_layer_size=hidden_nodes,\n",
    "    plot_steps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wd0Gl6f9Ijm3"
   },
   "source": [
    "While gradient descent with MAE loss does obtain the correct shape of the graph after 1000 steps, the scale is wrong.\n",
    "\n",
    "Looking at the loss vs steps graph, it is hard to tell whether the loss will converge towards a minimum with additional steps. With this in mind, MAE seems to be a worse choice than MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1356
    },
    "colab_type": "code",
    "id": "aoqIBm7kIhPQ",
    "outputId": "71c19fdc-6af0-4b7e-9822-5c35c687f622"
   },
   "outputs": [],
   "source": [
    "# step dataset / 1-node network : sigmoid-log-loss\n",
    "\n",
    "fig, axes = setup_figure(2, 1)\n",
    "\n",
    "## initial linear output weights\n",
    "w_out0 = 1\n",
    "b_out0 = 0\n",
    "hidden_nodes = 1\n",
    "\n",
    "## gradient descent parameters\n",
    "learning_rate_ = 10**-2\n",
    "threshold_ = 10**-5\n",
    "max_iter_ = 10**3\n",
    "\n",
    "show_opt_info(\n",
    "    datasets[STEP_DATA].x, # x,\n",
    "    datasets[STEP_DATA].y, # y,\n",
    "    plot_step, # dataset_plot\n",
    "    hidden_params[STEP_DATA]['w'], # ws,\n",
    "    hidden_params[STEP_DATA]['h'], # bs,\n",
    "    w_out0,\n",
    "    b_out0,\n",
    "    axes,\n",
    "    \"<em>{} vs Network Output</em><br/>\".format(dataset_titles[STEP_DATA]) +\n",
    "    \"{}-node hidden layer (manually tuned weights)<br />\".format(hidden_nodes) +\n",
    "    \"Linear output layer (optimized model: Sigmoid-Log Loss)\", # title,\n",
    "    loss_function=sigmoid_log_loss,\n",
    "    grad_function=grad_sigmoid_log_loss,\n",
    "    learning_rate=learning_rate_,\n",
    "    threshold=threshold_,\n",
    "    max_iter=max_iter_,\n",
    "    hidden_layer_size=hidden_nodes,\n",
    "    plot_steps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vyRd4zohMrMq"
   },
   "source": [
    "With this loss function, the calculated loss increases sharply at certain points and remains high. It seems to be decreasing, but the simple MSE is still a better choice.\n",
    "\n",
    "Going back the MSE-loss gradient descent, the loss seemed to quickly decrease, but then level out. It's possible out threshold is too strict. Let us try with a threshold one order of magnitude larger, but let it run for a larger number of iterations, to see if it converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1356
    },
    "colab_type": "code",
    "id": "9Mx_yQjiNmJa",
    "outputId": "b055da42-9ec4-4975-c4ca-650798fb0e0a"
   },
   "outputs": [],
   "source": [
    "# step dataset / 1-node network : mse\n",
    "\n",
    "fig, axes = setup_figure(2, 1)\n",
    "\n",
    "## initial linear output weights\n",
    "w_out0 = 1\n",
    "b_out0 = 0\n",
    "hidden_nodes = 1\n",
    "\n",
    "## gradient descent parameters\n",
    "learning_rate_ = 10**-2\n",
    "threshold_ = 10**-4\n",
    "max_iter_ = 10**4\n",
    "\n",
    "show_opt_info(\n",
    "    datasets[STEP_DATA].x, # x,\n",
    "    datasets[STEP_DATA].y, # y,\n",
    "    plot_step, # dataset_plot\n",
    "    hidden_params[STEP_DATA]['w'], # ws,\n",
    "    hidden_params[STEP_DATA]['h'], # bs,\n",
    "    w_out0,\n",
    "    b_out0,\n",
    "    axes,\n",
    "    \"<em>{} vs Network Output</em><br/>\".format(dataset_titles[STEP_DATA]) +\n",
    "    \"{}-node hidden layer (manually tuned weights)<br />\".format(\n",
    "        hidden_nodes) +\n",
    "    \"Linear output layer (optimized model MSE)\", # title,\n",
    "    learning_rate=learning_rate_,\n",
    "    threshold=threshold_,\n",
    "    max_iter=max_iter_,\n",
    "    hidden_layer_size=hidden_nodes,\n",
    "    plot_steps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yM5L94ClNZ-X"
   },
   "source": [
    "At i = 1000 steps, it has not fallen beneath our threshold, but it is very close. Given the graph, it seems unlikely it will do so, so we may consider these  weights as close to the best we can do with these three methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5-qTOVjYNJyf"
   },
   "source": [
    "### One Hump Dataset : 2-node network\n",
    "\n",
    "Since the MSE seemed to be the best loss function for the Step Dataset, we will start with it, and run with a small number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1356
    },
    "colab_type": "code",
    "id": "Qj99j6MSYQMU",
    "outputId": "e2446747-6e96-4729-dcce-7839bcb329a5"
   },
   "outputs": [],
   "source": [
    "# one hump dataset / 2-node network\n",
    "\n",
    "fig, axes = setup_figure(2, 1)\n",
    "\n",
    "## initial linear output weights\n",
    "w_out0 = 1\n",
    "b_out0 = 0\n",
    "hidden_nodes = 2\n",
    "\n",
    "## gradient descent parameters\n",
    "learning_rate_ = 10**-2\n",
    "threshold_ = 10**-4\n",
    "max_iter_ = 10**3\n",
    "\n",
    "show_opt_info(\n",
    "    datasets[ONE_HUMP_DATA].x, # x,\n",
    "    datasets[ONE_HUMP_DATA].y, # y,\n",
    "    plot_one_hump, # dataset_plot\n",
    "    hidden_params[ONE_HUMP_DATA]['w'], # ws,\n",
    "    hidden_params[ONE_HUMP_DATA]['h'], # bs,\n",
    "    w_out0,\n",
    "    b_out0,\n",
    "    axes,\n",
    "    \"<em>{} vs Network Output</em><br/>\".format(dataset_titles[ONE_HUMP_DATA]) +\n",
    "    \"{}-node hidden layer (manually tuned weights)<br />\".format(hidden_nodes) +\n",
    "    \"Linear output layer (optimized model MSE)\", # title,\n",
    "    learning_rate=learning_rate_,\n",
    "    threshold=threshold_,\n",
    "    max_iter=max_iter_,\n",
    "    hidden_layer_size=hidden_nodes,\n",
    "    plot_steps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B1T0yRM5O6uw"
   },
   "source": [
    "After 1000 steps, the MSE loss has shrunk significantly, but the predictions look very bad. We will try MAE next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1356
    },
    "colab_type": "code",
    "id": "bPAFamLXPOCw",
    "outputId": "2634fa40-93e7-4807-f2e5-fc66ba034897"
   },
   "outputs": [],
   "source": [
    "# one hump dataset / 2-node network\n",
    "\n",
    "fig, axes = setup_figure(2, 1)\n",
    "\n",
    "## initial linear output weights\n",
    "w_out0 = 1\n",
    "b_out0 = 0\n",
    "hidden_nodes = 2\n",
    "\n",
    "## gradient descent parameters\n",
    "learning_rate_ = 10**-2\n",
    "threshold_ = 10**-4\n",
    "max_iter_ = 10**3\n",
    "\n",
    "show_opt_info(\n",
    "    datasets[ONE_HUMP_DATA].x, # x,\n",
    "    datasets[ONE_HUMP_DATA].y, # y,\n",
    "    plot_one_hump, # dataset_plot\n",
    "    hidden_params[ONE_HUMP_DATA]['w'], # ws,\n",
    "    hidden_params[ONE_HUMP_DATA]['h'], # bs,\n",
    "    w_out0,\n",
    "    b_out0,\n",
    "    axes,\n",
    "    \"<em>{} vs Network Output</em><br/>\".format(dataset_titles[ONE_HUMP_DATA]) +\n",
    "    \"{}-node hidden layer (manually tuned weights)<br />\".format(hidden_nodes) +\n",
    "    \"Linear output layer (optimized model MAE)\", # title,\n",
    "    loss_function=mae,\n",
    "    grad_function=grad_mae,\n",
    "    learning_rate=learning_rate_,\n",
    "    threshold=threshold_,\n",
    "    max_iter=max_iter_,\n",
    "    hidden_layer_size=hidden_nodes,\n",
    "    plot_steps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YHorYM-aPwhR"
   },
   "source": [
    "MAE does not perform well with the predictions, and bizzarely, the loss seems to be trending upwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1356
    },
    "colab_type": "code",
    "id": "2Qo8WJ9UP4Yg",
    "outputId": "233c4e0e-ce14-43e3-b959-67b1fa84f891"
   },
   "outputs": [],
   "source": [
    "# one hump dataset / 2-node network\n",
    "\n",
    "fig, axes = setup_figure(2, 1)\n",
    "\n",
    "## initial linear output weights\n",
    "w_out0 = 1\n",
    "b_out0 = 0\n",
    "hidden_nodes = 2\n",
    "\n",
    "## gradient descent parameters\n",
    "learning_rate_ = 10**-2\n",
    "threshold_ = 10**-4\n",
    "max_iter_ = 10**3\n",
    "\n",
    "show_opt_info(\n",
    "    datasets[ONE_HUMP_DATA].x, # x,\n",
    "    datasets[ONE_HUMP_DATA].y, # y,\n",
    "    plot_one_hump, # dataset_plot\n",
    "    hidden_params[ONE_HUMP_DATA]['w'], # ws,\n",
    "    hidden_params[ONE_HUMP_DATA]['h'], # bs,\n",
    "    w_out0,\n",
    "    b_out0,\n",
    "    axes,\n",
    "    \"<em>{} vs Network Output</em><br/>\".format(dataset_titles[ONE_HUMP_DATA]) +\n",
    "    \"{}-node hidden layer (manually tuned weights)<br />\".format(hidden_nodes) +\n",
    "    \"Linear output layer (optimized model MAE)\", # title,\n",
    "    loss_function=sigmoid_log_loss,\n",
    "    grad_function=grad_sigmoid_log_loss,\n",
    "    learning_rate=learning_rate_,\n",
    "    threshold=threshold_,\n",
    "    max_iter=max_iter_,\n",
    "    hidden_layer_size=hidden_nodes,\n",
    "    plot_steps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "efCHp4EUQGbm"
   },
   "source": [
    "The sigmoid-transformed log loss once again decreases slowly, but suddenly increases again and again with more steps. It doesn't seem to be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YHtrxxunQah3"
   },
   "source": [
    "Among our three loss function choices, none of them peformed very well. Although gradient descent seems to be working on minimizing MSE, it does not seem to correspond to meaningfully improved predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wi38XF90Q0hM"
   },
   "source": [
    "### Two Hump: 4-node network\n",
    "\n",
    "We will start by seeing how gradient descent does with MSE-loss in a small number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1356
    },
    "colab_type": "code",
    "id": "laEyBM9Py1WG",
    "outputId": "c4bc6e29-5d5e-4ce0-f8aa-e5837e6538fa"
   },
   "outputs": [],
   "source": [
    "# two hump dataset / 4-node network\n",
    "\n",
    "fig, axes = setup_figure(2, 1)\n",
    "\n",
    "## initial linear output weights\n",
    "w_out0_ = 1\n",
    "b_out0_ = 0\n",
    "hidden_nodes_ = 4\n",
    "learning_rate_ = 10**-2\n",
    "threshold_ = 10**-5\n",
    "max_iter_ = 10**3\n",
    "\n",
    "show_opt_info(\n",
    "    datasets[TWO_HUMP_DATA].x, # x,\n",
    "    datasets[TWO_HUMP_DATA].y, # y,\n",
    "    plot_one_hump, # dataset_plot\n",
    "    hidden_params[TWO_HUMP_DATA]['w'], # ws,\n",
    "    hidden_params[TWO_HUMP_DATA]['h'], # bs,\n",
    "    w_out0,\n",
    "    b_out0,\n",
    "    axes,\n",
    "    \"<em>{} vs Network Output</em><br/>\".format(dataset_titles[TWO_HUMP_DATA]) +\n",
    "    \"{}-node hidden layer (manually tuned weights)<br />\".format(hidden_nodes) +\n",
    "    \"Linear output layer (optimized model MSE)\", # title,\n",
    "    learning_rate=learning_rate_,\n",
    "    threshold=threshold_,\n",
    "    max_iter=max_iter_,\n",
    "    hidden_layer_size=hidden_nodes,\n",
    "    plot_steps=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OfglhtNYRNKr"
   },
   "source": [
    "Again, minimizing MSE does not meaningfully improve the predictions. In the case of this data, the MSE might not be the right inverse measure of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1356
    },
    "colab_type": "code",
    "id": "YqxSeo47Ro4H",
    "outputId": "2a3b381a-3181-473e-dbc3-06ecf6054790"
   },
   "outputs": [],
   "source": [
    "# two hump dataset / 4-node network\n",
    "\n",
    "fig, axes = setup_figure(2, 1)\n",
    "\n",
    "## initial linear output weights\n",
    "w_out0_ = 1\n",
    "b_out0_ = 0\n",
    "hidden_nodes_ = 4\n",
    "learning_rate_ = 10**-2\n",
    "threshold_ = 10**-5\n",
    "max_iter_ = 10**3\n",
    "\n",
    "show_opt_info(\n",
    "    datasets[TWO_HUMP_DATA].x, # x,\n",
    "    datasets[TWO_HUMP_DATA].y, # y,\n",
    "    plot_one_hump, # dataset_plot\n",
    "    hidden_params[TWO_HUMP_DATA]['w'], # ws,\n",
    "    hidden_params[TWO_HUMP_DATA]['h'], # bs,\n",
    "    w_out0,\n",
    "    b_out0,\n",
    "    axes,\n",
    "    \"<em>{} vs Network Output</em><br/>\".format(dataset_titles[TWO_HUMP_DATA]) +\n",
    "    \"{}-node hidden layer (manually tuned weights)<br />\".format(hidden_nodes) +\n",
    "    \"Linear output layer (optimized model MSE)\", # title,\n",
    "    loss_function=mae,\n",
    "    grad_function=grad_mae,\n",
    "    learning_rate=learning_rate_,\n",
    "    threshold=threshold_,\n",
    "    max_iter=max_iter_,\n",
    "    hidden_layer_size=hidden_nodes,\n",
    "    plot_steps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1356
    },
    "colab_type": "code",
    "id": "lud4yutPRwRk",
    "outputId": "811cd7e8-72b2-4083-83f5-6291348ce5ce"
   },
   "outputs": [],
   "source": [
    "# two hump dataset / 4-node network\n",
    "\n",
    "fig, axes = setup_figure(2, 1)\n",
    "\n",
    "## initial linear output weights\n",
    "w_out0_ = 1\n",
    "b_out0_ = 0\n",
    "hidden_nodes_ = 4\n",
    "learning_rate_ = 10**-2\n",
    "threshold_ = 10**-5\n",
    "max_iter_ = 10**4\n",
    "\n",
    "show_opt_info(\n",
    "    datasets[TWO_HUMP_DATA].x, # x,\n",
    "    datasets[TWO_HUMP_DATA].y, # y,\n",
    "    plot_one_hump, # dataset_plot\n",
    "    hidden_params[TWO_HUMP_DATA]['w'], # ws,\n",
    "    hidden_params[TWO_HUMP_DATA]['h'], # bs,\n",
    "    w_out0,\n",
    "    b_out0,\n",
    "    axes,\n",
    "    \"<em>{} vs Network Output</em><br/>\".format(dataset_titles[TWO_HUMP_DATA]) +\n",
    "    \"{}-node hidden layer (manually tuned weights)<br />\".format(hidden_nodes) +\n",
    "    \"Linear output layer (optimized model MSE)\", # title,\n",
    "    loss_function=sigmoid_log_loss,\n",
    "    grad_function=grad_sigmoid_log_loss,\n",
    "    learning_rate=learning_rate_,\n",
    "    threshold=threshold_,\n",
    "    max_iter=max_iter_,\n",
    "    hidden_layer_size=hidden_nodes,\n",
    "    plot_steps=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-7kpejLhSbg7"
   },
   "source": [
    "None of the loss function choices were useful in gradient descent in these cases. For the MAE and the sigmoid-transformed log loss, further steps sometimes drastically increased the loss. They do not seem like reliable measures in the case of any of the datasets, and indeed we chose them for experimentation, without being clearly aware of their functional behavior, or the behavior of their derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TTQrf3suSloD"
   },
   "source": [
    "Gradient descent with MSE loss worked the best with the single step data, the dataset that most closely resembled a linear response.\n",
    "\n",
    "It is possible that a different transformation of inputs into log-loss, such as softmax log-loss, would have performed well in all three cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CK-kfHV4ndFB"
   },
   "source": [
    "## Part 2: Working with missing data\n",
    "\n",
    "In this part we are going to use the **Pima Indians onset of diabetes** dataset found in `pima-indians-diabetes.csv`. This dataset describes patient medical record data for Pima Indians and whether they had an onset of diabetes within five years. It is a binary classification problem (onset of diabetes as 1 or not as 0). The input variables that describe each patient are numerical and have varying scales. The list below shows the eight attributes plus the target variable for the dataset:\n",
    "\n",
    "- Number of times pregnant.\n",
    "- Plasma glucose concentration a 2 hours in an oral glucose tolerance test.\n",
    "- Diastolic blood pressure (mm Hg).\n",
    "- Triceps skin fold thickness (mm).\n",
    "- 2-Hour serum insulin (mu U/ml).\n",
    "-  Body mass index.\n",
    "-  Diabetes pedigree function.\n",
    "- Age (years).<br>\n",
    "- **Outcome** (1 for early onset of diabetes within five years, 0 for not), target class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zq7atqM_ndFH"
   },
   "source": [
    "**2.1**. Load the dataset into a pandas dataframe named `pima_df`. Clean the data by looking at the various features and making sure that their values make sense. Look for missing data including disguised missing data. The problem of disguised missing data arises when missing data values are not explicitly represented as such, but are coded with values that can be misinterpreted as valid data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hYu2hLN1tyXk"
   },
   "source": [
    "Let's first read the data into a dataframe and look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9l0Lhn-ondFJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "pima_df = pd.read_csv('data/pima-indians-diabetes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "7-ApDTTindFO",
    "outputId": "588873b6-3b55-44e2-eb82-6e55d8ce9320"
   },
   "outputs": [],
   "source": [
    "pima_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "etimCK5tt6QA",
    "outputId": "44f28447-d1a1-43ec-b4f4-073b5302e00e"
   },
   "outputs": [],
   "source": [
    "display(\"The data has {} observations and {} features.\".format(\n",
    "    pima_df.shape[0],\n",
    "    pima_df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "cUiikavWt8dQ",
    "outputId": "c4247a36-955c-482d-c0f2-a578d4c4d069"
   },
   "outputs": [],
   "source": [
    "response = 'Outcome'\n",
    "predictors = pima_df.columns.difference([response]).values\n",
    "print(\"The response variable is {}. \\nThe predictors are: {}\".format(\n",
    "    response,\n",
    "    predictors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lnVdmaHwUQ9s"
   },
   "source": [
    "We immediately see strange values for some of the predictors. Pregnancies range up to 17. The blood pressure, glucose, skin thickness, insulin, and BMI variables include zeros, which are physically implausible. We will have to handle these somehow.\n",
    "\n",
    "First, we will look at null and na values in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "nGtjBnRzuBTU",
    "outputId": "3d8f5867-c6f0-48d9-852a-ce7efd5cf616"
   },
   "outputs": [],
   "source": [
    "pima_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "dx1JYQRAuDEO",
    "outputId": "3e988a98-7836-4108-aa68-bf94c23bbfb5"
   },
   "outputs": [],
   "source": [
    "pima_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dnTqAst7t_D-"
   },
   "source": [
    "Although missing values are usually coded using NaN, Null, or None. It seems that none of the observations are clearly marked in this fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9SE9u0w5VgQu"
   },
   "source": [
    "Before we move on, let us look at possible interrelations between all the different variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 734
    },
    "colab_type": "code",
    "id": "uBLpss_anU0Q",
    "outputId": "78e0baa6-c7a1-4f9a-9978-c0d916bb957b"
   },
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(pima_df, figsize=(12,12))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8YHqgKhpbnsi"
   },
   "source": [
    "We note that SkinThinkness is strongly correlated to BMI. Insulin is positively correlated to Glucose level.\n",
    "\n",
    "There is a counterintuitive pattern with pregnancies and age. We can look more closely with a 2D historgram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 540
    },
    "colab_type": "code",
    "id": "zljt6HVendFt",
    "outputId": "efc94994-f103-46fa-f7b0-c5038d570d65"
   },
   "outputs": [],
   "source": [
    "fig, ax = setup_figure(1, 1, 6, 4)\n",
    "\n",
    "ax.hist2d(pima_df.Age, pima_df.Pregnancies, bins=6)\n",
    "ax.set_xlabel(\"Age\")\n",
    "ax.set_ylabel(\"Pregnancy Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5W0pQPMMWSL8"
   },
   "source": [
    "We would expect pregancy count to rise and remain high for higher ages, but instead we see that density is highest in the wengest age range, and that it drops. It is possible that pregnancies variable is coded incorrectly, or is measuring something completely different from cumulative pregnancies.\n",
    "\n",
    "Although it makes the variable as descibed suspicious, it is likely a measurement or coding problem rather than a missing data issue.\n",
    "\n",
    "We can turn the Pregnancies feature into a binary variable - 1 for having at least 1 pregnancy and 0 otherwise. By doing so it will be ambiguous how to interprete the value 0, which could either mean 'no pregnancy' or 'missing value'. We would loose the quantitative contribution of the count of pregnancies to the prediction of diabetes unset.  The same might happen if we choose to remove the Pregnancies feature completely.\n",
    "\n",
    "We prefer to standardize the pregnancy values betwen 0 and 1 and thus keeping the original variance and improving regularized regression modeling later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dgWCxkFzuKeU"
   },
   "source": [
    "In terms of missing data, the variables we need to look at most closely are Glucose, BloodPressure, SkinThickness, Insulin, and BMI, all of which contain 0 among their observations.\n",
    "\n",
    "A quick search in the litterature shows that these features cannot have a physiological value of zero. The most plausible explanation is that mssing observations for features were missing were somehow replaced with zero. \n",
    "\n",
    "This disguised missing data would mislead our later classification attempts. We will clean the data by marking disguised missing values clearly as NaN. \n",
    "\n",
    "\n",
    "The response variable, which should be coded as 0 or 1, contained values with \\ or } appended to the 0's and 1s. It looks like an error similar to those introduced when reading from or writing CSVs have found their way into the data. The solution we choose is simply to remove the characters.\n",
    "\n",
    "Looking at the data types, predictors are stored as float and the response as an object. The float data type makes sense for BMI and DiabetesPedigreeFunction. The remaining features can be stored as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QpO-1xw1uPgw",
    "outputId": "393824f1-05ec-4ee3-fef7-d5e7167b1a47"
   },
   "outputs": [],
   "source": [
    "pima_df.Pregnancies.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BI7srKMjuZNo"
   },
   "source": [
    "The following function cleans the data and replaces zeros with NaNs for the five columns discussed: `Glucose`, `BloodPressure`, `SkinThickness`, `Insulin`, and `BMI`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2HAF6VGhubQ8"
   },
   "outputs": [],
   "source": [
    "def clean_data(df_raw, \n",
    "               cols_with_zeros=['Glucose', 'BloodPressure',\n",
    "                                'SkinThickness', 'Insulin', 'BMI'], \n",
    "               response = ['Outcome']):\n",
    "    df = df_raw.copy()\n",
    "    # replace zero with NaN in features\n",
    "    df[cols_with_zeros] = df[cols_with_zeros].replace(0, np.nan)\n",
    "    # remove \\ and } from response\n",
    "    df = df.replace(to_replace=r'\\\\|\\}', value='', regex=True)\n",
    "    # change response data type to int\n",
    "    df[response] = df[response].astype('int')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V0NMGQ0mRUWq"
   },
   "source": [
    "Let's test our code for data cleaning. Below are the first few rows of observations for illustration. We have replaced any zero we have deciced that actually indicate missing values with NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "HUkyGrnUueYE",
    "outputId": "fee7c69e-849c-4037-874f-510dba79c0aa"
   },
   "outputs": [],
   "source": [
    "pima_df_cleaned = clean_data(pima_df)\n",
    "pima_df_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u8V3fjQOSCv6"
   },
   "source": [
    "We can calculate the proportion of missing values for each feature. 48.56% of Insulin, 29.58% of SkinThickness, 4.58% of BloodPressure, 1.43% of BMI and 0.65% of Glucose. The remaining features do not have any missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "uj20AFtVRfv6",
    "outputId": "8e2a9c1c-3fb0-4411-c7b1-f8e7de608ddf"
   },
   "outputs": [],
   "source": [
    "print(\"Proportion of missing values\")\n",
    "missing_values_count = (pima_df_cleaned.isna().sum()*100 /\n",
    "                        pima_df_cleaned.shape[0])\n",
    "missing_values_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Bm7IWVIxT4LY",
    "outputId": "6e68dcdf-2a0c-4d6d-d7cd-b51eb02a7c4b"
   },
   "outputs": [],
   "source": [
    "features_with_missing_values = missing_values_count[\n",
    "    missing_values_count>0].index.values\n",
    "features_with_missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-wLy779PWoWG"
   },
   "source": [
    "By looking closer, it seems that missing values for SkinThickness are correlated with missing values for Insulin. When SkinThickness is missing, then Insulin is also missing.  \n",
    "\n",
    "Furthermore when BloodPressure or BMI is missing, then the probability is higher that Insulin or SkinThickness values will be missing as well.\n",
    "\n",
    "We might need to perform some statistical tests of the hypothesis that the features are Missing at Random (MAR), Missing Completely at Random (MCAR) or Missing not at Random (MNAR).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "colab_type": "code",
    "id": "6bblHhTqWoti",
    "outputId": "8b0e6b0e-a89e-4f1c-f6c8-7a039ed9de1f"
   },
   "outputs": [],
   "source": [
    "pima_df[(pima_df.SkinThickness==0) & (pima_df.Insulin!=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "Vd6Pji43YoaQ",
    "outputId": "fd642b0f-1bf1-4271-e5b2-c7d39e428c51"
   },
   "outputs": [],
   "source": [
    "pima_df_cleaned[pima_df_cleaned.Insulin.isna()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FVPdnK2mndFS"
   },
   "source": [
    "**2.2** Split the dataset into a 75-25 train-test split (use `random_state=9001`). Fit a logistic regression classifier to the training set and report the  accuracy of the classifier on the test set. We should use $L_2$ regularization in logistic regression, with the regularization parameter tuned using cross-validation (`LogisticRegressionCV`).  Report the overall classification rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1FvmzrTTwcNE"
   },
   "source": [
    "In the previous section we clearly marked missing data with NaN.\n",
    "\n",
    "We can now either drop or impute those missing data. The function below can do either, and also supports mean and model-based imputation using KNN regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pge6hVxhucnc"
   },
   "outputs": [],
   "source": [
    "# function for KNN model-based imputation of missing values using features without NaN as predictors\n",
    "def impute_model_basic(df):\n",
    "  cols_nan = df.columns[df.isna().any()].tolist()    \n",
    "  cols_no_nan = df.columns.difference(cols_nan).values            \n",
    "  for col in cols_nan:\n",
    "      test_data = df[df[col].isna()]\n",
    "      train_data = df.dropna()\n",
    "      knr = KNeighborsRegressor(n_neighbors=5).fit(\n",
    "          train_data[cols_no_nan],\n",
    "          train_data[col])\n",
    "      df.loc[df[col].isna(), col] = knr.predict(test_data[cols_no_nan])\n",
    "  return df\n",
    "\n",
    "# function for KNN model-based imputation of missing values using features without NaN as predictors, including progressively added imputed features\n",
    "def impute_model_progressive(df):\n",
    "  cols_nan = df.columns[df.isna().any()].tolist()    \n",
    "  cols_no_nan = df.columns.difference(cols_nan).values            \n",
    "  while len(cols_nan)>0:\n",
    "      col = cols_nan[0]\n",
    "      test_data = df[df[col].isna()]\n",
    "      train_data = df.dropna()\n",
    "      knr = KNeighborsRegressor(n_neighbors=5).fit(\n",
    "          train_data[cols_no_nan], train_data[col])\n",
    "      df.loc[df[col].isna(), col] = knr.predict(test_data[cols_no_nan])\n",
    "      cols_nan = df.columns[df.isna().any()].tolist()    \n",
    "      cols_no_nan = df.columns.difference(cols_nan).values\n",
    "  return df\n",
    "\n",
    "# function for imputing missing data according to a given impute_strategy:\n",
    "#  drop_rows: drop all rows with one or more missing values\n",
    "#  drop_cols: drop columns with one or more missing values\n",
    "#  model_basic: KNN-model-based imputation with fixed predictors\n",
    "#  model_progressive: KNN-model-based imputation with\n",
    "#   progressively added predictors\n",
    "#  mean, median, most_frequent: imputation with mean, median\n",
    "#   or most frequent values\n",
    "#\n",
    "#  cols_to_standardize: if provided, the specified columns are scaled\n",
    "#   between 0 and 1, after imputation\n",
    "def impute_data(df_cleaned, impute_strategy=None, cols_to_standardize=None):\n",
    "    df = df_cleaned.copy()\n",
    "    if impute_strategy == 'drop_rows':\n",
    "      df = df.dropna(axis=0)\n",
    "    elif impute_strategy == 'drop_cols':\n",
    "      df = df.dropna(axis=1)\n",
    "    elif impute_strategy == 'model_basic':\n",
    "      df = impute_model_basic(df)\n",
    "    elif impute_strategy == 'model_progressive':\n",
    "      df = impute_model_progressive(df) \n",
    "    else:\n",
    "      # print(\"impute strategy: \".format(impute_strategy))\n",
    "      arr = SimpleImputer(missing_values=np.nan,\n",
    "                          strategy=impute_strategy).fit(\n",
    "                              df.values).transform(df.values)\n",
    "      df = pd.DataFrame(data=arr, index=df.index.values,\n",
    "                        columns=df.columns.values)\n",
    "      # print(df_cleaned.columns.difference(df.columns))\n",
    "    if cols_to_standardize != None:\n",
    "      cols_to_standardize = list(set(cols_to_standardize) & set(df.columns.values))\n",
    "      df[cols_to_standardize] = df[cols_to_standardize].astype('float')\n",
    "      df[cols_to_standardize] = pd.DataFrame(data=MinMaxScaler().fit(\n",
    "          df[cols_to_standardize]).transform(df[cols_to_standardize]), \n",
    "                                 index=df[cols_to_standardize].index.values,\n",
    "                                 columns=df[cols_to_standardize].columns.values)\n",
    "      # print(\"After standardization: \", df_cleaned.columns.difference(df.columns))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rlWVLNP-ZgFI"
   },
   "source": [
    "The drop strategy remove all observations where at least one of the features has a missing value (NaN). \n",
    "\n",
    "The mean strategy replace any missing value (NaN) by the mean of all values available for that feature.\n",
    "\n",
    "The model-based strategy uses the features without missing values for training KNN regression models. We choose KNN in order to capture the variability of available data. This would not be the case if we would use a linear regression model that would predict missing values along a regression line. We distinguish between two modes in this model-based strategy. The following features are used as predictors in the basic mode: Age, DiabetesPedigreeFunction, Outcome, Pregnancies. The fitted model are used to predict the missing values in the remaining features. In the progressive mode, after we fill in missing values in a given feature, we consider the feature as a predictor for estimating the missing values of the next feature.\n",
    "\n",
    "The imputed data can be optionaly standardized between 0 and 1. This might improved the performance of classification using regularized logistic regression. Because the features are on different scale (e.g. Age vs Insulin) and their values range differs (e.g. Pedigree vs Insulin), shrinkage penalty could be wrongly calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GE6-JMbwuVXK"
   },
   "source": [
    "### Logistic Regression\n",
    "\n",
    "The following function fits a logistic regression model on the cleaned data after applying a given imputation strategy: drop rows missing values, impute missing values with column mean, impute missing values with model-based prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cm0_WJldndFU"
   },
   "outputs": [],
   "source": [
    "def logistic_regression(data, impute_strategy=None,\n",
    "                        cols_to_standardize=None,\n",
    "                        test_size=0.25,\n",
    "                        random_state=9001):\n",
    "    start = timer()\n",
    "    \n",
    "    # store original columns\n",
    "    original_columns = data.columns.difference(['Outcome'])\n",
    "    df_imputed = impute_data(data, impute_strategy, cols_to_standardize)\n",
    "    train_data, test_data = train_test_split(df_imputed, test_size=test_size,\n",
    "                                             random_state=random_state)\n",
    "    \n",
    "    # note which predictor columns were dropped or kept\n",
    "    kept_columns = df_imputed.columns.difference(['Outcome'])\n",
    "    dropped_columns = original_columns.difference(df_imputed.columns)\n",
    "    original_columns = original_columns.difference(['Outcome'])\n",
    "\n",
    "    X_train = train_data.drop(columns=['Outcome'])\n",
    "    y_train = train_data['Outcome']\n",
    "    X_test = test_data.drop(columns=['Outcome'])\n",
    "    y_test = test_data['Outcome']\n",
    "    logistic_model = LogisticRegressionCV(cv=10, penalty='l2', max_iter=1000).fit(\n",
    "        X_train, y_train)\n",
    "    train_score = accuracy_score(y_train, logistic_model.predict(X_train))\n",
    "    test_score = accuracy_score(y_test, logistic_model.predict(X_test))\n",
    "    duration = timer() - start\n",
    "    print(\"Classification rate on training data: {}\".format(train_score))\n",
    "    print(\"Classification rate on test data: {}\".format(test_score))\n",
    "    print(\"Execution time: {}\".format(duration))\n",
    "    return {\n",
    "        'imputation strategy': impute_strategy,\n",
    "        'standardized': cols_to_standardize!=None,\n",
    "        'model': logistic_model,\n",
    "        'train score': train_score,\n",
    "        'test score': test_score,\n",
    "        'execution time (s)': duration,\n",
    "        # '_columns' : [original_columns, kept_columns, dropped_columns]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gjU5NTZkz5lg"
   },
   "source": [
    "Let's fit a logistic regression model on the cleaned data after **dropping rows with missing values** and **without  and with standardizing** predictors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "tnmBL9rM0H_i",
    "outputId": "d02abed3-1ae5-47a5-9912-4c2febb953cd"
   },
   "outputs": [],
   "source": [
    "lr_results = []\n",
    "pima_df_cleaned = clean_data(pima_df)\n",
    "result = logistic_regression(pima_df_cleaned,\n",
    "                             impute_strategy='drop_rows',\n",
    "                             cols_to_standardize=None)\n",
    "lr_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "sgPR-VlRDJpY",
    "outputId": "287afa8c-758d-4f55-f812-ca7ee88de657"
   },
   "outputs": [],
   "source": [
    "cols_to_standardize=['Age','BMI','BloodPressure','Glucose',\n",
    "                     'Insulin','Pregnancies','SkinThickness',\n",
    "                     'DiabetesPedigreeFunction']\n",
    "result = logistic_regression(pima_df_cleaned,\n",
    "                             impute_strategy='drop_rows',\n",
    "                             cols_to_standardize=cols_to_standardize)\n",
    "lr_results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4G2n9keffQhY"
   },
   "source": [
    "We also fit a logistic regression model on the cleaned data after **dropping columns with missing values** and **without  and with standardizing** predictors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "PQf1GuhlfLQ4",
    "outputId": "d38b4f35-a846-4ee7-f7a5-82af95b6bdc6"
   },
   "outputs": [],
   "source": [
    "result = logistic_regression(pima_df_cleaned,\n",
    "                             impute_strategy='drop_cols',\n",
    "                             cols_to_standardize=None)\n",
    "lr_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "F-ULVYkkDVuu",
    "outputId": "18754ea2-1659-44fd-86d1-c7e00c20e400"
   },
   "outputs": [],
   "source": [
    "result = logistic_regression(pima_df_cleaned,\n",
    "                             impute_strategy='drop_cols',\n",
    "                             cols_to_standardize=cols_to_standardize)\n",
    "lr_results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N8yxif_5ejaE"
   },
   "source": [
    "Dropping columns with missing value produces the worse classification accuracy score and would not be wise to do if we care about prediction performance when detecting the onset of diabetes. Dropping rows with missing values provides better prediction, but as we see in the next section, imputation is highly recommended instead. Standardizing features in the scale 0 to 1 has a slightly negative impact on test accuracy score. Looking at the coefficients we gain in better interpretability after standardizing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pYQwI7x8ndFX"
   },
   "source": [
    "**2.3** Restart with a fresh copy of the whole dataset and impute the missing data via mean imputation.  Split the data 75-25 (use `random_state=9001`) and fit a regularized logistic regression model.  Report the overall classification rate.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qUJniCqd3tH0"
   },
   "source": [
    "Let's now fit a logistic regression model on the cleaned data after **imputing missing values with their feature's mean** and **without standardizing predictors**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "gPxIltKqndFY",
    "outputId": "dfd5e96d-e6e8-4c61-fb07-3ce6e5be343d"
   },
   "outputs": [],
   "source": [
    "pima_df_cleaned = clean_data(pima_df)\n",
    "result = logistic_regression(pima_df_cleaned,\n",
    "                             impute_strategy='mean',\n",
    "                             cols_to_standardize=None)\n",
    "lr_results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "17ovZyftlbIi"
   },
   "source": [
    "Next we fit a logistic regression model on the cleaned data after **imputing missing values with their feature's mean** and **with standardizing predictors**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "vuOy-IznKOSM",
    "outputId": "1c75d1b0-8feb-4d9c-e062-b44507ebdeb4"
   },
   "outputs": [],
   "source": [
    "pima_df_cleaned = clean_data(pima_df)\n",
    "result = logistic_regression(pima_df_cleaned,\n",
    "                             impute_strategy='mean',\n",
    "                             cols_to_standardize=cols_to_standardize)\n",
    "lr_results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CTl7Fqc3ndFc"
   },
   "source": [
    "**2.4** Again restart with a fresh copy of the whole dataset and impute the missing data via a model-based imputation method. Once again split the data 75-25 (same `random_state=9001`) and fit a regularized logistic regression model.  Report the overall classification rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LTKc62dm4Jie"
   },
   "source": [
    "Here we fit a logistic regression model on the cleaned data after **imputing missing values with predictions made by a KNN regression model** in the **basic mode** and **without and with standardizing predictors**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "Br5TQ664ndFj",
    "outputId": "7058e79a-937b-4811-ea5d-0b227723f388"
   },
   "outputs": [],
   "source": [
    "pima_df_cleaned = clean_data(pima_df)\n",
    "result = logistic_regression(pima_df_cleaned,\n",
    "                             impute_strategy='model_basic',\n",
    "                             cols_to_standardize=None)\n",
    "lr_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "N7nVk8elBg6o",
    "outputId": "6eb507fd-7f71-4a09-9c8f-1d6f058e50eb"
   },
   "outputs": [],
   "source": [
    "pima_df_cleaned = clean_data(pima_df)\n",
    "result = logistic_regression(pima_df_cleaned,\n",
    "                             impute_strategy='model_basic',\n",
    "                             cols_to_standardize=cols_to_standardize)\n",
    "lr_results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uYeebRuamJwo"
   },
   "source": [
    "Finally we fit a logistic regression model on the cleaned data after **imputing missing values with predictions made by a KNN regression model** in the **progressive mode** and **with and without standardizing predictors**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "ARCrldAumLKe",
    "outputId": "85dfe0a3-8e42-4640-ec7f-854b2d1d5ee9"
   },
   "outputs": [],
   "source": [
    "result = logistic_regression(pima_df_cleaned,\n",
    "                             impute_strategy='model_progressive',\n",
    "                             cols_to_standardize=None)\n",
    "lr_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "H-IpFlfOCvMc",
    "outputId": "2ff2a50b-207a-41ae-9e5f-cb045b7c55e3"
   },
   "outputs": [],
   "source": [
    "result = logistic_regression(pima_df_cleaned,\n",
    "                             impute_strategy='model_progressive',\n",
    "                             cols_to_standardize=cols_to_standardize)\n",
    "lr_results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m2NVaPbL57cQ"
   },
   "source": [
    "**2.5** Compare the results in the 3 previous parts of this problem. Discuss the results, the computational complexity of the methods,  and explain why we get the results that we see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aGDhfVuCnAZ4"
   },
   "source": [
    "We recall the classification accuracy scores obtained on train and test datasets when using different missing data handling strategies in the table below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "lmMfSk2d_nDw",
    "outputId": "6dc69d21-5048-4a2a-a77b-2ab42f50c526"
   },
   "outputs": [],
   "source": [
    "lr_results_df_full = pd.DataFrame(lr_results)\n",
    "lr_results_df = lr_results_df_full.copy()\n",
    "lr_results_df.drop(['model'], axis=1).drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xmPUtdGlCbsq"
   },
   "source": [
    "By dropping rows or columns with missing values we lose valuable information that might have significant impact on the response variable. The consequence is overfitting in the training dataset and bad prediction performance on test. \n",
    "\n",
    "Mean imputation of missing data reduces overfitting and improves the prediction on test data. Classification is the best when a model-based is used when imputing missing data. This is because the original variance of the data is better approached when using k-nearest neighbors as a replacement of missing data. \n",
    "\n",
    "The computational complexity is assessed by measuring the cummulative execution time of imputation, logistic regression model fitting and prediction. The execution time for the model-based approach is the highest when predictors are not standardized. Calculating eucliding distance to nearest neighbors requires more execution time than claculating the mean of data. When dealing with a very large number of observations, we might prefer mean imputation at the cost of lower classification accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R0hvDHgfndFp"
   },
   "source": [
    "**2.6** Check which coefficients changed the most between the model in 2.1-2.2 and the models in 2.3 and 2.4. Are they the coefficients we expected to change given the imputation we performed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MGtEutBM4aso"
   },
   "source": [
    "We retrieve the coefficients estimated by our three regularized logistic regression models as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "colab_type": "code",
    "id": "F2uke8T3Is-4",
    "outputId": "3c5fe10d-13bb-4da7-c8b3-a515a895d1bf"
   },
   "outputs": [],
   "source": [
    "# get index of strategies\n",
    "strategies = lr_results_df['imputation strategy']\n",
    "\n",
    "# get a boolean array where True => standardized\n",
    "standardized = lr_results_df['standardized']\n",
    "st = lambda s: ' standardized' if s else ''\n",
    "coefs_ = {}\n",
    "for key, value in enumerate(strategies):\n",
    "  if value == 'drop_cols':\n",
    "    # skip\n",
    "    pass\n",
    "  else:\n",
    "    strategy = value + st(standardized[key])\n",
    "    coefs_[strategy] = lr_results_df['model'][key].coef_[0]\n",
    "coef_df = pd.DataFrame(data=coefs_, index=predictors)\n",
    "coef_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SEuZFD2otCRa"
   },
   "source": [
    "The following table compares the effect of mean imputation and model-based imputation on the coefficient magnitude obtained after dropping rows with missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "i1Lqfo9qS2Bu",
    "outputId": "4f33c2ba-f9e0-46b0-a8bf-539e10b9d819"
   },
   "outputs": [],
   "source": [
    "coef_perc_df = coef_df.copy()\n",
    "cols = coef_df.columns.difference(['drop_rows']).values\n",
    "for col in cols:\n",
    "  coef_perc_df[col] = np.round(100*(coef_df[col] /\n",
    "                                    coef_df['drop_rows']-1))\n",
    "coef_perc_df[['drop_rows','mean','model_basic','model_progressive']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t4ivU3yEndFq"
   },
   "source": [
    "The first column shows the coefficient estimates for the logistic model trained on data where rows with missing values where removed. The second row shows the percentage change of coefficients values using a logistic classification model on the data after mean imputation compared to the first model. The last two columns show the  percentage change when KNN model-based imputation is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mYo3ttZxn6HO"
   },
   "source": [
    "Additionally, we can come to the following conclusions:\n",
    "\n",
    "*   There is no obvious correlation between Age, BloodPressure and DiabetesPedigreeFunction and the onset of diabetes. When mean or model-based imputation is used, the coefficient estimates for Age, BloodPressure and DiabetesPedigreeFunction change drastically, compared to their values when missing data is simply dropped.\n",
    "\n",
    "*   Coefficients for Insulin do not vary much between imputation methods. This suggest that Insulin might be Missing Completely At Random (MCAR).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ip7dBcKYb_Jy"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cs109a_hw6_109_submit.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
