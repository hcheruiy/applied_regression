{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANNs\n",
    "\n",
    "Hillary Cheruiyot\n",
    "\n",
    "\n",
    "<hr style=\"height:2pt\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(112358)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "import tensorflow.keras \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "pd.set_option('display.width', 1500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.constraints import maxnorm\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style='height:2pt'>\n",
    "\n",
    "**Neural Networks** \n",
    "\n",
    "We'll focus on the key idea of NNs: they are able to learn a mapping from example input data (of fixed size) to example output data (of fixed size). We'll also partially explore what patterns the neural network learns and how well they generalize.\n",
    "\n",
    "In this project we'll see if Neural Networks can learn a (limited) version of the Fourier Transform. (The Fourier Transform takes in values from some function and returns a set of sine and cosine functions which, when added together, approximate the original function.)\n",
    "\n",
    "In symbols: $ \\cal{F(s)} = \\int_{-\\infty}^\\infty f(x)e^{-2\\pi i xs}dx$. In words, the value of the transformed function at some point, $s$, is the value of an integral which measures, in some sense, how much the original f(x) looks like a wave with period s. As an example, with $f(x) = 4cos(x) + sin(2x)$,  $\\cal{F}(s)$ is 0 everywhere except at -2, -1, 1, and 2, mapping to the waves of period 1 and 1/2. The values at these points are linked to the magnitude of the waves, and their phases (roughly: sin waves versus cosine waves).\n",
    "\n",
    "The only thing about the Fourier transform that matters for this project is this: function goes in, re-write in terms of sine and cosine comes out.\n",
    "\n",
    "In our specific problem, we'll train a network to map from 1000 sample values from a function (equally spaced along 0 to 2$\\pi$) to the four features of the sine and cosine waves that make up that function. Thus, the network is attempting to learn a mapping from a 1000-entry vector down to a 4-entry vector. Our X_train dataset is thus N by 1000 and our y_train is N by 4.\n",
    "\n",
    "We'll use 6 data files:\n",
    "- `sinewaves_X_train.npy` and `sinewaves_y_train.npy`: a (10,000 by 1,000) and (10,000 by 4) training dataset. Examples were generated by randomly selecting a,b,c,d in the interval [0,1] and building the curve $a\\sin(b\\,x) + c\\cos(d\\,x)$\n",
    "- `sinewaves_X_test.npy` and `sinewaves_y_test.npy`: a (2,000 by 1,000) and (2,000 by 4) test dataset, generated in the same way as the training data\n",
    "- `sinewaves_X_extended_test` and `sinewaves_y_extended_test`: a (9 by 1,000) and (9 by 4) test dataset, testing whether the network can generalize beyond the training data (e.g. to negative values of $a$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('data/sinewaves_X_train.npy')\n",
    "y_train = np.load('data/sinewaves_y_train.npy')\n",
    "\n",
    "X_test = np.load('data/sinewaves_X_test.npy')\n",
    "y_test = np.load('data/sinewaves_y_test.npy')\n",
    "\n",
    "X_extended_test = np.load('data/sinewaves_X_extended_test.npy')\n",
    "y_extended_test = np.load('data/sinewaves_y_extended_test.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1** Plot the first row of the `X_train` training data and visually verify that it is a sinusoidal curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_X(X, i):\n",
    "    x = np.linspace(0, 10*np.pi,X.shape[1])\n",
    "    plt.scatter(x, X[i])\n",
    "    plt.axhline(y=0)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('X')\n",
    "    plt.title('sinusoidal curve')\n",
    "    \n",
    "plot_X(X_train, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows a sinusoidal cuve of amplitude around 0.02, wavelength around 12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2** The first row of the `y_train` data is $[0.024, 0.533, 0.018, 0.558]$. Visually or numerically verify that the first row of X_train is 1000 equally-spaced samples in $[0,10\\pi]$ from the function $f(x) = 0.024\\sin(0.533\\,x) + 0.018\\cos(0.558\\,x)$. This pattern (y_train is the true parameters of the curve in X_train) will always hold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def f(x, y):\n",
    "    a = y[0]\n",
    "    b = y[1]\n",
    "    c = y[2]\n",
    "    d = y[3]\n",
    "    return a*np.sin(b*x) + c*np.cos(d*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 10*np.pi,1000)\n",
    "y = f(x, y_train[0])\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_X(X, y, i):\n",
    "    x = np.linspace(0, 10*np.pi,X.shape[1])\n",
    "    y_est = f(x, y[0])\n",
    "    plt.scatter(x, X[i], label='X', s=15, alpha=0.7)\n",
    "    plt.scatter(x, y_est, label='Y', s=10, alpha=0.2)\n",
    "    plt.axhline(y=0)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('value')\n",
    "    plt.title('sinusoidal curve')\n",
    "    plt.legend()\n",
    "    \n",
    "plot_X(X_train, y_train, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows that the first row X of X_train are points sampled from the function Y=f(X). Bpth X and f(X) match perfectly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **1.3** Use `Sequential` and `Dense` from Keras to build a fully-connected neural network. We can choose any number of layers and any number of nodes in each layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that create a FCN using the provided parameters\n",
    "def create_nn(input_dim, output_dim, nb_nodes=1, nb_hidden_layer=0, hidden_activation='relu', output_activation='linear'):\n",
    "    model = Sequential()\n",
    "    # input layer\n",
    "    model.add(\n",
    "        Dense(nb_nodes, input_dim=input_dim, activation=hidden_activation)\n",
    "    )    \n",
    "    # hidden layers\n",
    "    for i in range(nb_hidden_layer):\n",
    "        model.add(\n",
    "            Dense(nb_nodes, activation=hidden_activation)\n",
    "        )\n",
    "    # output layer\n",
    "    model.add(\n",
    "        Dense(output_dim, activation=output_activation)\n",
    "    )    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create 10 hidden layers, each with 100 nodes\n",
    "model = create_nn(X_train.shape[1], y_train.shape[1], 100, 10, hidden_activation='relu', output_activation='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4** Compile the model via the line `model.compile(loss='mean_absolute_error', optimizer='adam')` and display the `.summary()`. Explain why the first layer in the network has the indicated number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_nn(model, loss='mean_absolute_error', optimizer='adam'):\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compile_nn(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shape of the input and hidden layers is (None, 100). None is a placeholder for the number of samples presented to the layer. 100 refers to the number of nodes in the layer.\n",
    "\n",
    "The input vector has 1000 features which are fed into the 100 nodes of the input layer, this makes 1000 x 100 weights plus 100 bias (1 per node), what explains the number of parameters 100100 for the input layer.\n",
    "\n",
    "Each hidden layer receives an input from each of the 100 nodes in the previous layer, this makes 100 x 100 weights plus 100 bias; this makes in total 10100 parameters.\n",
    "\n",
    "The output layer has 4 nodes, each receiving 100 inputs from the last hidden layer, adding 4 bias, we have 404 parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.5** Fit the model to the data for 50 epochs using a batch size of 32 and a validation split of .2. We can train for longer if we wish- the fit tends to improve over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fit_nn(model, X, y, batch_size=32, epochs=50, validation_split=.2):\n",
    "    return model.fit(X, y, batch_size=batch_size, epochs=epochs, validation_split=validation_split, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_history = fit_nn(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.6** Use the `plot_predictions` function to plot the model's predictions on `X-test` to the true values in `y_test` (by default, it will only plot the first few rows). Report the model's overall loss on the test set. Comment on how well the model performs on this unseen data. Do we think it has accurately learned how to map from sample data to the coefecients that generated the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(model, test_x, test_y, count=None):\n",
    "    # Model - a Keras or SKlearn model that takes in (n,1000) training data and predicts (n,4) output data\n",
    "    # test_x - a (n,1000) input dataset\n",
    "    # test_y - a (n,4) output dataset\n",
    "    # This function will plot the sine curves in the training data and those implied by the model's predictions.\n",
    "    # It will also print the predicted and actual output values.\n",
    "    \n",
    "    #helper function that takes the n by 4 output and reverse-engineers \n",
    "    #the sine curves that output would create\n",
    "    def y2x(y_data):\n",
    "        #extract parameters\n",
    "        a=y_data[:,0].reshape(-1,1)\n",
    "        b=y_data[:,1].reshape(-1,1)\n",
    "        c=y_data[:,2].reshape(-1,1)\n",
    "        d=y_data[:,3].reshape(-1,1)\n",
    "\n",
    "        #build the matching training data\n",
    "        x_points = np.linspace(0,10*np.pi,1000)\n",
    "        x_data = a*np.sin(np.outer(b,x_points)) + c*np.cos(np.outer(d,x_points))\n",
    "        return x_data\n",
    "    \n",
    "    #if <20 examples, plot all. If more, just plot 5\n",
    "    if count==None:\n",
    "        if test_x.shape[0]>20:\n",
    "            count=5\n",
    "        else:\n",
    "            count=test_x.shape[0]\n",
    "    \n",
    "    #build predictions\n",
    "    predicted = model.predict(test_x)\n",
    "    implied_x = y2x(predicted)\n",
    "    for i in range(count):\n",
    "        plt.plot(test_x[i,:],label='true')\n",
    "        plt.plot(implied_x[i,:],label='predicted')\n",
    "        plt.legend()\n",
    "        plt.ylim(-2.1,2.1)\n",
    "        plt.xlabel(\"x value\")\n",
    "        plt.xlabel(\"y value\")\n",
    "        plt.title(\"Curves using the Neural Network's Approximate Fourier Transform\")\n",
    "        plt.show()\n",
    "        print(\"true:\", test_y[i,:])\n",
    "        print(\"predicted:\", predicted[i,:])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for plotting train vs validation of a fitted FCN\n",
    "def plot_nn_accuracy(model_history, loss='Mean Absolute Error', metric='loss', figsize=(10,5)):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    plt.plot(model_history.history[metric], '-o', label = 'train')\n",
    "    plt.plot(model_history.history['val_'+metric], '-*', label = 'val')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel(loss)\n",
    "    plt.legend();\n",
    "\n",
    "    print(\"{}: train={:.1f}, val={:.1f}\".format(loss,\n",
    "        model_history.history[metric][-1],\n",
    "        model_history.history['val_'+metric][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Overall loss on test set\")\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_nn_accuracy(model_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observer that the loss on test is ca. 25% which is not so good. The validation and train loss show a similar trend on the plot above, however with higher variability for validation. Around epoch 12, the model appears not to learn and overfitting starts.\n",
    "\n",
    "We cannot assume that the network has accurately learned from the training set. The features were not standardized between zero and one; their distribution might be skewed. The network might be overfitting to the training set and therefore is not generalizing very well to the unseen test data. Regularization might be required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.7** Examine the model's performance on the 9 train/test pairs in the `extended_test` variables. Which examples does the model do well on, and which examples does it struggle with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Overall loss on extended test set\")\n",
    "model.evaluate(X_extended_test, y_extended_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(model, X_extended_test, y_extended_test, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall loss on the extended test dataset is much worse, ca. 50%. The model is strugglying with samples where either the cosine or the sine component is missing. This happens when a=b=0 and/or c=d=0. These are pure sine and cosine functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.8** Is there something that stands out about the difficult observations, especially with respect to the data the model was trained on? Did the model learn the mapping we had in mind? Would we say the model is overfit, underfit, or neither?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model could not learn the special cases when the function to learn is made just of a sine or just of a cosine component. Such special cases where not covered by the orginal X_train data. \n",
    "\n",
    "The model overfitted to the X_train data where both sine and cosine components were always provided. All weights and bias were estimated to fit that data. Without regularizing those parameters, we end up with a model, which could not be generalize to simple Fourier transformations with pure components. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<hr style='height:2pt'>\n",
    "\n",
    "## Regulrizing Neural Networks\n",
    "\n",
    "In this project we have already explored how ANN are able to learn a mapping from example input data (of fixed size) to example output data (of fixed size), and how well the neural network can generalize. In this section we focus on issues of overfitting and regularization in Neural Networks.\n",
    "\n",
    "ANNs can be prone to overfitting, where they learn specific patterns present in the training data, but the patterns don't generalize to fresh data.\n",
    "\n",
    "There are several methods used to improve ANN generalization. One approach is to use an achitecutre just barely wide/deep enough to fit the data. The idea here is that smaller networks are less expressive and thus less able to overfit the data.\n",
    "\n",
    "However, it is difficult to know a priori the correct size of the ANN, and computationally costly to hunt for a correct size. Given this, other methodologies are used to fight overfitting and improve the ANN generalization. These, like other techniques to combat overfitting, fall under the umbrella of Regularization.\n",
    "\n",
    "In this problem we are asked to regularize a network given to us below. The train dataset can be generated using the code also given below. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 Data Exploration and Inspection:** Use `imshow` to display a handwritten 4 and a handwritten 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3 Overfit an ANN:** Build a fully connected network (FCN) using `keras`: \n",
    "\n",
    "1. Nodes per Layer: 100,100,100,2 (<-the two-class 'output' layer)\n",
    "2. Activation function: reLU \n",
    "3. Loss function: binary_crossentropy\n",
    "4. Output unit: Sigmoid \n",
    "5. Optimizer: sgd (use the defaults; no other tuning)\n",
    "6. Epochs: no more than 2,000\n",
    "7. Batch size: 128\n",
    "8. Validation size: .5\n",
    "\n",
    "This NN trained on the dataset we built in 2.1 will overfit to the training set. Plot the training accuracy and validation accuracy as a function of epochs and explain how we can tell it is overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4 Explore Regularization**: The task is to regularize this FCN. We are free to explore any method or combination of methods. If we are using anything besides the methods we have covered in class, give a citation and a short explanation. We should always have an understanding of the methods we are using. \n",
    "\n",
    "Save the model using `model.save(filename)` and submit in canvas along with the notebook. \n",
    "\n",
    "We will evaluate the model on a test set we've kept secret. \n",
    "\n",
    "1. Don't try to use extra data from NMIST. We will re-train the model on training set under the settings above. \n",
    "2. Keep the  architecture above as is. In other words keep the number of layers, number of nodes, activation function, and loss fucntion the same. We can change the number of epochs (max 2000), batch size, optimizer and of course add elements that can help to regularize (e.g. drop out, L2 norm etc). We can also do data augmentation. \n",
    "3. We *may* import new modules, following the citation rule above.\n",
    "\n",
    "Grading: The score will be based on how much we can improve on the test score via regularization:\n",
    "1.  (0-1] percent will result into 10 pts\n",
    "2.  (1-2] percent will result into 20 pts\n",
    "3.  (2-3] percent will result into 30 pts\n",
    "4.   Above 3 percent will result in 35 pts\n",
    "6.   Top 15 groups or single students will be awarded an additional 10 pts\n",
    "7.   The overall winner(s) will be awarded an additional 5 pts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1** **Data Download and Exploration**:  For this problem, we will be working with the MNIST dataset (Modified National Institute of Standards and Technology database) which is a large database of handwritten digits and commonly used for training various image processing systems. We will be working directly with the download from `keras MNIST dataset` of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images.\n",
    "\n",
    "For simplicity, we will only use the digits labeled `4` and `9`, and we want to use a total of 800 samples for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read and Setup train and test splits in one\n",
    "from keras.datasets import mnist \n",
    "from random import randint \n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data() \n",
    "\n",
    "#shuffle the data before we do anything\n",
    "x_train, y_train = shuffle(x_train, y_train, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogramm of the pixels \n",
    "plt.hist(x_train[0].reshape(28*28))\n",
    "plt.title('Pixel greyscale value distribution for digit {}'.format(y_train[0]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above the greyscales range from 0 to 255; most pixel have values close to zero because of the background. The distribution is right skweed and would cause issue when training the network, which could get stucked in local minima. Therefore we need to standardize the features between 0 and 1. We further need to reshape the features from (28,28) into one input vector (784,) for training our network.\n",
    "\n",
    "Since we will consider only the digits 4 and 9, the output of our network should be a categorical vector (2,)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper function for preparing the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a random subset of nb_samples NMIST samples corresponding to the given digits\n",
    "def get_data_subset(X, y, digits=[4, 9], nb_samples=800):\n",
    "    mask = np.isin(y, digits)\n",
    "    X_subset = X[mask]\n",
    "    y_subset = y[mask]\n",
    "    if nb_samples == -1:\n",
    "        return X_subset, y_subset\n",
    "    idx = np.random.choice(y_subset.shape[0], nb_samples, replace=False)\n",
    "    return X_subset[idx], y_subset[idx]\n",
    "\n",
    "# normalize scale features X to a 1D vector \n",
    "# turn the response y into a binary categorical representation (2,)\n",
    "def get_scaled_data(X, y):\n",
    "    X = X.astype('float32')\n",
    "    X_scaled  = keras.utils.normalize(X)\n",
    "    #X_scaled  /= 255\n",
    "    X_scaled_reshaped = X_scaled.reshape(-1, 28*28)\n",
    "    y_scaled = keras.utils.np_utils.to_categorical(y)\n",
    "    y_scaled = y_scaled.T[~np.all(y_scaled == 0, axis=0)].T\n",
    "    return X_scaled_reshaped, y_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a random subset of 800 training samples labelled 4 or 9\n",
    "x_train_subset, y_train_subset = get_data_subset(x_train, y_train, digits=[4, 9], nb_samples=800)\n",
    "\n",
    "# prepare a random subset of 1991 test samples labelled 4 or 9\n",
    "x_test_subset, y_test_subset = get_data_subset(x_test, y_test, digits=[4, 9], nb_samples=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the shape of the train data subset\n",
    "x_train_subset.shape, y_train_subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the shape of the test data subset\n",
    "x_test_subset.shape, y_test_subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the training features to a 1D vector with [0..1] values range, and response to category 0 or 1\n",
    "x_train_subset_scaled,  y_train_subset_scaled = get_scaled_data(x_train_subset, y_train_subset)\n",
    "\n",
    "# scale the test features to a 1D vector with [0..1] values range, and response to category 0 or 1\n",
    "x_test_subset_scaled,  y_test_subset_scaled = get_scaled_data(x_test_subset, y_test_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the shape of the scaled train data subset\n",
    "x_train_subset_scaled.shape, y_train_subset_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the shape of the scaled test data subset\n",
    "x_test_subset_scaled.shape, y_test_subset_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 Data Exploration and Inspection:** Use `imshow` to display a handwritten 4 and a handwritten 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.tight_lawet()\n",
    "plt.imshow(x_train_subset[y_train_subset==4][0], cmap='gray', interpolation='none')\n",
    "plt.title('Digit 4')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.subplot(1,2,2)\n",
    "plt.tight_lawet()\n",
    "plt.imshow(x_train_subset[y_train_subset==9][0], cmap='gray', interpolation='none')\n",
    "plt.title('Digit 9')\n",
    "plt.xticks([])\n",
    "plt.yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3 Overfit an ANN:** Build a fully connected network (FCN) using `keras`: \n",
    "\n",
    "1. Nodes per Layer: 100,100,100,2 (<-the two-class 'output' layer)\n",
    "2. Activation function: reLU\n",
    "3. Loss function: binary_crossentropy\n",
    "4. Output unit: Sigmoid \n",
    "5. Optimizer: sgd (use the defaults; no other tuning)\n",
    "6. Epochs: no more than 1,000\n",
    "7. Batch size: 128\n",
    "8. Validation size: .5\n",
    "\n",
    "This NN trained on the dataset we built in 2.1 will overfit to the training set. Plot the training accuracy and validation accuracy as a function of epochs and explain how we can tell it is overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and compile a FCN without regularization\n",
    "def build_model_overfit(optimizer='sdg'):\n",
    "        model = Sequential([\n",
    "            Dense(100, input_shape=(784,), activation='relu'),\n",
    "            Dense(100, activation='relu'),\n",
    "            Dense(100, activation='relu'),\n",
    "            Dense(2, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic function that fits a FCN with given parameters \n",
    "# evaluates the fitted FCN on test set \n",
    "# reports training, validation and test accuracy scores\n",
    "def build_fit_evaluate_nn(build_model, Xtrain, ytrain, Xtest, ytest, \n",
    "                          epochs=500, batch_size=128,\n",
    "                          callbacks = None,\n",
    "                          optimizer='sgd',\n",
    "                          verbose=False):\n",
    "    np.random.seed(5)\n",
    "    model = build_model(optimizer=optimizer)\n",
    "    model_history = model.fit(Xtrain, ytrain, \n",
    "                                        epochs=epochs, batch_size=batch_size, \n",
    "                                        validation_split=.5, \n",
    "                                        callbacks = callbacks,\n",
    "                                        verbose=verbose) \n",
    "    plot_nn_accuracy(model_history, 'Accuracy Score', metric='acc', figsize=(15,5))\n",
    "    accuracy_score_train = model_history.history['acc'][-1]\n",
    "    accuracy_score_val = model_history.history['val_acc'][-1]\n",
    "    accuracy_score_test = model.evaluate(Xtest, ytest)[1];\n",
    "    print(\"Accuracy Score - Train: {0:.2%}\".format(accuracy_score_train))\n",
    "    print(\"Accuracy Score - Val: {0:.2%}\".format(accuracy_score_val))\n",
    "    print(\"Accuracy Score - Test: {0:.2%}\".format(accuracy_score_test))\n",
    "    return {'train': [accuracy_score_train], 'val': [accuracy_score_val], 'test': [accuracy_score_test]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for reporting scores\n",
    "def report_scores(df, label, scores):\n",
    "    scores['regularization'] = label\n",
    "    scores['improvment'] = str(round((scores['test'][0]/df[df['regularization']=='none']['test'][0] - 1)*100,2))+'%'\n",
    "    df = df[df.regularization != label]\n",
    "    df = df.append(pd.DataFrame(scores, columns=['regularization', 'train', 'val', 'test','improvment']))\n",
    "    df = df.drop_duplicates()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overfitted FCN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score_overfit = build_fit_evaluate_nn(build_model_overfit, \n",
    "                                               x_train_subset_scaled, y_train_subset_scaled, \n",
    "                                               x_test_subset_scaled, y_test_subset_scaled);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score_overfit['regularization'] = 'none'\n",
    "df_scores = pd.DataFrame(accuracy_score_overfit, columns=['regularization', 'train', 'val', 'test', 'improvment'])\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "We observe that, unlike the training accuracy that increases monotonically, the validation accuracy increases to a maximum, and then decreases for a while. As we move towards the right (increasing epochs) the accuracy on training keeps improving but the accuracy on validation does not. \n",
    "\n",
    "Based on this observation we can make an interpretation that probably during the first phase of the training the model was improving its overall fit, but then as it continues training the training loss decreases, but the validation accuracy starts decresing. This is a sign where we are probably starting to overfit to our training set. \n",
    "\n",
    "Ideally if we are going to do early stopping, we probably want to stop around epoch where the trend changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4 Explore Regularization**: The task is to regularize this FCN. We are free to explore any method or combination of methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "Our network from 2.3 is very complex (with 10 of thousands of weights). This makes is very prone to overfitting. Regularization is a technique to make modification in the learning algorithm of our network so that the network will generalize better. We do so by penalizing the weights of the network. For example if the penalty is so high that some of the weights are nearly equal to zero, than this would result in a model closer to a simple linear model. Our goal is to find the optimal value of regularization.\n",
    "\n",
    "We tried to find the optimal value of some hyperparameters using GridSearchCV (cross-validation). We will not present the results here because it took hours to run and most searches did not end. Our approach here is based on intuition and try/fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "Normally we would choose an optimization approach and tune the hyper-parameters to fit that approach. Here we investigate whether stochastic gradient descent or adam would be the best apriori approach for our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score_adam = build_fit_evaluate_nn(build_model_overfit, \n",
    "                                               x_train_subset_scaled, y_train_subset_scaled, \n",
    "                                               x_test_subset_scaled, y_test_subset_scaled,\n",
    "                                               optimizer='adam');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = report_scores(df_scores, 'optimizer Adam', accuracy_score_adam)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that Adam optimizer provides a slightly improve in test accuracy score, compare to stochastic gradient descent. We will continue investigating further regularization while using Adam in the next steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Epochs\n",
    "\n",
    "The number of epochs is the number of times the entire training set is used during training. One epoch is when an entire dataset is pass forward through the neural network and weights are updated backwards once.\n",
    "\n",
    "Below we investigate the appropriate epochs between 2000, 1000, 125 (compared to the default 500 used in the previous sections) to use in the iterative gradient descent approach, that will not underfit the data. In the next section we will find the best approach to divide the data into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score_2000_epochs = build_fit_evaluate_nn(build_model_overfit, \n",
    "                                               x_train_subset_scaled, y_train_subset_scaled, \n",
    "                                               x_test_subset_scaled, y_test_subset_scaled,\n",
    "                                               optimizer='adam',\n",
    "                                               epochs=2000\n",
    "                                               );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = report_scores(df_scores, '2000 epochs', accuracy_score_2000_epochs)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score_1000_epochs = build_fit_evaluate_nn(build_model_overfit, \n",
    "                                               x_train_subset_scaled, y_train_subset_scaled, \n",
    "                                               x_test_subset_scaled, y_test_subset_scaled,\n",
    "                                               optimizer='adam',\n",
    "                                               epochs=1000\n",
    "                                               );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = report_scores(df_scores, '1000 epochs', accuracy_score_1000_epochs)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score_125_epochs = build_fit_evaluate_nn(build_model_overfit, \n",
    "                                               x_train_subset_scaled, y_train_subset_scaled, \n",
    "                                               x_test_subset_scaled, y_test_subset_scaled,\n",
    "                                               optimizer='adam',\n",
    "                                               epochs=125\n",
    "                                               );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = report_scores(df_scores, '125 epochs', accuracy_score_125_epochs)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score_83_epochs = build_fit_evaluate_nn(build_model_overfit, \n",
    "                                               x_train_subset_scaled, y_train_subset_scaled, \n",
    "                                               x_test_subset_scaled, y_test_subset_scaled,\n",
    "                                               optimizer='adam',\n",
    "                                               epochs=83\n",
    "                                               );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = report_scores(df_scores, '83 epochs', accuracy_score_83_epochs)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the number of epochs from 500 to 1000 or 2000 improves the test accuracy just a little bit. \n",
    "\n",
    "Reducing training to 125 epochs, and further to 83 epochs seems to achive less overfit. \n",
    "\n",
    "We will continue with **83 epochs** and thus gain more rooms for data augmentation. In fact, we are limited to use 200000 training samples. By choosing 83 epochs we can augment our training/validation dataset to 4800 samples (2400 for train, 2400 for validation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Size\n",
    "\n",
    "The batch size is the number of samples shown to the network before the weights are updated.\n",
    "\n",
    "It has been observed in practice that when using a larger batch there is a significant degradation in the quality of the model, as measured by its ability to generalize. Reference: https://arxiv.org/abs/1609.04836\n",
    "\n",
    "We tend to use a smaller batch size also because of lower computational power. We will try batch size of 32 and 64, compared to 128 which we used so far. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score_batchsize_32 = build_fit_evaluate_nn(build_model_overfit, \n",
    "                                               x_train_subset_scaled, y_train_subset_scaled, \n",
    "                                               x_test_subset_scaled, y_test_subset_scaled,\n",
    "                                               optimizer='adam',\n",
    "                                               epochs = 83,\n",
    "                                               batch_size=32\n",
    "                                               );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = report_scores(df_scores, 'batch size 32', accuracy_score_batchsize_32)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score_batchsize_64 = build_fit_evaluate_nn(build_model_overfit, \n",
    "                                               x_train_subset_scaled, y_train_subset_scaled, \n",
    "                                               x_test_subset_scaled, y_test_subset_scaled,\n",
    "                                               optimizer='adam',\n",
    "                                               epochs = 83,\n",
    "                                               batch_size=64\n",
    "                                               );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = report_scores(df_scores, 'batch size 64', accuracy_score_batchsize_64)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lowering the batch size improves test accuracy. It also leads to a smaller number of iterations of a training algorithm. \n",
    "\n",
    "For example, we divide 400 training samples into batches of 32, which take 12 iterations to complete an epoch.\n",
    "\n",
    "We will continue with **batch size 32**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "We try a simple way to reduce overfitting by increasing the training set size by rotating, flipping, scaling or shifting the original images.\n",
    "\n",
    "Partly ideas and code adapted from https://machinelearningmastery.com/image-augmentation-deep-learning-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(Xtrain, ytrain, nb_new_samples, datagen, append=True, display=True):\n",
    "    # reshape to be (samples, pixels, width, height\n",
    "    Xtrain_4D = Xtrain.reshape(Xtrain.shape[0], 28, 28, 1)\n",
    "    # convert from int to float\n",
    "    Xtrain_4D = Xtrain_4D.astype('float32')\n",
    "    # fit parameters from data\n",
    "    datagen.fit(Xtrain_4D)\n",
    "    # retrieve one batch of images\n",
    "    (Xbatch, ybatch) = datagen.flow(Xtrain_4D, ytrain, batch_size=nb_new_samples, shuffle=False).next()\n",
    "    Xbatch = Xbatch.reshape(Xbatch.shape[0], 28, 28)\n",
    "    if display:\n",
    "        #display an example\n",
    "        plt.imshow(Xbatch[0].reshape(28, 28), cmap=plt.get_cmap('gray'))\n",
    "    if append:\n",
    "        # append\n",
    "        Xtrain_augmented = np.append(Xtrain, Xbatch, axis=0)\n",
    "        ytrain_augmented = np.append(ytrain, ybatch, axis=0)  \n",
    "    else:\n",
    "        Xtrain_augmented = Xbatch\n",
    "        ytrain_augmented = ybatch\n",
    "    #shuffle the data\n",
    "    Xtrain_augmented, ytrain_augmented = shuffle(Xtrain_augmented, ytrain_augmented, random_state=1)    \n",
    "    # scale the data\n",
    "    Xtrain_augmented_scaled,  ytrain_augmented_scaled = get_scaled_data(Xtrain_augmented, ytrain_augmented)\n",
    "    return Xtrain_augmented_scaled, ytrain_augmented_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Darkening, Lightening**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
    "x_train_subset_augmented1, y_train_subset_augmented1 = augment_data(x_train_subset, y_train_subset, \n",
    "                                                                        nb_new_samples=800, datagen=datagen)\n",
    "accuracy_score_augmented1 = build_fit_evaluate_nn(build_model_overfit, \n",
    "                                               x_train_subset_augmented1, y_train_subset_augmented1, \n",
    "                                               x_test_subset_scaled, y_test_subset_scaled,\n",
    "                                               optimizer='adam',\n",
    "                                               epochs = 83,\n",
    "                                               batch_size=32);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = report_scores(df_scores, 'data augmentation - darkening, lightening', accuracy_score_augmented1)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ZCA Whitening**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(zca_whitening=True)\n",
    "x_train_subset_augmented2, y_train_subset_augmented2 = augment_data(x_train_subset, y_train_subset, \n",
    "                                                                    nb_new_samples=800, datagen=datagen)\n",
    "accuracy_score_augmented2 = build_fit_evaluate_nn(build_model_overfit, \n",
    "                                               x_train_subset_augmented2, y_train_subset_augmented2, \n",
    "                                               x_test_subset_scaled, y_test_subset_scaled,\n",
    "                                               optimizer='adam',\n",
    "                                               epochs = 83,\n",
    "                                               batch_size=32\n",
    "                                                 );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = report_scores(df_scores, 'data augmentation - ZCA whitening', accuracy_score_augmented2)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Rotation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rotation_range=90)\n",
    "x_train_subset_augmented3, y_train_subset_augmented3 = augment_data(x_train_subset, y_train_subset, \n",
    "                                                                    nb_new_samples=800, datagen=datagen)\n",
    "accuracy_score_augmented3 = build_fit_evaluate_nn(build_model_overfit, \n",
    "                                               x_train_subset_augmented3, y_train_subset_augmented3, \n",
    "                                               x_test_subset_scaled, y_test_subset_scaled,\n",
    "                                               optimizer='adam',\n",
    "                                               epochs = 83,\n",
    "                                               batch_size=32);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = report_scores(df_scores, 'data augmentation - random rotation', accuracy_score_augmented3)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Shifts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(width_shift_range=0.2, height_shift_range=0.2)\n",
    "x_train_subset_augmented4, y_train_subset_augmented4 = augment_data(x_train_subset, y_train_subset, \n",
    "                                                                    nb_new_samples=800, datagen=datagen)\n",
    "accuracy_score_augmented4 = build_fit_evaluate_nn(build_model_overfit, \n",
    "                                               x_train_subset_augmented4, y_train_subset_augmented4, \n",
    "                                               x_test_subset_scaled, y_test_subset_scaled,\n",
    "                                               optimizer='adam',\n",
    "                                               epochs = 83,\n",
    "                                               batch_size=32);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = report_scores(df_scores, 'data augmentation - random shift', accuracy_score_augmented4)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Flips**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True)\n",
    "x_train_subset_augmented5, y_train_subset_augmented5 = augment_data(x_train_subset, y_train_subset, \n",
    "                                                                    nb_new_samples=800, datagen=datagen)\n",
    "accuracy_score_augmented5 = build_fit_evaluate_nn(build_model_overfit, \n",
    "                                               x_train_subset_augmented5, y_train_subset_augmented5, \n",
    "                                               x_test_subset_scaled, y_test_subset_scaled,\n",
    "                                               optimizer='adam',\n",
    "                                               epochs = 83,\n",
    "                                               batch_size=32);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = report_scores(df_scores, 'data augmentation - random flips', accuracy_score_augmented5)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multiple Augmentations**\n",
    "\n",
    "Here we use all 5 image transformations above in order to build a single augmented dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that performs 5 types of image transformation on a 800x28x28 set\n",
    "# it adds 480*5 new samples, scales and returns a 1D input vector\n",
    "def augment_data_full(x_train_subset, y_train_subset, nb_samples_to_augment=480):\n",
    "    datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
    "    x_train_subset_augmented1, y_train_subset_augmented1 = augment_data(x_train_subset, y_train_subset, \n",
    "                                                                        nb_new_samples=nb_samples_to_augment, \n",
    "                                                                        datagen=datagen, append=False, display=False)\n",
    "    datagen = ImageDataGenerator(zca_whitening=True)\n",
    "    x_train_subset_augmented2, y_train_subset_augmented2 = augment_data(x_train_subset, y_train_subset, \n",
    "                                                                        nb_new_samples=nb_samples_to_augment, \n",
    "                                                                        datagen=datagen, append=False, display=False)\n",
    "    datagen = ImageDataGenerator(rotation_range=90)\n",
    "    x_train_subset_augmented3, y_train_subset_augmented3 = augment_data(x_train_subset, y_train_subset, \n",
    "                                                                        nb_new_samples=nb_samples_to_augment, \n",
    "                                                                        datagen=datagen, append=False, display=False)\n",
    "    datagen = ImageDataGenerator(width_shift_range=0.2, height_shift_range=0.2)\n",
    "    x_train_subset_augmented4, y_train_subset_augmented4 = augment_data(x_train_subset, y_train_subset, \n",
    "                                                                        nb_new_samples=nb_samples_to_augment, \n",
    "                                                                        datagen=datagen, append=False, display=False)\n",
    "    datagen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True)\n",
    "    x_train_subset_augmented5, y_train_subset_augmented5 = augment_data(x_train_subset, y_train_subset, \n",
    "                                                                        nb_new_samples=nb_samples_to_augment, \n",
    "                                                                        datagen=datagen, append=False, display=False)\t\n",
    "    x_train_subset_scaled,  y_train_subset_scaled = get_scaled_data(x_train_subset, y_train_subset)\n",
    "    x_test_subset_scaled,  y_test_subset_scaled = get_scaled_data(x_test_subset, y_test_subset)\n",
    "    # append all augmented datasets\n",
    "    x_train_subset_augmented = np.append(np.append(np.append(np.append(np.append(\n",
    "                                 x_train_subset_augmented1, \n",
    "                                 x_train_subset_augmented2, axis=0), \n",
    "                                 x_train_subset_augmented3, axis=0), \n",
    "                                 x_train_subset_augmented4, axis=0), \n",
    "                                 x_train_subset_augmented5, axis=0), \n",
    "                                 x_train_subset_scaled, axis=0)\n",
    "    y_train_subset_augmented = np.append(np.append(np.append(np.append(np.append(\n",
    "                                 y_train_subset_augmented1, \n",
    "                                 y_train_subset_augmented2, axis=0), \n",
    "                                 y_train_subset_augmented3, axis=0), \n",
    "                                 y_train_subset_augmented4, axis=0), \n",
    "                                 y_train_subset_augmented5, axis=0), \n",
    "                                 y_train_subset_scaled, axis=0)\n",
    "    return x_train_subset_augmented, y_train_subset_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_subset_augmented, y_train_subset_augmented = augment_data_full(x_train_subset, y_train_subset, nb_samples_to_augment=800)\n",
    "print(\"Fully augmented training set features shape\", x_train_subset_augmented.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score_augmented = build_fit_evaluate_nn(build_model_overfit, \n",
    "                                               x_train_subset_augmented, y_train_subset_augmented, \n",
    "                                               x_test_subset_scaled, y_test_subset_scaled,\n",
    "                                               optimizer='adam',\n",
    "                                               epochs = 83,\n",
    "                                               batch_size=32);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = report_scores(df_scores, 'data augmentation - multiple', accuracy_score_augmented)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we extend the training/validation set to 4800 samples using a transformation of the images, we can observe a good improve on test accuracy. \n",
    "\n",
    "We will investigate further regularization using this augmented training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping\n",
    "\n",
    "With early stopping we abort training shortly after the validation accuracy was not improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_acc', patience=6)]\n",
    "accuracy_score_early_stop = build_fit_evaluate_nn(build_model_overfit, \n",
    "                                                   x_train_subset_augmented, y_train_subset_augmented, \n",
    "                                                   x_test_subset_scaled, y_test_subset_scaled,\n",
    "                                                   optimizer='adam',\n",
    "                                                   epochs = 83,\n",
    "                                                   batch_size=32,\n",
    "                                                callbacks=callbacks);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = report_scores(df_scores, 'early stopping', accuracy_score_early_stop)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping after a low number of epochs with patience 6 achieves good accuracy on test, with less overfitting to the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "We randomly drop some weights during training in order to learn independent internal representations. We add a new 'Dropout' layer and impose a constraint on the weights on input and hidden layers. We further constrain the size of network weights.\n",
    "\n",
    "The following reference was used: https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_dropout(dropout_rate=0.17, optimizer='adam'):\n",
    "        model = Sequential([\n",
    "            Dense(100, input_shape=(784,), activation='relu', kernel_initializer='normal', kernel_constraint=maxnorm(3)),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(100, activation='relu', kernel_initializer='normal', kernel_constraint=maxnorm(3)),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(100, activation='relu', kernel_initializer='normal', kernel_constraint=maxnorm(3)),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(2, activation='sigmoid', kernel_initializer='normal')\n",
    "        ])\n",
    "        model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#callbacks = [EarlyStopping(monitor='val_acc', patience=1)]\n",
    "accuracy_score_dropout = build_fit_evaluate_nn(build_model_dropout, \n",
    "                                                   x_train_subset_augmented, y_train_subset_augmented, \n",
    "                                                   x_test_subset_scaled, y_test_subset_scaled,\n",
    "                                                   optimizer='adam',\n",
    "                                                   epochs = 83,\n",
    "                                                   batch_size=32\n",
    "                                                #,callbacks=callbacks\n",
    "                                              );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = report_scores(df_scores, 'dropout', accuracy_score_dropout)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the model on augmented dataset with **dropout rate 0.17** improves the the baseline overfitted model by over 4%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization\n",
    "\n",
    "We use L2 to force the weights to decay towards zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_l2(dropout_rate=0.17, optimizer='adam', regularization=0.0001):\n",
    "        model = Sequential([\n",
    "            Dense(100, input_shape=(784,), activation='relu', \n",
    "                  kernel_initializer='normal', kernel_constraint=maxnorm(3), \n",
    "                  kernel_regularizer=regularizers.l2(regularization)),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(100, activation='relu', \n",
    "                  kernel_initializer='normal', kernel_constraint=maxnorm(3), \n",
    "                  kernel_regularizer=regularizers.l2(regularization)),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(100, activation='relu', \n",
    "                  kernel_initializer='normal', kernel_constraint=maxnorm(3), \n",
    "                  kernel_regularizer=regularizers.l2(regularization)),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(2, activation='sigmoid', kernel_initializer='normal')\n",
    "        ])\n",
    "        model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score_l2 = build_fit_evaluate_nn(build_model_l2, \n",
    "                                                   x_train_subset_augmented, y_train_subset_augmented, \n",
    "                                                   x_test_subset_scaled, y_test_subset_scaled,\n",
    "                                                   optimizer='adam',\n",
    "                                                   epochs = 83,\n",
    "                                                   batch_size=32\n",
    "                                         );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = report_scores(df_scores, 'L2 penalty', accuracy_score_l2)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the model with augmented training data together with **L2 regularization 0.001 with dropout rate 0.17** improved the test accuracy by over 4%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization\n",
    "\n",
    "L1 regularization forces some weights to zero. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_l1(dropout_rate=0.17, optimizer='adam', regularization=0.0001):\n",
    "        model = Sequential([\n",
    "            Dense(100, input_shape=(784,), activation='relu', \n",
    "                  kernel_initializer='normal', kernel_constraint=maxnorm(3), \n",
    "                  kernel_regularizer=regularizers.l1(regularization)),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(100, activation='relu', \n",
    "                  kernel_initializer='normal', kernel_constraint=maxnorm(3), \n",
    "                  kernel_regularizer=regularizers.l1(regularization)),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(100, activation='relu', \n",
    "                  kernel_initializer='normal', kernel_constraint=maxnorm(3), \n",
    "                  kernel_regularizer=regularizers.l1(regularization)),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(2, activation='sigmoid', kernel_initializer='normal')\n",
    "        ])\n",
    "        model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score_l1 = build_fit_evaluate_nn(build_model_l1, \n",
    "                                                   x_train_subset_augmented, y_train_subset_augmented, \n",
    "                                                   x_test_subset_scaled, y_test_subset_scaled,\n",
    "                                                   optimizer='adam',\n",
    "                                                   epochs = 83,\n",
    "                                                   batch_size=32\n",
    "                                         );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = report_scores(df_scores, 'L1 penalty', accuracy_score_l1)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**L1 regularization 0.0001 with dropout rate 0.17** also achieves one of the best test accuracy, similarly to L2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate\n",
    "\n",
    "Here we are interested in finding how much to update the weight at the end of each batch (learning rate), and how much to let the previous update influence the current weight update (momentum) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_lr(dropout_rate=0.17, optimizer='adam', regularization=0.0001, learn_rate=0.001):\n",
    "        model = Sequential([\n",
    "            Dense(100, input_shape=(784,), activation='relu', \n",
    "                  kernel_initializer='normal', kernel_constraint=maxnorm(3), \n",
    "                  kernel_regularizer=regularizers.l1(regularization)),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(100, activation='relu', \n",
    "                  kernel_initializer='normal', kernel_constraint=maxnorm(3), \n",
    "                  kernel_regularizer=regularizers.l1(regularization)),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(100, activation='relu', \n",
    "                  kernel_initializer='normal', kernel_constraint=maxnorm(3), \n",
    "                  kernel_regularizer=regularizers.l1(regularization)),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(2, activation='sigmoid', kernel_initializer='normal')\n",
    "        ])\n",
    "        model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr=learn_rate), metrics=['accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#callbacks = [EarlyStopping(monitor='val_acc', patience=7)]\n",
    "accuracy_score_lr = build_fit_evaluate_nn(build_model_lr, \n",
    "                                                   x_train_subset_augmented, y_train_subset_augmented, \n",
    "                                                   x_test_subset_scaled, y_test_subset_scaled,\n",
    "                                                   optimizer='adam',\n",
    "                                                   epochs = 83,\n",
    "                                                   batch_size=32\n",
    "                                                   #,callbacks=callbacks\n",
    "                                         );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = report_scores(df_scores, 'learning rate', accuracy_score_lr)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning the **learning rate to 0.001** helped us regularizing the model further up to 4.95%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model\n",
    "\n",
    "Based on the observations above, we regularize our FCN as follows:\n",
    "\n",
    "- adam optimizer\n",
    "- 32 batch size\n",
    "- 125 epochs\n",
    "- data augmentation to 4800 samples in the training/validation dataset\n",
    "- L1 regularization with penalty 0.0001\n",
    "- dropout with rate 0.17\n",
    "- learning rate 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Augment training/validation dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_subset_augmented, y_train_subset_augmented = augment_data_full(x_train_subset, y_train_subset, nb_samples_to_augment=800)\n",
    "print(\"Original training set features shape\", x_train_subset.shape)\n",
    "print(\"Fully augmented training set features shape\", x_train_subset_augmented.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Create a FCN with tuned hyper-parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_optimal(dropout_rate=0.17, optimizer='adam', regularization=0.0001, learn_rate=0.001):\n",
    "        model = Sequential([\n",
    "            Dense(100, input_shape=(784,), activation='relu', \n",
    "                  kernel_initializer='normal', kernel_constraint=maxnorm(3), \n",
    "                  kernel_regularizer=regularizers.l1(regularization)),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(100, activation='relu', \n",
    "                  kernel_initializer='normal', kernel_constraint=maxnorm(3), \n",
    "                  kernel_regularizer=regularizers.l1(regularization)),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(100, activation='relu', \n",
    "                  kernel_initializer='normal', kernel_constraint=maxnorm(3), \n",
    "                  kernel_regularizer=regularizers.l1(regularization)),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(2, activation='sigmoid', kernel_initializer='normal')\n",
    "        ])\n",
    "        model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr=learn_rate), metrics=['accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Fit and Evaluate the FCN with further tuned hyper-parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score_optimal = build_fit_evaluate_nn(build_model_optimal, \n",
    "                                                   x_train_subset_augmented, y_train_subset_augmented, \n",
    "                                                   x_test_subset_scaled, y_test_subset_scaled,\n",
    "                                                   optimizer='adam',\n",
    "                                                   epochs = 83,\n",
    "                                                   batch_size=32\n",
    "                                         );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Report the accuracy score, compared to other regularization approaches**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = report_scores(df_scores, 'optimal', accuracy_score_optimal)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Save the fitted model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model_optimal()\n",
    "model_history = model.fit(x_train_subset_augmented, y_train_subset_augmented, \n",
    "                                    epochs=83, batch_size=32, \n",
    "                                    validation_split=.5, \n",
    "                                    verbose=False)\n",
    "model.save('final.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best accuracy score we have obtained on test set was **4.95%**. Each time the network is fitted, the score changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
