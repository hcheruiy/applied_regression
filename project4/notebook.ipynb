{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uaoVZSrv48aE"
   },
   "source": [
    "## Regularization \n",
    "\n",
    "Hillary Cheruiyot \n",
    "\n",
    "\n",
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j02kGLE-48ah"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "\n",
    "from pandas import datetools\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuing Bike Sharing Usage Data\n",
    "\n",
    "In this project, we will focus on regularization and cross validation. We will continue to build regression models for the [Capital Bikeshare program](https://www.capitalbikeshare.com) in Washington D.C.  See project 3 for more information about the Capital Bikeshare data that we'll be using extensively. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1: Data pre-processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1** Read in the provided `bikes_student.csv` to a data frame named `bikes_main`. Split it into a training set `bikes_train` and a validation set `bikes_val`. Use `random_state=90`, a test set size of .2, and stratify on month. Remember to specify the data's index column as we read it in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Import the CSV data file into a dataframe*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eWIlhxR39wwq"
   },
   "outputs": [],
   "source": [
    "bikes_main = pd.read_csv('data/bikes_student.csv', index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Explore the data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes_main.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes_main.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes_main.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes_main.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Describe the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with a dataset contained in data/bikes_student.csv. Each row in this file represents the number of rides by registered users and casual users in a given hour of a specific date in the years 2011 and 2012. There are 36 attributes in total: \n",
    "\n",
    "\n",
    "- `Unnamed: 0` (unique row identifier)\n",
    "- `dteday` (date in the format YYYY-MM-DD, e.g. 2011-01-01)\n",
    "- `hour` (0 for 12 midnight, 1 for 1:00am, 23 for 11:00pm)\n",
    "- `year` (0 for 2011, 1 for 2012)\n",
    "- `holiday` (1 = the day is a holiday, 0 = otherwise)\n",
    "- `workingday` (1 = the day is a working day, 0 = otherwise)\n",
    "- `temp` (temperature in Celsius)\n",
    "- `atemp` (apparent temperature, or relative outdoor temperature, in Celsius)\n",
    "- `hum` (relative humidity)\n",
    "- `windspeed` (wind speed)\n",
    "- `casual` (number of rides that day made by casual riders, not registered in the system)\n",
    "- `registered` (number of rides that day made by registered riders)\n",
    "- `count` (total number of rides that day made)\n",
    "- `Feb` (1 = the day is a Feb day, 0 = otherwise)\n",
    "- `Mar` (1 = the day is a Mar day, 0 = otherwise)\n",
    "- `Apr` (1 = the day is a Apr day, 0 = otherwise)\n",
    "- `May` (1 = the day is a May day, 0 = otherwise)\n",
    "- `Jun` (1 = the day is a Jun day, 0 = otherwise)\n",
    "- `Jul` (1 = the day is a Jul day, 0 = otherwise)\n",
    "- `Aug` (1 = the day is a Aug day, 0 = otherwise)\n",
    "- `Sep` (1 = the day is a Sep day, 0 = otherwise)\n",
    "- `Oct` (1 = the day is a Oct day, 0 = otherwise)\n",
    "- `Nov` (1 = the day is a Nov day, 0 = otherwise)\n",
    "- `Dec` (1 = the day is a Dec day, 0 = otherwise)\n",
    "- `spring` (1 = the day is a spring day, 0 = otherwise)\n",
    "- `summer` (1 = the day is a summer day, 0 = otherwise)\n",
    "- `fall` (1 = the day is a fall day, 0 = otherwise)\n",
    "- `Mon` (1 = the day is Monday, 0 = otherwise)\n",
    "- `Tue` (1 = the day is Tuesday, 0 = otherwise)\n",
    "- `Wed` (1 = the day is Wednesday, 0 = otherwise)\n",
    "- `Thu` (1 = the day is Thursday, 0 = otherwise)\n",
    "- `Fri` (1 = the day is Friday, 0 = otherwise)\n",
    "- `Sat` (1 = the day is Saturday, 0 = otherwise)\n",
    "- `Cloudy` (1 = the weather is Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist, 0 = otherwise)\n",
    "- `Snow` (1 = the weather is Light Snow, Light Rain + Thunderstorm, 0 = otherwise)\n",
    "- `Storm` (1 = the weather is Heavy Rain + Thunderstorm + Mist, Snow + Fog, 0 = otherwise)\n",
    "- `month` (1 = Jan, 2= Feb, 3 = Mar, 4 = Apr, 5 = May, 6 = Jun, 7 = Jul, 8 = Aug, 9 = Sep, 10 = Oct, 11 = Nov, 12 = Dec)\n",
    "\n",
    "If all binary predictors that represent months have the value 0 for a given row, then that row represents an observation done in the month of January.\n",
    "If all binary predictors that represent week days have the value 0 for a given row, then that row represents an observation made on Sunday.\n",
    "If all binary predictors that represent season have the value 0 for a given row, then that row represents an observation made on winter.\n",
    "If all binary predictors that represent weather have the value 0 for a given row, then that row represents an observation made during clear weather.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data (1250 rows) into a training (80%) and a validation set (20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    bikes_train, bikes_val = train_test_split(bikes_main, test_size = 0.2, random_state = 90, stratify=bikes_main['month'])\n",
    "except:\n",
    "    # Drop missing lines\n",
    "    print(\"Lines with missing values dropped\")\n",
    "    bikes_main = bikes_main.dropna(subset=['month'])\n",
    "    bikes_train, bikes_val = train_test_split(bikes_main, test_size = 0.2, random_state = 90, stratify=bikes_main['month'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that each month is represented equally within each set: ca 83 rows per month in the training set and ca. 21 rows per month in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes_train.groupby(['month']).count().dteday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes_val.groupby(['month']).count().dteday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i0YncxWuaW3k"
   },
   "source": [
    "**1.2** As with project 3, the response will be the `counts` column and we'll drop `counts`, `registered` and `casual` for being trivial predictors, drop `workingday` and `month` for being multicolinear with other columns, and `dteday` for being inappropriate for regression. Write code to do this.\n",
    "\n",
    "Encapsulate this process as a function with appropriate inputs and outputs, and test the code by producing `practice_y_train` and `practice_X_train`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop columns**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that takes a data frame and a list of columns as parameters, and return a new dataframe where the columns have been dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(df, cols):\n",
    "    for col in cols:\n",
    "        if col in df:\n",
    "            df = df.drop(columns = [col])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the function on the train data set and create response and predictor data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['counts', 'registered', 'casual', 'workingday', 'month', 'dteday']\n",
    "practice_y_train = drop_columns(bikes_train, bikes_train.columns.difference(['counts']))\n",
    "practice_X_train = drop_columns(bikes_train, cols_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the response and predictor data frames ad verify that the specified columns have been correctly dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "practice_y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "practice_X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "practice_X_train.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mr3zjw-Y48cx"
   },
   "source": [
    "**1.3** Write a function to standardize a provided subset of columns in the training/validation/test sets. Remember that while we will be scaling all of the data, we must learn the scaling parameters (mean and SD) from only the training set.\n",
    "\n",
    "Test the code by building a list of all non-binary columns in the `practice_X_train` and scaling only those columns. Call the result `practice_X_train_scaled`. Display the `.describe()` and verify that we have correctly scaled all columns, including the polynomial columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardize function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that takes a data frame and a set of columns as parameters, and scales data using the mean and standard deviation of training data (also a parameter of the function). There are other standardization methods, for example dividing the data by their standard deviation only (without removing the mean). This later method is decribed in the class book. In this homework, we substract the mean as it was done in the lab.\n",
    "\n",
    "The function will be used to standardize predictors prior to regularization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_columns(df, cols, scaler):\n",
    "    df_copy = df.copy()\n",
    "    df_copy[cols] = scaler.transform(df_copy[cols])\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardize continuous predictors**\n",
    "\n",
    "After unecessary columns have been dropped, the remaining predictors can be split into two groups.\n",
    "\n",
    "* binary predictors: \n",
    "'holiday', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sept', 'Oct', 'Nov', 'Dec', 'spring', 'summer', 'fall', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Cloudy', 'Snow', 'Storm'\n",
    "\n",
    "* continuous predictors: \n",
    "'hour', 'temp', 'atemp', 'hum', 'windspeed', 'year'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_columns = ['holiday','Feb', 'Mar', 'Apr',\n",
    "       'May', 'Jun', 'Jul', 'Aug', 'Sept', 'Oct', 'Nov', 'Dec', 'spring',\n",
    "       'summer', 'fall', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat',\n",
    "       'Cloudy', 'Snow', 'Storm']\n",
    "continuous_columns = practice_X_train.columns.difference(binary_columns)\n",
    "scaler = StandardScaler().fit(practice_X_train[continuous_columns])\n",
    "\n",
    "practice_X_train_scaled = standardize_columns(practice_X_train.astype(float), continuous_columns, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify if the continuous predictors have mean 0 (almost) and standard deviation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "practice_X_train_scaled[continuous_columns].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify scaling on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardize_columns(bikes_val, continuous_columns, scaler).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zEBl-5bU48ci"
   },
   "source": [
    "**1.4** Write a code to augment the dataset with higher-order features for `temp`, `atemp`, `hum`,`windspeed`, and `hour`. We should include ONLY pure powers of these columns. So with degree=2 we should produce `atemp^2` and `hum^2` but not `atemp*hum` or any other two-feature interactions. \n",
    "\n",
    "\n",
    "Encapsulate this process as a function with apropriate inputs and outputs, and test the code by producing `practice_X_train_poly`, a training dataset with qudratic and cubic features built from `practice_X_train_scaled`, and printing `practice_X_train_poly`'s column names and `.head()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Higher-oder function**\n",
    "\n",
    "Function that takes a dataframe, a list of columns and a polynomial degree, and creates higher order features for the given columns.\n",
    "\n",
    "For example if the degree is 3 and the columns list is ['x'], then the function will return a new dataframe with new columns x^2 and x^3 additional to the existing x. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "id": "UXgj9yxu48cj",
    "outputId": "d1e9e4f7-41ab-4cae-a900-e11e0890638e"
   },
   "outputs": [],
   "source": [
    "def create_higher_order(df, cols, degree):\n",
    "    df_copy = df.copy()\n",
    "    for col in cols:\n",
    "        for order in range(2, degree + 1):\n",
    "            col_name = col + '^' + str(order)\n",
    "            if not col_name in df:\n",
    "                df_copy[col_name] = df[col] ** order\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create higher order features of continuous variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The measured data contains the temperature (temp), relative temperature (atemp), humidity (hum), windspeed and hour.\n",
    "\n",
    "The data is now augmented with temp^2, temp^3, atemp^2, atemp^3, hum^2, hum^3, windspeed^2, windspeed^3, hour^2, hour^3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "practice_X_train_poly = create_higher_order(practice_X_train_scaled, ['temp', 'atemp', 'hum', 'windspeed', 'hour'], 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the new polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols = practice_X_train_poly.columns.difference(list(practice_X_train.columns))\n",
    "new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "practice_X_train_poly[new_cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pK5Mhlqc48dG"
   },
   "source": [
    "**1.5** Write code to add interaction terms to the model. Specifically, we want interactions between the continuous predictors (`temp`,`atemp`, `hum`,`windspeed`) and the month and weekday dummies (`Feb`, `Mar`...`Dec`, `Mon`, `Tue`, ... `Sat`). That means we SHOULD build `atemp*Feb` and `hum*Mon` and so on, but NOT `Feb*Mar` and NOT `Feb*Tue`. The interaction terms should always be a continuous feature times a month dummy or a continuous feature times a weekday dummy.\n",
    "\n",
    "\n",
    "Encapsulate this process as a function with appropriate inputs and outputs, and test the code by adding interaction terms to `practice_X_train_poly` and show its column names and `.head()`**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interaction terms function**\n",
    "\n",
    "Function takes a dataframe, a list of continuous predictors, a list of time-related predictors; and returns a new dataframe augmented with interaction terms between each of the continuous predictors and each of the time-related predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gB6wlgj948dP",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_interaction_terms(df, cols_continuous = ['temp', 'atemp', 'hum', 'windspeed'], \n",
    "                             cols_time = ['Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sept', 'Oct', 'Nov', 'Dec', \n",
    "                                          'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat']):\n",
    "    df_copy = df.copy()\n",
    "    for colc in cols_continuous:\n",
    "        for colb in cols_time:\n",
    "            col_name = colc + '*' + colb\n",
    "            if not col_name in df_copy:\n",
    "                df_copy[col_name] = df_copy[colc] * df_copy[colb]\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create interaction terms between countinuous predictors and months/weekday**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "practice_X_train_poly_inter = create_interaction_terms(practice_X_train_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the new interaction terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols = practice_X_train_poly_inter.columns.difference(list(practice_X_train_poly.columns))\n",
    "new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "practice_X_train_poly_inter[new_cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MGd5qdPu48da"
   },
   "source": [
    "**1.6** Combine all the code so far into a function that takes in `bikes_train`, `bikes_val`, the names of columns for polynomial, the target column, the columns to be dropped and produces computation-ready design matrices `X_train` and `X_val` and responses `y_train` and `y_val`. The final function should build correct, scaled design matrices with the stated interaction terms and any polynomial degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function for computation-ready design matrices**\n",
    "\n",
    "Function that accepts the following parameters:\n",
    "\n",
    "* a train dataframe\n",
    "* a validation dataframe\n",
    "* a polynomial degree\n",
    "* a list of columns for polynomial features. This list is also the list of continuous predictors.\n",
    "* the response variable\n",
    "* a list of columns to remove.\n",
    "\n",
    "The function performs the following actions:\n",
    "* retrieves the response variable from the input dataframe\n",
    "* drops bad columns\n",
    "* create polynomial terms for continuous predictors up to the given degree\n",
    "* create interaction terms between continuous predictors and month/weekday binary variables\n",
    "* standardize all continuous predictors\n",
    "\n",
    "The function standardizes only the continuous predictors, their corresponding polynomial features and interaction terms are not standardized. Standardizing the predictors would help reduce multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_design_mats(train_df, val_df,  degree, \n",
    "                    columns_forpoly=['temp', 'atemp', 'hum','windspeed', 'hour'],\n",
    "                    target_col='counts', \n",
    "                    bad_columns=['counts', 'registered', 'casual', 'workingday', 'month', 'dteday']):\n",
    "    # retrieve response variables\n",
    "    y_train = train_df[[target_col]]\n",
    "    y_val = val_df[[target_col]]\n",
    "    # drop bad variables\n",
    "    x_train = drop_columns(train_df, bad_columns)\n",
    "    x_val = drop_columns(val_df, bad_columns)\n",
    "    # get binary and continuous columns\n",
    "    binary_columns = ['holiday','Feb', 'Mar', 'Apr',\n",
    "       'May', 'Jun', 'Jul', 'Aug', 'Sept', 'Oct', 'Nov', 'Dec', 'spring',\n",
    "       'summer', 'fall', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat',\n",
    "       'Cloudy', 'Snow', 'Storm']\n",
    "    continuous_columns = x_train.columns.difference(binary_columns)\n",
    "    # standardize continuous predictors\n",
    "    scaler = StandardScaler().fit(x_train[continuous_columns])\n",
    "    x_train = standardize_columns(x_train, continuous_columns, scaler)\n",
    "    x_val = standardize_columns(x_val, continuous_columns, scaler)\n",
    "    # create polynomial terms\n",
    "    x_train = create_higher_order(x_train, columns_forpoly, degree)\n",
    "    x_val = create_higher_order(x_val, columns_forpoly, degree)\n",
    "    # create interaction terms\n",
    "    x_train = create_interaction_terms(x_train)\n",
    "    x_val = create_interaction_terms(x_val)\n",
    "    return x_train, y_train, x_val, y_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test the design matrices for train and validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val = get_design_mats(bikes_train, bikes_val, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the number of rows in the train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the data in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.columns.values)\n",
    "print(X_train.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the data in the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_val.columns.values)\n",
    "print(X_val.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Regularization via Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1** For each degree in 1 through 8:\n",
    "\n",
    "1.  Build the training design matrix and validation design matrix using the function `get_design_mats` with polynomial terms up through the specified degree.\n",
    "\n",
    "2.  Fit a regression model to the training data.\n",
    "\n",
    "3.  Report the model's score on the validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Polynomial models fitting and validation**\n",
    "\n",
    "For each polynomial degree, we compute the design matrices and fit a polynomial linear regression on the training dataset. We use the model to predict the counts on the validation dataset. The R^2 score is returned not only for the validation dataset but also for the training dataset. We return the coefficients variance and the model with best validation R2 score as well.\n",
    "\n",
    "The following function performs the above steps for a given max_degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "colab_type": "code",
    "id": "VTWx4demHUo0",
    "outputId": "fb9ca7f7-8b7b-42e5-fab8-471ab5f57180"
   },
   "outputs": [],
   "source": [
    "def poly_fit(max_degree = 8):\n",
    "    r2_scores_val = []\n",
    "    r2_scores_train = []\n",
    "    coefs_var = []\n",
    "    residuals = []\n",
    "    poly_degrees = range(1,max_degree + 1)\n",
    "    best_model = None\n",
    "    best_score_dict = {\"degree\": 1, \"score\": 0}\n",
    "    for degree in poly_degrees:\n",
    "        # build the design matrices\n",
    "        X_train, y_train, X_val, y_val = get_design_mats(bikes_train, bikes_val, degree)\n",
    "        # fit a regression model on training data\n",
    "        model = LinearRegression().fit(X_train.values, y_train.values)\n",
    "        # predict on training data\n",
    "        y_train_predict_ols = model.predict(X_train.values)\n",
    "        # predict on validation data\n",
    "        y_val_predict_ols = model.predict(X_val.values)\n",
    "        # calculate R2 score\n",
    "        r2_train = r2_score(y_train.values, y_train_predict_ols)\n",
    "        r2_scores_train.append(r2_train)\n",
    "        r2_val = r2_score(y_val.values, y_val_predict_ols)\n",
    "        r2_scores_val.append(r2_val)\n",
    "        # calculate coefficients variance\n",
    "        coefs_var.append(np.var(model.coef_))\n",
    "        # calculate residuals on validation data\n",
    "        residuals.append(y_val.values - y_val_predict_ols)\n",
    "        if r2_val > best_score_dict['score']:\n",
    "            best_score_dict = dict({\"degree\": degree, \"score\": r2_val, \"predictors\": X_train.columns.values})\n",
    "            best_model = model\n",
    "    df_scores = pd.DataFrame(dict({\"poly_degrees\": poly_degrees, \n",
    "                              \"r2_scores_val\": r2_scores_val,\n",
    "                              \"r2_scores_train\": r2_scores_train,\n",
    "                              \"coefs_variance\": coefs_var,\n",
    "                              \"residuals\": residuals}))\n",
    "    df_scores.poly_degrees = df_scores.poly_degrees.astype('int')\n",
    "    return df_scores, best_score_dict, best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reporting scores**\n",
    "\n",
    "We run the above function for up to 8 degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poly_scores, best_poly_score_info, best_poly_model = poly_fit(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's scores on the validation data is obtained as follows (together with coefficients variance):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poly_scores[['poly_degrees', 'r2_scores_val', 'coefs_variance']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now plot how the R2 score and variance change with increasing model complexity (polynomial order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_scores_plot(df):\n",
    "    fig, ax = plt.subplots(1,3, figsize=(18,6))\n",
    "    ax[0].plot(df.poly_degrees, df.r2_scores_val, ls='-', marker='o', label = 'validation data')\n",
    "    ax[0].plot(df.poly_degrees, df.r2_scores_train, ls='-', marker='x', label = 'train data')\n",
    "    ax[0].set_ylabel('$R^2$ score')\n",
    "    ax[0].set_xlabel('polynomial degree');\n",
    "    ax[0].set_title('polynomial models\\' score')\n",
    "    ax[0].legend()\n",
    "    ax[0].grid()\n",
    "    ax[1].plot(df.poly_degrees, df.coefs_variance, label = 'normalized variance')\n",
    "    ax[1].set_ylabel('variance')\n",
    "    ax[1].set_xlabel('polynomial degree');\n",
    "    ax[1].set_title('polynomial models\\' coefficients variance')\n",
    "    for degree in range(1, len(df.residuals)+1):\n",
    "        ax[2].violinplot(df.residuals[degree-1], positions=[degree], \n",
    "                  showmeans=True, showmedians=True, showextrema=True, vert=False)\n",
    "    ax[2].set_ylabel('polynomial degree')\n",
    "    ax[2].set_xlabel('residuals');\n",
    "    ax[2].set_title('polynomial models\\' residuals distribution')\n",
    "    ax[2].axvline(x=0, color='black', ls='--', alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_scores_plot(df_poly_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2** Discuss patterns we see in the results from 2.1. Which model would we select, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R2 score on training data increases as new polynomial features are added to the model (up to 8th power). This trend is expected.\n",
    "\n",
    "The R2 score on validation data follows a similar trend as on training data; except of a decrease of R2 score when the 4th polynomial order is added. We might need to investigate some multicollinearity effect introduced by those polynomials. By adding a 5th order the increasing R2 trend is resumed.\n",
    "\n",
    "Most residuals for all models are found below zero for all polynomial orders. \n",
    "\n",
    "Normally we would choose the model that provides the best R2 score 0.556 on validation data. Th R2 score indicates the proportion of variance in the counts of bikes riding that can be explained using the predictors. High values of R2 are signs for good model fitness. Therefore the model with 8 degree of polynomials is the one we would select.\n",
    "\n",
    "However higher model complexity introduces higher variance in the predicted response as shown in the middle plot above. This means that changing the validation data a little bit would come with wider changes in the predicted response. This reduces the significance of regression coefficients and has negative impact on prediction quality. The polynomial degree 6 has the lowest variance ratio with R2 score 0.478 that is still acceptable. We recall having a R2 score 0.445 in project 3 using forward features selection on a quadratic polynomial model. \n",
    "\n",
    "If we investigate further by increasing the degree of polynomials, we obtain the plots below. The residual sum of squares becomes higher than the total variance in the original data with polynomial order 17.\n",
    "\n",
    "At this moment, we would select the model with polynomial terms up to degree 6 because of it seems to offer the most acceptable prediction error, variance and bias. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poly_scores_18, best_poly_score_info_18, best_poly_model_18 = poly_fit(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best R2 score \" + str(best_poly_score_info_18['score']) + \" obtained with polynomial degree \" + str(best_poly_score_info_18['degree']) + \" but at the cost of a large variance \" + str(np.var(best_poly_model_18.coef_)))\n",
    "poly_scores_plot(df_poly_scores_18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sQJ1vesGNwOf"
   },
   "source": [
    "**2.3** Let's try regularizing our models via ridge regression. Build a table showing the validation set $R^2$ of polynomial models with degree from 1-8, regularized at the levels $\\lambda = (.01, .05, .1,.5, 1, 5, 10, 50, 100)$. We do not perform cross validation at this point, simply report performance on the single validation set. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model selection with Ridge regularization**\n",
    "\n",
    "Previously we created different linear regression models with polynomial features. We found out that they provide a good R2 score on validation data. However we were not sure about the importance of predictors and the quality of our estimates in general. Variable selection can be performed by looking for the best subset of predictors, this means searching among 2^193 models. Stepwise forward selection requires less computing effort (18722 possible models). Stepwise backward selection and mixed of both would have a similar computational impact. The best subset of predictors might still miss key predictors, which are significant for the response.\n",
    "\n",
    "Here we try regularization using a new loss function that augments the MSE with a penalty on large coefficients (except of the intercept). We start with Ridge shrinkage that imposes a constraint on the sum of squares of estimates by shrinking them towards zero. A shrinkage parameter lambda is used to tune the penalty effect. This reduces the variance of those coefficients, what could provide more quality in our predictions and better interpretation of the importance and role of predictors on the response. \n",
    "\n",
    "The below function gets a polynomial degree and a list of lambdas as parameters. It creates a ridge regression for each of the lambdas, using predictors which are augmented with their polynomial features up to the specified degree. The function returns the R2 scores obtained on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_fit(max_degree = 8, lambdas = [0, .01, .05, .1, .5, 1, 5, 10, 50, 100]):\n",
    "    r2_scores_dict = dict()\n",
    "    coefs_variance_dict = dict()\n",
    "    residuals_dict = dict()\n",
    "    poly_degrees = range(1,max_degree + 1)\n",
    "    r2_scores_dict['poly_degrees'] = poly_degrees\n",
    "    coefs_variance_dict['poly_degrees'] = poly_degrees\n",
    "    residuals_dict['poly_degrees'] = poly_degrees\n",
    "    best_score_dict = {'degree': 0, 'lambda': 0, 'score': 0}\n",
    "    best_model = None\n",
    "    for lamb in lambdas:\n",
    "        r2_scores_val = [] \n",
    "        coefs_variance = []\n",
    "        residuals = []\n",
    "        for degree in poly_degrees:\n",
    "            # build the design matrices\n",
    "            X_train, y_train, X_val, y_val = get_design_mats(bikes_train, bikes_val, degree)               \n",
    "            # fit a regularized regression model on training data\n",
    "            ridge = Ridge(alpha = lamb)\n",
    "            model = ridge.fit(X_train.values, y_train.values)\n",
    "            # predict on validation data\n",
    "            y_val_predict_ols = model.predict(X_val.values) \n",
    "            # calculate R2 score   \n",
    "            r2 = r2_score(y_val.values, y_val_predict_ols)\n",
    "            r2_scores_val.append(r2)\n",
    "            # retrieve coefficients\n",
    "            coefs_variance.append(np.var(model.coef_))\n",
    "            # calculate residuals\n",
    "            residuals.append(y_val.values - y_val_predict_ols)\n",
    "            if (r2 > best_score_dict['score']) & (lamb != 0):\n",
    "                best_score_dict = {'degree': degree, 'lambda': lamb, 'score': r2, 'predictors': X_train.columns.values}\n",
    "                best_model = model\n",
    "        r2_scores_dict[r'$\\lambda = ' + str(lamb) + '$'] = r2_scores_val\n",
    "        coefs_variance_dict[r'$\\lambda = ' + str(lamb) + '$'] = coefs_variance\n",
    "        residuals_dict[r'$\\lambda = ' + str(lamb) + '$'] = residuals\n",
    "    return pd.DataFrame(r2_scores_dict), pd.DataFrame(coefs_variance_dict), pd.DataFrame(residuals_dict), best_score_dict, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ridge_scores, df_ridge_coefs_variance, df_ridge_residuals, best_ridge_score_info, best_ridge_model = ridge_fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Report Ridge R2 scores**\n",
    "\n",
    "The following table shows the R2 scores obtained when predicting bikes riding with polynomial Ridge regression model on the validation dataset. The models were trained using the training dataset and candidate $\\lambda$s.\n",
    "\n",
    "The R2 scores are displayed for each value of lambda, the Ridge tuning parameter. We added a lambda=0 for comparison purpose which gives the same results as those obtained with ordinary linear regression (without Ridge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ridge_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function plots the $R^2$ score on validation data, obtained using a Ridge model for each polynomial degree and lambda combination, as well as the corresponding variance of models coefficients after Ridge schrinkage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_scores_plot(df_scores, df_coefs_variance):\n",
    "    lambdas = df_scores.columns.difference(['poly_degrees', '$\\lambda = 0$']).values\n",
    "    fig, ax = plt.subplots(1,2, figsize=(15,6)) \n",
    "    for lamb in lambdas:\n",
    "        ax[0].plot(df_scores.poly_degrees, df_scores[lamb], ls='-', marker='o', markersize=3, label = lamb)\n",
    "    ax[0].set_ylabel('$R^2$ score')\n",
    "    ax[0].set_xlabel('polynomial degree');\n",
    "    ax[0].set_title('Ridge polynomial models\\' scores on validation data')\n",
    "    ax[0].legend()\n",
    "    ax[0].grid()\n",
    "    for lamb in lambdas:\n",
    "        ax[1].plot(df_coefs_variance.poly_degrees, df_coefs_variance[lamb], ls='-', marker='o', markersize=3, label = lamb)\n",
    "    ax[1].set_ylabel('variance')\n",
    "    ax[1].set_xlabel('polynomial degree');\n",
    "    ax[1].set_title('polynomial models\\' coefficients variance')\n",
    "    ax[1].legend()\n",
    "    ax[1].grid()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_scores_plot(df_ridge_scores, df_ridge_coefs_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When higher values of $\\lambda$ are used, this penalizes high model complexity, causing models with higher polynomial terms to produce a weak R2 score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TBwFV1wCTWnw"
   },
   "source": [
    "**2.4** Find the best-scoring degree and regularization combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best-scoring degree appears to be \" + str(best_ridge_score_info['degree']) + \" with Ridge tuning parameter \" + str(best_ridge_score_info['lambda']) + \". The corresponding regression model provides predictions with R2 score = \" + str(best_ridge_score_info['score']) + \" at the cost of a high variance of coefficients estimates \" + str(np.var(best_ridge_model.coef_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance of coefficients obtained with each degree and lambda combination is given in the table below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ridge_coefs_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KU3rqk4qU5BK"
   },
   "source": [
    "**2.5** It's time to see how well our selected model will do on future data. Read in the provided test dataset `data/bikes_test.csv`, do any required formatting, and report the best model's $R^2$ score. How does it compare to the validation set score that made us choose this model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read test dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cbzfV7HaW_U1"
   },
   "outputs": [],
   "source": [
    "bikes_test = pd.read_csv('data/bikes_test.csv', index_col='Unnamed: 0')\n",
    "\n",
    "bikes_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create design matrices for test data**\n",
    "\n",
    "We will perform similar data cleaning and preparation on the test dataset as we did for the training and validation dataset. \n",
    "We use the same function get_design_mats for this purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_best, y_train_best, X_test_best, y_test_best = get_design_mats(bikes_train, bikes_test, degree = best_ridge_score_info['degree'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_best.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict and get R2 score on test data**\n",
    "\n",
    "The previous investigations identified the best tuning parameter for the Ridge regularization, combined with the best polynomial order. This model is now fit and used on test data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha = best_ridge_score_info['lambda'])\n",
    "model = ridge.fit(X_train_best.values, y_train_best.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predict_ridge = model.predict(X_test_best.values)\n",
    "r2_score_ridge_test = r2_score(y_test_best.values, y_test_predict_ridge)\n",
    "print(\"Test score \" + str(r2_score_ridge_test))\n",
    "print(\"Validation score \" + str(best_ridge_score_info['score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.6** Why do we think our model's test score was quite a bit worse than its validation score? Does the test set simply contain harder examples, or is something else going on?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is surpising that the Ridge regression with high variance is performing so well on our test data. The model's test score is a bit higher than its validation score. A reason could be that the distribution of test samples is quite similar to those of the validation set. We observed that after scaling the test data using the mean and std learned from the training data, the std of the test data is close to 1 and their mean is almost zero.\n",
    "\n",
    "It might be interesting to verify why we are doing better on the test set than on the train set. A reason could be that the validation set inculded outliers.\n",
    "\n",
    "We will perform cross-validation for reducing variability biais, as well as a Lasso regression in order to force some variables to zero and better understand the effect of each variable on the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Comparing Ridge, Lasso, and OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1** Build a dataset with polynomial degree 1 and fit an OLS model, a Ridge model, and a Lasso model. Use `RidgeCV` and `LassoCV` to select the best regularization level from among `(.1,.5,1,5,10,50,100)`. \n",
    "\n",
    "Note: On the lasso model, we will need to increase `max_iter` to 100,000 for the optimization to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build a training and validation dataset with polynomial degree 1**\n",
    "\n",
    "We use the get_design_mats function to create a design matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1, y_train_1, X_val_1, y_val_1 = get_design_mats(bikes_train, bikes_val, degree = 1)\n",
    "X_train_1, y_train_1, X_test_1, y_test_1 = get_design_mats(bikes_train, bikes_test, degree = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit an OLS model**\n",
    "\n",
    "We fit a linear regression model on the training data without polynomial features of higher order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_model = LinearRegression().fit(X_train_1.values, y_train_1.values)\n",
    "y_val_predicted_ols = ols_model.predict(X_val_1.values)\n",
    "r2_score_val_ols = r2_score(y_val_1.values, y_val_predicted_ols)\n",
    "y_test_predicted_ols = ols_model.predict(X_test_1.values)\n",
    "r2_score_test_ols = r2_score(y_test_1.values, y_test_predicted_ols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit a Ridge model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous part we performed regularization on a polynomial multiple linear regression model using Ridge cost function  by choosing the tuning parameter and polynomial degree that provided the best $R^2$ score on validation dataset. \n",
    "\n",
    "It could however happen by chance that the validation data set was clustered around the regression line of our chosen model. In this case although we got a good validation score, we could experience a much worse score on production. \n",
    "\n",
    "By using cross-validation we can select the best tuning parameter with the lowest cross-validation error. Instead of just using one single validation set, we split our data in chunks (folds) on approximately equal size. For each value of the tuning parameter, we use one fold as validation set, and the remaining folds as training set. We average the errors and at the end we chose the $\\lambda$ value that provides the lowest mean error. \n",
    "\n",
    "For this purpose let's use the built-in, automated cross validation feature of sklearn for Ridge regression. Since our data has a time predictor, we shuffle the data explicitely before searching for the optimal $\\lambda$ via cross-validation.\n",
    "\n",
    "We use a fold size of 20, as recommended by law of thumb in litterature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "alphas_list = (.1,.5,1,5,10,50,100)\n",
    "splitter = KFold(n_splits = int(X_train_1.shape[0]/20), random_state=42, shuffle=True)\n",
    "\n",
    "ridge_cv_model = RidgeCV(alphas=alphas_list, cv=splitter, normalize=False)\n",
    "ridge_cv_model.fit(X_train_1.values, y_train_1.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The Ridge tuning parameter for obtained via cross-validation is \" + str(ridge_cv_model.alpha_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit a multilinear regression model with Ridge regularization on the whole training training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model = Ridge(alpha = ridge_cv_model.alpha_)\n",
    "ridge_model.fit(X_train_1.values, y_train_1.values)\n",
    "y_val_predicted_ridge = ridge_model.predict(X_val_1.values)\n",
    "r2_score_val_ridge = r2_score(y_val_1.values, y_val_predicted_ridge)\n",
    "y_test_predicted_ridge = ridge_model.predict(X_test_1.values)\n",
    "r2_score_test_ridge = r2_score(y_test_1.values, y_test_predicted_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit a Lasso model**\n",
    "\n",
    "Whereas a Ridge regularization adds a penalty term as sum of sqares of coefficients magnitude, Lasso uses the sum of their absolute values, nulling those with no effect on the response. \n",
    "\n",
    "We use cross-validation in order to find the best tuning parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_cv_model = LassoCV(alphas=alphas_list, cv=splitter, max_iter = 100000)\n",
    "lasso_cv_model.fit(X_train_1.values, y_train_1.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The Lasso tuning parameter for obtained via cross-validation is \" + str(lasso_cv_model.alpha_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit a multilinear regression model with Lasso regularization on the whole training training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model = Lasso(alpha = lasso_cv_model.alpha_)\n",
    "lasso_model.fit(X_train_1.values, y_train_1.values)\n",
    "y_val_predicted_lasso = lasso_model.predict(X_val_1.values)\n",
    "r2_score_val_lasso = r2_score(y_val_1.values, y_val_predicted_lasso)\n",
    "y_test_predicted_lasso = lasso_model.predict(X_test_1.values)\n",
    "r2_score_test_lasso = r2_score(y_test_1.values, y_test_predicted_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R2 scores of all three approaches are given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'model': ['osl', 'ridge', 'lasso'], \n",
    "              'R2 score on val': [r2_score_val_ols, r2_score_val_ridge, r2_score_val_lasso],\n",
    "              'R2 score on test': [r2_score_test_ols, r2_score_test_ridge, r2_score_test_lasso]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**3.2** Plot histograms of the coefficients found by each of OLS, ridge, and lasso. What trends do we see in the magnitude of the coefficients?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retrieve model coefficients**\n",
    "\n",
    "We retrieve predictor coefficients from our OLS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = pd.DataFrame({'predictors': X_train_1.columns.values, 'ols': ols_model.coef_.ravel()})\n",
    "coefs.ols = round(coefs.ols.astype('float'), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We retrieve predictor coefficients from our Ridge model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs['ridge'] = ridge_model.coef_.ravel()\n",
    "coefs.ridge = round(coefs.ridge.astype('float'), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We retrieve predictor coefficients from our Lasso model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs['lasso'] = lasso_model.coef_.ravel()\n",
    "coefs.lasso = round(coefs.lasso.astype('float'), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coefficients histograms**\n",
    "\n",
    "Below is a distribution plot of all coefficients for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3,figsize=(20,5))\n",
    "sns.distplot(coefs.ols, rug=True, kde=False, norm_hist=False, ax=ax[0])\n",
    "ax[0].set_title('histogram of coefficients - OLS model')\n",
    "ax[0].set_xlabel('coefficient magnitude')\n",
    "ax[0].set_ylabel('frequency')\n",
    "sns.distplot(coefs.ridge, rug=True, kde=False, norm_hist=False, ax=ax[1])\n",
    "ax[1].set_title('histogram of coefficients - RIDGE model')\n",
    "ax[1].set_xlabel('coefficient magnitude')\n",
    "ax[1].set_ylabel('frequency')\n",
    "sns.distplot(coefs.lasso[coefs.lasso!=0], rug=True, kde=False, norm_hist=False, ax=ax[2])\n",
    "ax[2].set_title('histogram of coefficients - LASSO model')\n",
    "ax[2].set_xlabel('coefficient magnitude')\n",
    "ax[2].set_ylabel('frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histograms show smaller magnitude for regularized coefficients. With the OLS model, coefficients are distributed in a wider values range. Ridge and Lasso models have shrinked coefficients with less variance. Lasso caused a lot of coefficients to be zero (we did not show them in the Lasso histogram in order to keep the remaining distribution visible, the list of zero coefficients is given below). \n",
    "\n",
    "Outliers coefficient are particularly large in the OLS model. Their magnitude is significantly reduced by Ridge and set to zero by Lasso. Most coefficients are found around zero.\n",
    "\n",
    "The overall variance of the coefficients for each model is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.var(coefs.ols), np.var(coefs.ridge), np.var(coefs.lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following coefficients were removed by Lasso regression. The corresponding predictors or interaction terms have very little or no effect on the number of bike rides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs[coefs.lasso==0].predictors.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3** The plots above show the overall distribution of coefficient values in each model, but do not show how each model treats individual coefficients. Build a plot which cleanly presents, for each feature in the data, 1) The coefficient assigned by OLS, 2) the coefficient assigned by ridge, and 3) the coefficient assigned by lasso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first sort the coefficients by overall magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs['all'] = abs(coefs.ols) + abs(coefs.ridge) + abs(coefs.lasso)\n",
    "coefs = coefs.sort_values(by = ['all'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stacked barplot below shows the magnitude of the coefficients associated with the predictors. The predictors with the cummulative largest coefficient magnitude are displayed at the beginning of the plot. The bar shows the magnitude obtained with each of the three models (OLS - blue, Ridge - orange, Lasso - green). \n",
    "\n",
    "Bars oriented to the left identify those predictors which have a negative effect on the predicted count of ridership. The remaining ones have a positive effect. A step increase of the later would imply a step increase in the count by the magnitude of the coefficient.\n",
    "\n",
    "Lasso provides the best interpretability. For example humidity has a considerable impact on ridership during certain months (Sept). High temperature brings definetely more ridership. It is strange that Storm has no effect on the ridership. This is probably because of the lack of data during stormy weather.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs.plot.barh(x='predictors', y=['ols', 'ridge', 'lasso'], stacked=True, legend=True, figsize=(10, 30))\n",
    "plt.axvline(x=0, color='black', alpha=0.4)\n",
    "plt.xlabel('magnitude')\n",
    "plt.ylabel('variables')\n",
    "plt.title('coefficients estimates by OLS - Ridge - Lasso')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4** What trends do we see in the plot above? How do the three approaches handle the correlated pair `temp` and `atemp`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the coefficient is large, the sign of coefficients appears to be the same regardless the approach used. The coefficients which were set to zero by Lasso tend to be handled differently by Ridge and OLS when it comes to their sign. \n",
    "\n",
    "OLS handles temp as a positive predictor of ridership, and atemp as a negative predictore of ridership. This is strange because both predictors are positively correlated. We would expect temp and atemp to behave in the same direction. This is not the case for Lasso and Ridge approaches, where both temp and atemp predict an increase in ridership.\n",
    "\n",
    "Ridge assumes that both temp and atemp would have approximately the same impact on the response. Lasso however seems to have detected the collinearity between both and tried to shrink atemp towards zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs[(coefs.predictors=='temp')|(coefs.predictors=='atemp')].plot.barh(x='predictors', y=['ols', 'ridge', 'lasso'], stacked=True, legend=True, figsize=(6, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1** Reflect back on the `get_design_mats` function we built. Writing this function useful in the analysis? What issues might we have encountered if we copy/pasted the model-building code instead of tying it together in a function? Does a `get_design_mat` function seem wise in general, or are there better options?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The get_design_mats function was useful for better code visibility, debugging, it promoted less lines of code, allowing to reuse the same code logic through the questions. If the data structure is the same throughout the project, then using such a function is wise for exploring various polynomial and regularized models. An improvement of get_design_mats could be adding a flag for enabling/disabling the creation of interaction terms, and also providing the list of predictors that could interact as a parameter to the function. get_design_mats could also handle exceptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2** What are the costs and benefits of applying ridge/lasso regularization to an overfit OLS model, versus setting a specific degree of polynomial or forward selecting features for the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding polynomials in OLS improves the validation score while increasing prediction variance and risk of overfitting; Ridge/Lasso reduces the variance. Polynomial terms bring multicolinearity issues with them, what negatively affects interpretability; by shrinking the coefficients of less important predictors, Ridge/Lasso helps avoiding that issue. Forward selection identifies the subset of predictors which give the less error, but this approach can miss important predictors. Lasso can help in the special case of high dimensionality where there are more predictors than observations. The down side is, Ridge/Lasso comes at the cost of computation power especcially when the data set is very big.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.3** This project posed a purely predictive goal: forecast ridership as accurately as possible. How important is interpretability in this context? Considering, e.g., the lasso and ridge models from part 3, how would we react if the models predicted well, but the coefficient values didn't make sense once interpreted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although prediction was more important than interpretability in this context, having for example two strange coefficients for temp and atemp, or storm should also be considered closely, as they could affect prediction on another test dataset. Inference should be ideally investigated even if the main goal is prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**4.4** Reflect back on our original goal of helping BikeShare predict what demand will be like in the week ahead, and thus how many bikes they can bring in for maintenance. In the view, did we accomplish this goal? If yes, which model would we put into production and why? If not, which model came closest, what other analyses might we conduct, and how likely do we think they are to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any given month, day of week, weather forecast and hour of the day, our models are able to predict the number of rides that will be made by both registered and casual users with an acceptable R2 score (see summary table below). However without a proper analysis of coefficients variance, bias, residuals and coefficients p-values/CI, we think that the prediction goal was not fully achieved. \n",
    "\n",
    "We suggest a model obtained with interaction terms, polynomial model with degree 6 regularized with Lasso using a tuning parameter obtained via cross-validation. This model would predict with R2 score around 0.478, would bring low coefficients variance, and would consider only significant predictors.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"Model\": ['original', 'polynomial degree 2', 'polynomial degree 2 with forward selection', 'polynomial degree 6 with int. terms', 'multilinear polynomia degree 8 with inter. terms', 'polynomial degree 8 with Ridge lamba 0.1', 'polynomial degree 1 with inter. terms', 'polynomial degree 1 with inter. terms and Ridge', 'polynomial degree 1 with inter. terms and Lasso'], \"Test R2\": [0.406, 0.496, 0.445, 0.478, 0.556, 0.556, 0.349, 0.388, 0.383]})"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "day_level_test.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
