{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p3ZpGrdEO9Lw"
   },
   "source": [
    "## Logistic Regression, High Dimensionality and PCA\n",
    "\n",
    "Hillary Cheruiyot\n",
    "\n",
    "<hr style=\"height:2pt\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "w6kC__i_O9N-",
    "outputId": "bd224476-de23-43c7-94aa-4a0d945900b8"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.api import OLS\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import math\n",
    "from scipy.special import gamma\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RK13snFKO9Oo"
   },
   "source": [
    "**Cancer Classification from Gene Expressions**\n",
    "\n",
    "In this project, we will build a classification model to distinguish between two related classes of cancer, acute lymphoblastic leukemia (ALL) and acute myeloid leukemia (AML), using gene expression measurements. The data set is provided in the file `data/dataset_hw5_1.csv`. Each row in this file corresponds to a tumor tissue sample from a patient with one of the two forms of Leukemia. The first column contains the cancer type, with 0 indicating the ALL class and 1 indicating the AML class. Columns 2-7130 contain expression levels of 7129 genes recorded from each tissue sample. \n",
    "\n",
    "In the following parts, we will use linear and logistic regression to build classification models for this data set. We will also use Principal Components Analysis (PCA) to reduce its dimensions. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SxtrdNV1O9O0"
   },
   "source": [
    "## Part 1: Data Exploration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NwtlfIWUO9PA"
   },
   "source": [
    "\n",
    "First step is to split the observations into an approximate 50-50 train-test split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mgdhHx5SO9PO"
   },
   "outputs": [],
   "source": [
    "np.random.seed(9002)\n",
    "df = pd.read_csv('data/dataset_hw5_1.csv')\n",
    "msk = np.random.rand(len(df)) < 0.5\n",
    "data_train = df[msk]\n",
    "data_test = df[~msk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yk7VXYrdO9QA"
   },
   "source": [
    "**1.1** Take a peek at the training set: we should notice the severe differences in the measurements from one gene to the next (some are negative, some however around zero, and some are well into the thousands).  To account for these differences in scale and variability, normalize each predictor to vary between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Es1QVK9A6tKc"
   },
   "outputs": [],
   "source": [
    "def normalize_columns(df, cols, scaler):\n",
    "    df_copy = df.copy()\n",
    "    df_copy[cols] = scaler.transform(df[cols])\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W7UWvcco6ulk"
   },
   "outputs": [],
   "source": [
    "response_col = 'Cancer_type'\n",
    "predictors = data_train.columns.difference([response_col])\n",
    "scaler = MinMaxScaler().fit(data_train[predictors])\n",
    "data_train_scaled = normalize_columns(data_train, predictors, scaler)\n",
    "data_test_scaled = normalize_columns(data_test, predictors, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GqO7DDhPDc1m"
   },
   "outputs": [],
   "source": [
    "# separate into predictors and response\n",
    "## train\n",
    "y_train = data_train_scaled[response_col]\n",
    "X_train_scaled = data_train_scaled[predictors]\n",
    "## test\n",
    "y_test = data_test_scaled[response_col]\n",
    "X_test_scaled = data_test_scaled[predictors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 346
    },
    "colab_type": "code",
    "id": "mhOeNALHEE1E",
    "outputId": "067e4696-d211-46a0-aa28-38853fa81731"
   },
   "outputs": [],
   "source": [
    "# verify transformation\n",
    "X_train_scaled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7TxkV9LViB0e"
   },
   "source": [
    "The variance is indeed very large accross genes. After scaling this issue is solved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HlNBMg7EO9RG"
   },
   "source": [
    "\n",
    "**1.2** Notice that the resulting training set contains more predictors than observations. Do we foresee a problem in fitting a classification model to such a data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "colab_type": "code",
    "id": "2L3pKickVxXg",
    "outputId": "03f5ba20-7904-42f1-d9d8-7c5c54ec83c2"
   },
   "outputs": [],
   "source": [
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aaoH0ZD-O9R4"
   },
   "source": [
    "High numbers of predictors make things harder in three key ways:\n",
    "\n",
    "- Matrices are often not invertable\n",
    "- There's a bigger chance of multicollinearity\n",
    "- There's a danger of overfitting\n",
    "\n",
    "There are 7130 predictors and only 40 samples. The classification model might be unidentifiable. Regularization and PCA could be required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BYuoF2gUO9SC"
   },
   "source": [
    "**1.3** Let's explore a few of the genes and see how well they discriminate between cancer classes. Create a single figure with four subplots arranged in a 2x2 grid. Consider the following four genes: `D29963_at`, `M23161_at`, `hum_alu_at`, and `AFFX-PheX-5_at`. For each gene overlay two histograms of the gene expression values on one of the subplots, one histogram for each cancer type. Does it appear that any of these genes discriminate between the two classes well? How are we able to tell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AcfUF3WuO9SU"
   },
   "outputs": [],
   "source": [
    "def plot_genes_histograms(genes_list, df):\n",
    "    count = len(genes_list)\n",
    "    if (count % 2 != 0) | (count<2):\n",
    "        return\n",
    "    genes_arr = np.array(genes_list).reshape(-1, 2)\n",
    "    fig, ax = plt.subplots(count//2, 2, figsize=(18, 4*count//2))\n",
    "    for (i,j), gene in np.ndenumerate(genes_arr):\n",
    "        sns.distplot(df[df['Cancer_type'] == 0][gene],\n",
    "                     label='Acute Lymphoblastic Leukemia (ALL)', ax=ax[i][j])\n",
    "        sns.distplot(df[df['Cancer_type'] == 1][gene],\n",
    "                     label='Acute Myeloid Leukemia (AML)', ax=ax[i][j])\n",
    "        ax[i][j].set_title(gene)\n",
    "        ax[i][j].legend()\n",
    "        ax[i][j].set_xlabel('')\n",
    "        ax[i][j].set_ylabel('frequency')\n",
    "    ax[i][j].set_xlabel('expression level')\n",
    "    ax[i][j-1].set_xlabel('expression level')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "colab_type": "code",
    "id": "XsKK87D_iz_M",
    "outputId": "0795da90-7834-4933-d617-e7cc20ae86ab"
   },
   "outputs": [],
   "source": [
    "genes_list = ['D29963_at', 'M23161_at', 'hum_alu_at', 'AFFX-PheX-5_at']\n",
    "plot_genes_histograms(genes_list, data_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4vd3nrWgO9S6"
   },
   "source": [
    "The histogram overlay shows the distribution of gene expression levels. If the probability density function of ALL does not overlay with the function for AML, then we can conclude that that gene discriminates between both classes of cancer. Unfortunately there is a significant overlay of the pair-PDFs for all four genes. None of the genes clearly discrininate between ALL and AML. However we can say that low levels of D29963_at, low levels of hum_alu_at, very high levels of M23161_at, very low levels of AFFX-PheX-5_at provide a higher probability that the patient develops ALL. For very high levels of D29963_at the probability is higher that the cancer type is AML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G-vdu-8QO9TI"
   },
   "source": [
    "**1.4** Since our data has dimensions that are not easily visualizable, we want to reduce the dimensionality of the data to make it easier to visualize. Using PCA, find the top two principal components for the gene expression data. Generate a scatter plot using these principal components, highlighting the two cancer types in different colors and different markers ('x' vs 'o', for example). How well do the top two principal components discriminate between the two classes? How much of the variance within the predictor set do these two principal components explain? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V9EvQDG1kHq-"
   },
   "outputs": [],
   "source": [
    "pca_transformer = PCA(2).fit(X_train_scaled)\n",
    "X_train_2d = pca_transformer.transform(X_train_scaled)\n",
    "var_explained = pca_transformer.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "colab_type": "code",
    "id": "uH95PMd3kOzK",
    "outputId": "0678e711-a883-43f2-912f-e8c5cf5ba0bd"
   },
   "outputs": [],
   "source": [
    "colors = ['r','c']\n",
    "label_text = [\"Acute Lymphoblastic Leukemia (ALL)\", \"Acute Myeloid Leukemia (AML)\"]\n",
    "\n",
    "markshapes = ['x','o']\n",
    "# and we loop over the different groups\n",
    "for cancer_type in [0,1]:\n",
    "    df_per_type = X_train_2d[y_train==cancer_type]\n",
    "    plt.scatter(df_per_type[:,0], df_per_type[:,1],\n",
    "                marker=markshapes[cancer_type],\n",
    "                c = colors[cancer_type],\n",
    "                label=label_text[cancer_type])\n",
    "    \n",
    "plt.xlabel(\"PCA Dimension 1\")\n",
    "plt.ylabel(\"PCA Dimention 2\")\n",
    "plt.title(\"top two principal components for the gene expression data\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eJ8PVIBvO9UE"
   },
   "source": [
    "Both disease conditions are not very well discriminated by the top two PCA components. Low values (< 4) in dimension 2 predict both ALL and AML conditions with relatively equal probability. High values (>4) in dimension 2 could be used to predict ALL with high accuracy. The combination of values with dimension 1 greater than 3, and dimension 2 less than zero could be used to classify AML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "tFX_dNAJliNK",
    "outputId": "746ae74c-6afc-4075-9492-779f5667ac25"
   },
   "outputs": [],
   "source": [
    "print(\"Variance explained by each PCA component:\", var_explained)\n",
    "print(\"Total Variance Explained:\", np.sum(var_explained))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4zCGvMuIlm7i"
   },
   "source": [
    "The first PCA dimension captures 15.88% of the variance in the data, and the second PCA dimension adds another 11.43%. Together, we've got just 27.31% of the total variation in the training data. We migh need additional PCA dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ru5fcPUUO9UM"
   },
   "source": [
    "**1.5** Plot the cumulative variance explained in the feature set as a function of the number of PCA-components (up to the first 50 components).  Do we feel 2 components is enough, and if not, how many components would we choose to consider?  Determine how many components are needed to explain at least 90% of the variability in the feature set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GonrVBaqR9gh"
   },
   "outputs": [],
   "source": [
    "pca_transformer = PCA(50).fit(X_train_scaled)\n",
    "X_train_50d = pca_transformer.transform(X_train_scaled)\n",
    "var_explained_50d = pca_transformer.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1289
    },
    "colab_type": "code",
    "id": "41ieYOqVnYNG",
    "outputId": "2f8f9ba8-7fcb-4a38-8158-f264f2cb04d7"
   },
   "outputs": [],
   "source": [
    "var_explained_cum = np.cumsum(var_explained_50d)\n",
    "\n",
    "# display as a table\n",
    "var_explained_df = pd.DataFrame()\n",
    "var_explained_df['dimensions'] = np.array(list(range(1,len(var_explained_cum)+1)))\n",
    "var_explained_df['cumulative_var_explained'] = var_explained_cum\n",
    "display(var_explained_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "colab_type": "code",
    "id": "g58KZDUJnae-",
    "outputId": "3592b7d1-5e6f-4043-f10f-d5e74cf28923"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(18, 5))\n",
    "ax[0].scatter(range(1, 41), var_explained_cum)\n",
    "ax[0].set_xlabel(\"PCA Dimension\")\n",
    "ax[0].set_ylabel(\"Total Variance Captured\")\n",
    "ax[0].set_title(\"Cumulative Variance Explained by PCA\");\n",
    "ax[1].scatter(range(1, 41), var_explained_50d)\n",
    "ax[1].set_xlabel(\"PCA Dimension\")\n",
    "ax[1].set_ylabel(\"Variance Captured\")\n",
    "ax[1].set_title(\"Variance Explained by PCA\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3xwKeM0fO9U6"
   },
   "source": [
    "Adding more components has no effect on the first two principal components. It is safe to add more components in order to achieve greater variance coverage of the feature set.\n",
    "\n",
    "Looking at the plot, more variance is captured by increasing the number of PCA components. We can have a maximum number of 40 components since we have 40 samples in the training dataset.  Therefore we can consider adding components as long as the explained variance increases significantly. For example, with 22 dimensions, our model can explain 80% of the original variance. In order to explain 100% from there, we have to almost double the number of dimensions. This would have drawback on interpretability and increases the danger of overfitting.\n",
    "\n",
    "But if we look at the graph of variance captured (\"Variance Explained by PCA\"), the slope of the curve flattens out after about 10 dimensions. Adding additional dimensions after around this point only adds around 2% to the total variance captured. So, even beyond 10 components, when we add additional components, we would be adding complexity without explaining that much more variance.\n",
    "\n",
    "So even while we only capture 58% of the variance (under two-thirds), 10 dimensions might be a good tradeoff between variance and additional model complexity.\n",
    "\n",
    "However, we would ordinarily perform cross-validation to help determine the optimal number of components.\n",
    "\n",
    "By looking at the plot, 29 components are required in order to explain at least 90% of the variability in the feature set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "leSW_gW8O9VE"
   },
   "source": [
    "## Part 2: Linear Regression vs. Logistic Regression\n",
    "\n",
    "We discuss how to use both linear regression and logistic regression for classification. For this part, we will work with a single gene predictor, `D29963_at`, to explore these two methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QXTZMVwdO9VO"
   },
   "source": [
    "**2.1** Fit a simple linear regression model to the training set using the single gene predictor `D29963_at` to predict cancer type and plot the histogram of predicted values. We could interpret the scores predicted by the regression model for a patient as an estimate of the probability that the patient has `Cancer_type`=1 (AML). Is there a problem with this interpretation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "colab_type": "code",
    "id": "1k0jBGBZO9VW",
    "outputId": "9fc616f3-84fd-4caf-a000-fd3ea88a6f0c"
   },
   "outputs": [],
   "source": [
    "model_D29963at_ols = OLS(data_train_scaled.Cancer_type.values,\n",
    "                         sm.add_constant(data_train_scaled.D29963_at)\n",
    "                        ).fit()\n",
    "model_D29963at_ols.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rOIIS31yX4aT"
   },
   "outputs": [],
   "source": [
    "# get predictions for train and test\n",
    "lin_y_pred_train = model_D29963at_ols.predict(sm.add_constant(data_train_scaled.D29963_at))\n",
    "lin_y_pred_test = model_D29963at_ols.predict(sm.add_constant(data_test_scaled.D29963_at))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uoPE84MQhD4k"
   },
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def plot_hist(data, label, color, ax):\n",
    "    sns.distplot(data, color=color, label=label, ax=ax)\n",
    "\n",
    "def set_ax_title_label_legend(ax, title = None, xlabel=\"predictor\", ylabel=\"response\"):\n",
    "  if title is None:\n",
    "    ax.set_title(ylabel + \" vs. \" + xlabel)\n",
    "  else:\n",
    "    ax.set_title(title)\n",
    "  ax.set_xlabel(xlabel)\n",
    "  ax.set_ylabel(ylabel)\n",
    "  ax.legend()\n",
    "  \n",
    "def plot_overlay_hist(data_to_overlay, title, labels, colors, ax):\n",
    "  for i, data in enumerate(data_to_overlay):\n",
    "    plot_hist(data, labels[i], colors[i], ax)\n",
    "    # set labels\n",
    "    # call labels/legends function\n",
    "    set_ax_title_label_legend(ax,\n",
    "        title=title,\n",
    "        xlabel=labels[i],\n",
    "        ylabel=\"Frequency\")\n",
    "\n",
    "def plot_overlay_hist_series(data_list, titles, labels, colors, axes):\n",
    "  if len(axes) < len(data_list):\n",
    "    raise Exception('Fewer axes ({}) than data groups ({})'.format(\n",
    "      len(axes),\n",
    "      len(data_list)))\n",
    "  for i, data_to_overlay in enumerate(data_list):\n",
    "    plot_overlay_hist(data_to_overlay, titles[i], labels, colors, axes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1047
    },
    "colab_type": "code",
    "id": "HxsD5MyYakFz",
    "outputId": "848abaed-f5fd-4a47-f99f-2f852c8bdd1c"
   },
   "outputs": [],
   "source": [
    "# histogram of predictions\n",
    "def plot_simple_linear_hist():\n",
    "  ## set up plots\n",
    "  nrows = 2\n",
    "  ncols = 1\n",
    "  fsize = (8, 16)\n",
    "  fig, ax = plt.subplots(nrows, ncols, figsize=fsize)\n",
    "\n",
    "  ## data to iterate over\n",
    "  colors = [ 'C2', 'C3', ]\n",
    "  y_train_to_plot = [ data_train_scaled.Cancer_type, lin_y_pred_train, ]\n",
    "  y_test_to_plot = [ data_test_scaled.Cancer_type, lin_y_pred_test, ]\n",
    "  y_labels = [  \"Actual response\", \"OLS on gene D29963_at\", ]\n",
    "  plot_labels = [ \"Training Dataset\", \"Test Dataset\", ]\n",
    "  plot_overlay_hist_series(\n",
    "      [y_train_to_plot, y_test_to_plot],\n",
    "      plot_labels,\n",
    "      y_labels,\n",
    "      colors,\n",
    "      ax)\n",
    "\n",
    "  # add vertical lines\n",
    "  threshold = 0.5\n",
    "  ymin, ymax = 0, 2.3\n",
    "  for a in ax:\n",
    "    a.vlines(threshold, 0, ymax, colors='k', linestyles='dashed')\n",
    "    a.set_ylim(ymin, ymax) # reset frame\n",
    "  \n",
    "  fig.suptitle(\"Cancer type: ALL[0] or ALM[1]\", fontsize=18)\n",
    "\n",
    "plot_simple_linear_hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u5WSvZGAO9WK"
   },
   "source": [
    "\n",
    "Although the response is binary, the OLS regession model's range of outputs are continuous. This leads to major issues for interpreting the output values.\n",
    "\n",
    "For example, in the plots above the true response is trivially bimodal at zero and one in both training and test sets. But the histograms for the OLS predictions are in other ranges. In the training set, there are peak counts for values that are near 0.4 and 1.2. In the test set, we see the most number of predictions near 0.4. What does it mean to be 0.4 between two cancer types? And moreso, what does it mean to be 1.2 like a certain cancer type? Are observations with response value 1.2 three times _more similar_ to ALM (cancer type 1)?\n",
    "\n",
    "The fundamental problem is that regression model can produce predictions that fall outside the bounds of $(0, 1)$, and thus produces values that do not fit the definition of probabilities, i.e. there is no such thing as a probabilities greater than one or less than zero. The 1.2 we see above cannot be a probability.\n",
    "\n",
    "As one consequence, the usual way of discussing false positives and false negatives, which relies on Bayes' Theorem, breaks down because of this, since the inputs to Bayes' theorem (and thus the outputs) are no longer probabilities.\n",
    "\n",
    "In a medical context, where a missed diagnosis can be very expensive, and series of tests need to be designed around the rate of false positives and false negatives, this becomes detrimental."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TpcBFFZUO9X-"
   },
   "source": [
    "**2.2** The fitted linear regression model can be converted to a classification model (i.e. a model that predicts one of two binary classes 0 or 1) by classifying patients with predicted score greater than 0.5 into `Cancer_type`=1, and the others into the `Cancer_type`=0. Evaluate the classification accuracy of the obtained classification model on both the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UJNcCgT3O9YI",
    "outputId": "b0bfbcd0-89c3-43b9-85e5-f81f1500baff"
   },
   "outputs": [],
   "source": [
    "# get quantitative response\n",
    "y_train_D29963at_predicted_ols_qr = model_D29963at_ols.predict(\n",
    "    sm.add_constant(data_train_scaled.D29963_at))\n",
    "# convert to binary response\n",
    "y_train_D29963at_predicted_ols = y_train_D29963at_predicted_ols_qr >= 0.5\n",
    "accuracy_train_D29963at_ols = (np.sum(\n",
    "    y_train_D29963at_predicted_ols == data_train_scaled.Cancer_type.values) /\n",
    "                               len(data_train_scaled.Cancer_type.values))\n",
    "\n",
    "print(\"Classification accuracy of the OLS classification model on train set is \",\n",
    "      accuracy_train_D29963at_ols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "GUYpg31IxsXe",
    "outputId": "e4019955-af5d-47a8-85e3-b21944c86d24"
   },
   "outputs": [],
   "source": [
    "# get quantitative response\n",
    "y_test_D29963at_predicted_ols_qr = model_D29963at_ols.predict(\n",
    "    sm.add_constant(data_test_scaled.D29963_at))\n",
    "# convert to binary response\n",
    "y_test_D29963at_predicted_ols = y_test_D29963at_predicted_ols_qr >= 0.5\n",
    "accuracy_test_D29963at_ols = (np.sum(\n",
    "    y_test_D29963at_predicted_ols == data_test_scaled.Cancer_type.values) /\n",
    "                              len(data_test_scaled.Cancer_type.values))\n",
    "\n",
    "print(\"Classification accuracy of the OLS classification model on test set is \",\n",
    "      accuracy_test_D29963at_ols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mnomIFR_O9Yo"
   },
   "source": [
    "The classification based on linear regression predicts the cancer type with 75.75% accuracy on test set while reaching 80% on training set. However, we see the model generating predictions greater than one and less than zero, which are confusing to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KbCSyRinO9Y2"
   },
   "source": [
    "**2.3** Next, fit a simple logistic regression model to the training set. How do the training and test classification accuracies of this model compare with the linear regression model? If there are no substantial differences, why do we think this happens? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "Z-wnCaRGO9ZE",
    "outputId": "57ffcac0-d884-417a-8e4d-d9dbb3cc347d"
   },
   "outputs": [],
   "source": [
    "BIG_C = 10**5\n",
    "columns = ['D29963_at']\n",
    "fitted_lr = LogisticRegression(\n",
    "    C=BIG_C).fit(\n",
    "    X_train_scaled[columns],\n",
    "    y_train)\n",
    "\n",
    "# predict\n",
    "y_pred_train = fitted_lr.predict(X_train_scaled[columns])\n",
    "y_pred_test = fitted_lr.predict(X_test_scaled[columns])\n",
    "    \n",
    "# performance\n",
    "train_score = accuracy_score(y_train, y_pred_train) * 100\n",
    "test_score = accuracy_score(y_test, y_pred_test) * 100\n",
    "\n",
    "print(\"Coefficients:\")\n",
    "print(fitted_lr.coef_)\n",
    "print(\"Intercepts:\")\n",
    "print(fitted_lr.intercept_)\n",
    "print(\"Train: {}%\".format(train_score))\n",
    "print(\"Test: {}%\".format(test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MJbsN238O9Zm"
   },
   "source": [
    "Strangely, the classification accuracy of both models seems to be the same. This probably occurs because the straight regression line fit and the logistic function divide the plane of data points in a similar fashion. Data points falling in the middle of the plane, i.e. with middle expression levels for this gene, would be most likely to fall on different sides of the boundary, depending on the function. It is possible there aren't many points in this area of the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "352qpYolO9aG"
   },
   "source": [
    "**2.4** Create a figure with 4 items displayed on the same plot:\n",
    "- the quantitative response from the linear regression model as a function of the gene predictor `D29963_at`.\n",
    "- the predicted probabilities of the logistic regression model as a function of the gene predictor `D29963_at`.  \n",
    "- the true binary response for the test set points for both models in the same plot. \n",
    "- a horizontal line at $y=0.5$. \n",
    "\n",
    "Based on these plots, does one of the models appear better suited for binary classification than the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 700
    },
    "colab_type": "code",
    "id": "1s3dxBEcO9aS",
    "outputId": "f94d6ea8-fc15-4d10-c47e-338988d5f062"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(10, 10))\n",
    "\n",
    "colors = ['C0', 'C4', 'C2']\n",
    "\n",
    "binary_size = 120\n",
    "\n",
    "# linear regression\n",
    "# quantitative response\n",
    "plt.scatter(data_test_scaled.D29963_at,\n",
    "            y_test_D29963at_predicted_ols_qr,\n",
    "            color=colors[0],\n",
    "            label = ' Linear regression quantitative response')\n",
    "# binary response\n",
    "plt.scatter(data_test_scaled.D29963_at,\n",
    "            y_test_D29963at_predicted_ols,\n",
    "            color=colors[0],\n",
    "            alpha=0.5,\n",
    "            s=binary_size,\n",
    "            label = ' Linear regression binary response')\n",
    "\n",
    "# logistic regression\n",
    "# probabilities\n",
    "y_prob_test = fitted_lr.predict_proba(\n",
    "    data_test_scaled.D29963_at.values.reshape(-1,1))[:,1]\n",
    "plt.scatter(data_test_scaled.D29963_at,\n",
    "            y_prob_test,\n",
    "            color=colors[1],\n",
    "            label = ' Logistic regression probability')\n",
    "# binary response\n",
    "plt.scatter(data_test_scaled.D29963_at,\n",
    "            y_pred_test,\n",
    "            s=binary_size,\n",
    "            color=colors[1],\n",
    "            alpha=0.5,\n",
    "            label = ' Logistic regression binary response')\n",
    "\n",
    "# true test response\n",
    "# binary response\n",
    "plt.scatter(data_test_scaled.D29963_at,\n",
    "            y_test,\n",
    "            color=colors[2],\n",
    "            marker='+',\n",
    "            alpha=1,\n",
    "            label = ' True binary response')\n",
    "\n",
    "# add y tick marks for cancer types?\n",
    "ax.axhline(y = 0.5, color = 'k', linestyle = '--')\n",
    "plt.ylabel(\"response\", fontsize=16)\n",
    "plt.xlabel(\"{} expression level\".format(columns[0]), fontsize=16)\n",
    "plt.legend(loc=\"best\", bbox_to_anchor=(1,1))\n",
    "plt.suptitle(\n",
    "    \"Linear and Logistic Regression Binary Predictions on Test Dataset\",\n",
    "    fontsize=16)\n",
    "plt.title(\n",
    "    \"\\nCancer types ALL[0]/ALM[1] vs. D29963_at gene expression level\",\n",
    "    fontsize=16)\n",
    "print(\"\") # suppress message\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UkEKB_s-O9aw"
   },
   "source": [
    "Based on the plot of the binary model predictions, it is hard to tell whether the logistic regression or the linear regression fares better in predicting the correct binary response in the test set. The linear regression seems to have one more false negative, and the logistic regression seems to have one more false positive. However, both models misclassify several of the same points.\n",
    "\n",
    "The logistic regession produces probabilities, which is an advantage, while the linear regression outputs a quantitative response that can exceed the bounds of $(0, 1)$, which is a disadvantage. But purely for the purposes of predicting the binary response in the test, they have fared roughly the same. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jKnx8LAbO9bK"
   },
   "source": [
    "## Part 3: Multiple Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rkZ_dTeXO9bo"
   },
   "source": [
    "**3.1** Fit a multiple logistic regression model with all the gene predictors from the data set.  How does the classification accuracy of this model compare with the models fitted in question 2 with a single gene (on both the training and test sets)?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "U68cHr73GTuR",
    "outputId": "241c40f5-1447-4445-fcf2-26034b079d9c"
   },
   "outputs": [],
   "source": [
    "fitted_lr = LogisticRegression(C=BIG_C).fit(X_train_scaled, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred_train = fitted_lr.predict(X_train_scaled)\n",
    "y_pred_test = fitted_lr.predict(X_test_scaled)\n",
    "    \n",
    "# performance\n",
    "train_score = accuracy_score(y_train, y_pred_train) * 100\n",
    "test_score = accuracy_score(y_test, y_pred_test) * 100\n",
    "\n",
    "# report\n",
    "print(\"Coefficients:\")\n",
    "print(fitted_lr.coef_)\n",
    "print(\"Intercepts:\")\n",
    "print(fitted_lr.intercept_)\n",
    "print(\"Train: {}%\".format(train_score))\n",
    "print(\"Test: {}%\".format(test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "yG0cwQqxQIsG",
    "outputId": "cd3bce0b-058f-45f1-a223-5152ae27e1b0"
   },
   "outputs": [],
   "source": [
    "# classification counts\n",
    "# overview of false positives and false negatives\n",
    "pd.crosstab(y_test, y_pred_test,\n",
    "            margins=True,\n",
    "            rownames=['Actual'],\n",
    "            colnames=['Predicted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L2G4ztOeO9cS"
   },
   "source": [
    "By fitting our logistic regression model with all 7129 predictors, we have improved the train and test score to 100%. A big question is whether we are now in danger of overfitting, which such a large ratio of predictors to data points.\n",
    "We are obviously facing confounding as the results obtained with one predictor is different from those obtained with multiple predictors, indicating some correlations among them. A study of coefficients would bring more light into this observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MTUApTUbO9ck"
   },
   "source": [
    "**3.2** How many of the coefficients estimated by this multiple logistic regression in the previous part are significantly different from zero at a *significance level of 5%*? Use the same value of C=100000 as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oq4XDVnuO9cu"
   },
   "outputs": [],
   "source": [
    "#Creating model\n",
    "model = LogisticRegression(C=BIG_C)\n",
    "\n",
    "#Initializing variables\n",
    "bootstrap_iterations = 1000\n",
    "coeffs = np.zeros((bootstrap_iterations,\n",
    "                   data_train_scaled.shape[1]-1))\n",
    "\n",
    "#Conduct bootstraping iterations\n",
    "for i in range(bootstrap_iterations):\n",
    "    sample = data_train_scaled.sample(frac=1, replace=True)\n",
    "    y_train_sample = sample['Cancer_type']\n",
    "    X_train_sample = sample.drop(['Cancer_type'], axis=1)\n",
    "    model.fit(X_train_sample, y_train_sample)  \n",
    "    coeffs[i,:] = model.coef_    \n",
    "\n",
    "#Find Significant Columns, Count\n",
    "coeffs_count, significant_cols = 0, []\n",
    "for i in range(coeffs.shape[1]):\n",
    "    coeff_samples = coeffs[:,i]\n",
    "    lower_bound = np.percentile(coeff_samples, 2.5)\n",
    "    upper_bound = np.percentile(coeff_samples, 97.5) \n",
    "    if lower_bound>0 or upper_bound<0:\n",
    "        coeffs_count += 1\n",
    "        significant_cols.append(data_train_scaled.columns[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "JHVmUWy0sF-e",
    "outputId": "16872769-db7b-469b-d43d-0c4113f37fba"
   },
   "outputs": [],
   "source": [
    "print('Count of 95% statistically significant coefficients :', coeffs_count)\n",
    "print((\"Discussion: Out of 7130 coefficients, \" +\n",
    "       \"only {} of them are significant at p > 0.05.\\n\" +\n",
    "       \"That is  only {:.2f}% of them.\").format(coeffs_count, coeffs_count/7130*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hoOQUQJrO9dq"
   },
   "source": [
    "**3.3** Use the `visualize_prob` function provided below (or any other visualization) to visualize the probabilties predicted by the fitted multiple logistic regression model on both the training and test data sets. The function creates a visualization that places the data points on a vertical line based on the predicted probabilities, with the different cancer classes shown in different colors, and with the 0.5 threshold highlighted using a dotted horizontal line. Is there a difference in the spread of probabilities in the training and test plots? Are there data points for which the predicted probability is close to 0.5? If so, what can we say about these points?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W0jwpZIVO9bS"
   },
   "outputs": [],
   "source": [
    "#--------  visualize_prob\n",
    "# A function to visualize the probabilities predicted by a Logistic Regression model\n",
    "# Input: \n",
    "#      model (Logistic regression model)\n",
    "#      x (n x d array of predictors in training data)\n",
    "#      y (n x 1 array of response variable vals in training data: 0 or 1)\n",
    "#      ax (an axis object to generate the plot)\n",
    "\n",
    "def visualize_prob(model, x, y, ax):\n",
    "    # Use the model to predict probabilities for x\n",
    "    y_pred = model.predict_proba(x)\n",
    "    \n",
    "    # Separate the predictions on the label 1 and label 0 points\n",
    "    ypos = y_pred[y==1]\n",
    "    yneg = y_pred[y==0]\n",
    "    \n",
    "    # Count the number of label 1 and label 0 points\n",
    "    npos = ypos.shape[0]\n",
    "    nneg = yneg.shape[0]\n",
    "    \n",
    "    # Plot the probabilities on a vertical line at x = 0, \n",
    "    # with the positive points in blue and negative points in red\n",
    "    pos_handle = ax.plot(np.zeros((npos,1)), ypos[:,1], 'bo', label = 'Cancer Type 1')\n",
    "    neg_handle = ax.plot(np.zeros((nneg,1)), yneg[:,1], 'ro', label = 'Cancer Type 0')\n",
    "\n",
    "    # Line to mark prob 0.5\n",
    "    ax.axhline(y = 0.5, color = 'k', linestyle = '--')\n",
    "    \n",
    "    # Add y-label and legend, do not display x-axis, set y-axis limit\n",
    "    ax.set_ylabel('Probability of AML class')\n",
    "    ax.legend(loc = 'best')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.set_ylim([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "colab_type": "code",
    "id": "C99ab4_EO9d0",
    "outputId": "32150908-3fd5-4bba-b3e9-f6c1c8e87079"
   },
   "outputs": [],
   "source": [
    "\"\"\" Plot classification model \"\"\"\n",
    "\n",
    "#Create Plot\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "\n",
    "#Plot Training\n",
    "visualize_prob(fitted_lr, X_train_scaled, y_train, ax1)\n",
    "ax1.set_title('Training Dataset')\n",
    "\n",
    "#Plot Testing\n",
    "visualize_prob(fitted_lr, X_test_scaled, y_test, ax2)\n",
    "ax2.set_title('Testing Dataset')\n",
    "ax2.set_ylabel('')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q29W9MhRO9eG"
   },
   "source": [
    "The probabilities in the training set are concentrated solely at 0 and 1.  The probabilities for the data in the test set are more spread out, even though the accuracy for the test set was 100%. A few of the points classified as ALM (cancer type 1) received probabilities fairly close to the threshold of 0.5. These points correspond to samples which are close to the classification boundary. Their class can be highly impacted when the boundary is slightly moved. The associated patients are good candidates for false positives or false negatives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cLhH159QO9eM"
   },
   "source": [
    "**3.4** Comment on the classification accuracy of the train and test sets. Given the results above how would we assess the generalization capacity of the trained model?  What other tests or approaches would we suggest to better guard against the false sense of security on the accuracy of the model as a whole. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3sIUZyw-O9eS"
   },
   "source": [
    "Although the classification accuracy is 100% both on train and test sets, boostraping shows that only around 1/4 of coefficients are significant to the response. In order to generalize the model, it might be safe to perform regularization, or better, principal components analysis in order to account for the high dimensionality of our data and its possible negative impact on the prediction quality and interpretability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UEPXfgw4O9eY"
   },
   "source": [
    "## Part 4: PCR: Principal Components Regression \n",
    "\n",
    "High dimensional problems can lead to problematic behavior in model estimation (and make prediction on a test set worse), thus we often want to try to reduce the dimensionality of our problems. A reasonable approach to reduce the dimensionality of the data is to use PCA and fit a logistic regression model on the smallest set of principal components that explain at least 90% of the variance in the predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QKILzqplO9ee"
   },
   "source": [
    "**4.1:** Fit two separate Logistic Regression models using principal components as the predictors: (1) with the number of components we selected from part 1.5 and (2) with the number of components that explain at least 90% of the variability in the feature set. How do the classification accuracy values on both the training and tests sets compare with the models fit in part 3?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "hwbjLQ2MO9ek",
    "outputId": "6a6c2ef0-2d0c-44d5-a24f-91e4e2524280"
   },
   "outputs": [],
   "source": [
    "selected_dim = 10\n",
    "ninety_percent_dim = 29\n",
    "models_to_fit = [selected_dim, ninety_percent_dim]\n",
    "models_comparison = {}\n",
    "\n",
    "# Logistic PCR:\n",
    "# get principal components for different specific dimensions,\n",
    "# then train and fit a logistic regression model for each\n",
    "for i, dim in enumerate(models_to_fit):\n",
    "    cur_model = None\n",
    "    results = {}\n",
    "    # fit model\n",
    "    results['dim'] = dim\n",
    "    pca_transformer = PCA(dim).fit(X_train_scaled)\n",
    "    cur_X_train = pca_transformer.transform(X_train_scaled)\n",
    "    cur_X_test =  pca_transformer.transform(X_test_scaled)\n",
    "    cur_model = LogisticRegression(C=BIG_C).fit(cur_X_train, y_train)\n",
    "    \n",
    "    # predict\n",
    "    y_pred_train = cur_model.predict(cur_X_train)\n",
    "    y_pred_test = cur_model.predict(cur_X_test)\n",
    "    \n",
    "    # performance\n",
    "    train_score = accuracy_score(y_train, y_pred_train) * 100\n",
    "    test_score= accuracy_score(y_test, y_pred_test) * 100\n",
    "    \n",
    "    # store results and model info for later\n",
    "    results['model'] = cur_model # model used\n",
    "    results['X_train'] = cur_X_train\n",
    "    results['X_test'] = cur_X_test\n",
    "    results['train_score'] = train_score\n",
    "    results['test_score'] = test_score\n",
    "    results['y_pred_train'] = y_pred_train\n",
    "    results['y_pred_test'] = y_pred_test\n",
    "    models_comparison[dim] = results\n",
    "\n",
    "# report scores\n",
    "for d in models_comparison.keys():\n",
    "    print(\"{} dimension PCA - train: {:.2f}% test: {:.2f}%\".format(\n",
    "      models_comparison[d]['dim'],\n",
    "      models_comparison[d]['train_score'],\n",
    "      models_comparison[d]['test_score'],\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rj-Kz9ADO9e0"
   },
   "source": [
    "Previously by fitting a logistic regression model  with all 7129 predictors, we obtained 100% classification accuracy on train and test. Now using principal components analysis, we fit a logistic regression model using principal components as predictors.\n",
    "\n",
    "When using 10 components we achieve 100% classification accuracy on train set and 87.87% on test set. Using 29 components does not change the classification accuracy on training set, but the classification accuracy is improved to 96.97%. With 10 components, 58.07% of the variance is captured, against 90% captured by 29 components. It seems that our principal components regression has introduced few new false positives and/or false negatives compared to the logistic regression model from all 7129 predictors. By allowing some loss in prediction accuracy (from 100% down to 87.87% on test set), we have gained in better interpretability by using mostly significant prodictors. We recall that only 1/4 of predictors were found to be significant at p > 0.05 via cross-validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ema02v9KO9e6"
   },
   "source": [
    "**4.2:** Use the code provided in part 3 (or the choice of visualization) to visualize the probabilities predicted by the fitted models in the previous part on both the training and test sets. How does the spread of probabilities in these plots compare to those for the model in question 3.2? If the lower dimensional representation yields comparable predictive power, what advantage does the lower dimensional representation provide?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1044
    },
    "colab_type": "code",
    "id": "ctXNziX61uyI",
    "outputId": "74e5209c-ad0f-4dad-f0f5-3a21a8193b1c"
   },
   "outputs": [],
   "source": [
    "# function to visualize the probabilities\n",
    "# predicted by a set of fitted models with PCR  \n",
    "def visualize_prob_train_test(model, model_desc,\n",
    "                              X_train, y_train,\n",
    "                              X_test, y_test,\n",
    "                              ax1, ax2):\n",
    "  visualize_prob(model, X_train, y_train, ax1)\n",
    "  ax1.set_title(\"{}\\nTraining Data Probabilities\".format(model_desc))\n",
    "  visualize_prob(model, X_test, y_test, ax2)\n",
    "  ax2.set_title(\"{}\\nTest Data Probabilities\".format(model_desc))\n",
    "\n",
    "## visualize PCR models with 10 and 29 components\n",
    "\n",
    "# set up figure\n",
    "plot_rows = len(models_to_fit)+1\n",
    "plot_cols = 2\n",
    "fig, axes = plt.subplots(plot_rows, plot_cols, figsize=(plot_cols*6, plot_rows*6))\n",
    "\n",
    "count = 0\n",
    "for m in models_to_fit:\n",
    "  # model metadata\n",
    "  model = models_comparison[m]['model']\n",
    "  dim = models_comparison[m]['dim']\n",
    "  model_desc = \"{}-dim Logistic PCR\".format(dim)\n",
    "  \n",
    "  # plot\n",
    "  ax1, ax2 = axes[count][:]\n",
    "  X_train = models_comparison[m]['X_train']\n",
    "  X_test = models_comparison[m]['X_test']\n",
    "  visualize_prob_train_test(model, model_desc,\n",
    "                            X_train, y_train,\n",
    "                            X_test, y_test,\n",
    "                            ax1, ax2)\n",
    "\n",
    "  y_pred_test = models_comparison[m]['y_pred_test']\n",
    "  count += 1\n",
    "\n",
    "## visualize logistic regression model with significant predictors\n",
    "\n",
    "lr_model_significant_cols = LogisticRegression(C=BIG_C).fit(\n",
    "    X_train_scaled[significant_cols],\n",
    "    y_train)\n",
    "train_score_significant_cols = lr_model_significant_cols.score(\n",
    "    X_train_scaled[significant_cols],\n",
    "    y_train)\n",
    "test_score_significant_cols = lr_model_significant_cols.score(\n",
    "    X_test_scaled[significant_cols],\n",
    "    y_test)\n",
    "ax1, ax2 = axes[count][:]\n",
    "visualize_prob_train_test(lr_model_significant_cols,\n",
    "      'Logistic Regression with significant predictors',\n",
    "      X_train_scaled[significant_cols], y_train,\n",
    "      X_test_scaled[significant_cols], y_test,\n",
    "      ax1, ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 553
    },
    "colab_type": "code",
    "id": "7xVSWOrycOGW",
    "outputId": "53d5f266-7a61-4e99-ed9a-21469a204a3a"
   },
   "outputs": [],
   "source": [
    "## display an overview of false positives and false negatives for PCR models with 10 and 29 components\n",
    "count = 0\n",
    "display(\"Predicted class vs Actual class\")\n",
    "for m in models_to_fit:\n",
    "  # model metadata\n",
    "  model = models_comparison[m]['model']\n",
    "  dim = models_comparison[m]['dim']\n",
    "  model_desc = \"{}-dim Logistic PCR\".format(dim)\n",
    "\n",
    "  y_pred_test = models_comparison[m]['y_pred_test']\n",
    "  display(model_desc)\n",
    "  display(pd.crosstab(\n",
    "      y_test,\n",
    "      y_pred_test,\n",
    "      margins=True,\n",
    "      rownames=['Actual'],\n",
    "      colnames=['Predicted']))\n",
    "  count += 1\n",
    "\n",
    "## display an overview of false positives and false negatives \n",
    "# for logistic regression model with significant predictors\n",
    "display('Logistic Regression with significant predictors')\n",
    "display(pd.crosstab(\n",
    "    y_test,\n",
    "    lr_model_significant_cols.predict(\n",
    "        X_test_scaled[significant_cols]),\n",
    "    margins=True,\n",
    "    rownames=['Actual'],\n",
    "    colnames=['Predicted']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "svNMM5s8O9fY"
   },
   "source": [
    "The spread of probabilities appears to be the same accross all three models for training data, though the higher dimensional models seem to generate a larger number of predictions indicating uncertainty. All labels for class 0 are clustered around the probability 0, while labels for class 1 are found at probability 1. Such a spread produces a classification accuracy of 100% on the training data.\n",
    "\n",
    "The probabilities on test data are spread with more variability for the logistic regression model fit on significant predictors (at p < 0.05). This model produces 2 false negatives and 1 false positive. Both logistic models fit with 10 principal components produces 4 false negatives and 1 false positive. The probabilities obtained with those PCR models are mostly clustered near 0 and 1 with few data points spread towards the classification boundary at 0.5.\n",
    "\n",
    "The plots below shows how the explained variance (\"Variance Explained by PCA\") and cumulative explained variance (\"Cumulative Variance Explained by PCA\") changes with increasing dimensions. Lower dimensional representation with 10 components yields worse predictive power (test score 88%) compared to higher dimensions with 29 components (test score 96.97%). Lower dimension was chosen by eyeballing the plot of variance explained and looking at the elbow area.\n",
    "\n",
    "The test performance for 10 dimensions is slightly worse then the performance of nearby dimensions (a local minimum). It would have been possible to increase the amount of dimensions slightly in order to improve performance, while still staying close to the elbow region of the variance captured graph. A model selection process utilizing cross-validaton would have better suggested the best options for the number of PCR dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xDE-AMVjStlO"
   },
   "outputs": [],
   "source": [
    "def pca_logistic(dim):\n",
    "    pca_transformer = PCA(dim).fit(X_train_scaled)\n",
    "    X_train_pca = pca_transformer.transform(X_train_scaled)\n",
    "    X_test_pca = pca_transformer.transform(X_test_scaled)\n",
    "    model = LogisticRegression(C=1000000).fit(X_train_pca,y_train)\n",
    "    train_score = model.score(X_train_pca,y_train)\n",
    "    test_score = model.score(X_test_pca,y_test)\n",
    "    #train_score = accuracy_score(y_train, model.predict(X_train_pca))\n",
    "    #test_score = accuracy_score(y_test, model.predict(X_test_pca))\n",
    "    clf = LogisticRegressionCV(cv=5).fit(X_train_pca, y_train)\n",
    "    cv_score = clf.score(X_train_pca, y_train)\n",
    "    return (train_score,\n",
    "            test_score,\n",
    "            pca_transformer.explained_variance_ratio_,\n",
    "            model.coef_,\n",
    "            cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1507
    },
    "colab_type": "code",
    "id": "B0a4x3ljaFJI",
    "outputId": "1030ccd1-ab9a-4203-ec14-a368cd910454"
   },
   "outputs": [],
   "source": [
    "def plot_pca_scores(max_dim=40):\n",
    "    \n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    cv_scores = []\n",
    "    dim_range = range(1, max_dim+1)\n",
    "    \n",
    "    for dim in dim_range:\n",
    "        (train_score,\n",
    "         test_score,\n",
    "         var_explained,\n",
    "         coefs,\n",
    "         cv_score) = pca_logistic(dim)\n",
    "        train_scores.append(round(train_score, 2))\n",
    "        test_scores.append(round(test_score, 2))\n",
    "        cv_scores.append(round(cv_score,2))\n",
    "        \n",
    "    fig, ax = plt.subplots(3,1,figsize=(8,20))\n",
    "    ax[0].plot(dim_range, train_scores, label='train')\n",
    "    ax[0].plot(dim_range, test_scores, label='test')\n",
    "    ax[0].axvline(x=10, color='r', linestyle='--')\n",
    "    ax[0].axvline(x=29, color='b', linestyle='--')\n",
    "    ax[0].set_xlabel('PCA dimension')\n",
    "    ax[0].set_ylabel('classification accuracy')\n",
    "    ax[0].set_title('PCR on multiple dimensions')\n",
    "    ax[0].legend()\n",
    "    \n",
    "    var_explained_cum = np.cumsum(var_explained)\n",
    "    ax[1].scatter(dim_range, var_explained_cum)\n",
    "    ax[1].set_xlabel(\"PCA Dimension\")\n",
    "    ax[1].set_ylabel(\"Cumulative Variance Captured\")\n",
    "    ax[1].set_title(\"Cumulative Variance Explained by PCA\")\n",
    "    \n",
    "    ax[2].scatter(dim_range, var_explained)\n",
    "    ax[2].set_xlabel(\"PCA Dimension\")\n",
    "    ax[2].set_ylabel(\"Variance Captured\")\n",
    "    ax[2].set_title(\"Variance Explained by PCA\")\n",
    "    \n",
    "    \n",
    "    print(\"Classification accuracy on test for each dimension: \", test_scores)\n",
    "    print(\"Variance captured by each single dimension: \", var_explained)\n",
    "    print(\"Cumulative variance captured at each dimension: \", var_explained_cum)\n",
    "    print(\"Cross-validation score for each dimension: \", cv_scores)\n",
    "\n",
    "plot_pca_scores()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "cs109a_hw5_109_submit.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
